This file is a merged representation of the entire codebase, combined into a single document. The content has been processed where security check has been disabled.
Generated by Repomix on: 2025-02-17T18:16:00.056Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    action-fail.md
    bug-report.md
    documentation.md
    feature-request.md
    refactor.md
  workflows/
    changelog.yml
    codeql-analysis.yml
    pre-commit.yml
    project.yml
    release.yml
    stale.yml
    test.yml
  CHANGELOG.md
  CODE_OF_CONDUCT.md
  CONTRIBUTING.md
  dependabot.yml
  FUNDING.yml
  pull_request_template.md
docs/
  source/
    conf.py
    index.rst
    installation.md
    introduction.md
    kale.embed.rst
    kale.evaluate.rst
    kale.interpret.rst
    kale.loaddata.rst
    kale.pipeline.rst
    kale.predict.rst
    kale.prepdata.rst
    kale.utils.rst
    notebooks.md
    tutorial.md
    yaml.md
  make.bat
  Makefile
  README.md
  requirements.txt
examples/
  action_dann/
    configs/
      EPIC-D12D2-CDAN.yaml
      EPIC-D12D2-DAN.yaml
      EPIC-D12D2-DANN.yaml
      README.md
    config.py
    main.py
    model.py
    README.md
    test.py
  avmnist_multimodal/
    configs/
      bimodal_interaction_fusion.yaml
      late_fusion.yaml
      low_rank_tensor_fusion.yaml
      README.md
    config.py
    main.py
    model.py
    README.md
  bindingdb_deepdta/
    configs/
      IC50-DeepDTA.yaml
      Kd-DeepDTA.yaml
      Ki-DeepDTA.yaml
      README.md
      tutorial.yaml
    config.py
    main.py
    model.py
    README.md
    tutorial.ipynb
  cifar_cnntransformer/
    configs/
      CF10-CNNTransformer-huge.yaml
      CF10-CNNTransformer-large.yaml
      CF10-CNNTransformer-small.yaml
      CF10-CNNTransformer.yaml
      CF10-StandardCNN-huge.yaml
      CF10-StandardCNN.yaml
      README.md
    config.py
    main.py
    model.py
    README.md
  cifar_isonet/
    configs/
      CF10-ISO38.yaml
      README.md
    config.py
    main.py
    model.py
    README.md
    trainer.py
  cmri_mpca/
    configs/
      README.md
      tutorial_lr.yaml
      tutorial_svc.yaml
    config.py
    main.py
    README.md
    tutorial.ipynb
  digits_dann/
    configs/
      MN2UP-CDAN.yaml
      MN2UP-DAN.yaml
      MN2UP-DANN.yaml
      README.md
      tutorial.yaml
    config.py
    main.py
    model.py
    README.md
    tutorial.ipynb
  fewshot_protonet/
    configs/
      demo.yaml
      template.yaml
    config.py
    eval_unseen_classes.py
    main.py
    README.md
  landmark_uncertainty/
    configs/
      4CH_data.yaml
      isbi_config.yaml
      no_gt_test_example.yaml
      one_target_example.yaml
      SA_data.yaml
    config.py
    main.py
    README.md
  multiomics_mogonet/
    configs/
      MOGONET_BRCA_quick_test.yaml
      MOGONET_ROSMAP_quick_test.yaml
    config.py
    main.py
    model.py
    README.md
  multisite_neuroimg_adapt/
    configs/
      README.md
      tutorial.yaml
    config.py
    main.py
    README.md
    tutorial.ipynb
  office_multisource_adapt/
    configs/
      digits-M3SDA.yaml
      Office2A-M3SDA.yaml
      Office2A-MFSAN.yaml
      README.md
      tutorial.yaml
    config.py
    main.py
    model.py
    README.md
  polypharmacy_gripnet/
    configs/
      PoSE_MINI-GripNet.yaml
      README.md
    config.py
    main.py
    model.py
    README.md
  toy_domain_adaptation/
    main.py
    README.md
    tutorial.ipynb
  video_loading/
    main.py
    README.md
  README.md
kale/
  embed/
    attention_cnn.py
    factorization.py
    feature_fusion.py
    gcn.py
    gripnet.py
    image_cnn.py
    mogonet.py
    positional_encoding.py
    README.md
    seq_nn.py
    uncertainty_fitting.py
    video_feature_extractor.py
    video_i3d.py
    video_res3d.py
    video_se_i3d.py
    video_se_res3d.py
    video_selayer.py
  evaluate/
    cross_validation.py
    metrics.py
    README.md
    similarity_metrics.py
    uncertainty_metrics.py
  interpret/
    model_weights.py
    README.md
    uncertainty_quantiles.py
    visualize.py
  loaddata/
    avmnist_datasets.py
    dataset_access.py
    few_shot.py
    image_access.py
    mnistm.py
    multi_domain.py
    multiomics_datasets.py
    polypharmacy_datasets.py
    README.md
    sampler.py
    tabular_access.py
    tdc_datasets.py
    usps.py
    video_access.py
    video_datasets.py
    video_multi_domain.py
    videos.py
  pipeline/
    base_nn_trainer.py
    deepdta.py
    domain_adapter.py
    fewshot_trainer.py
    mpca_trainer.py
    multi_domain_adapter.py
    multiomics_trainer.py
    README.md
    video_domain_adapter.py
  predict/
    class_domain_nets.py
    decode.py
    isonet.py
    README.md
    uncertainty_binning.py
  prepdata/
    chem_transform.py
    graph_negative_sampling.py
    image_transform.py
    README.md
    string_transform.py
    supergraph_construct.py
    tabular_transform.py
    tensor_reshape.py
    video_transform.py
  utils/
    download.py
    initialize_nn.py
    logger.py
    print.py
    README.md
    save_xlsx.py
    seed.py
  __init__.py
  README.md
tests/
  embed/
    test_attention_cnn.py
    test_factorization.py
    test_feature_fusion.py
    test_gripnet.py
    test_image_cnn.py
    test_mogonet.py
    test_seq_nn.py
    test_video_feature_extractor.py
    test_video_i3d_r3d.py
  evaluate/
    test_cross_validation.py
    test_metrics.py
    test_uncertainty_metrics.py
  helpers/
    boring_model.py
    pipe_test_helper.py
  interpret/
    test_uncertainty_quantiles.py
  loaddata/
    test_avmnist_datasets.py
    test_few_shot.py
    test_image_access.py
    test_multiomics_datasets.py
    test_sampler.py
    test_tabular_access.py
    test_tdc_datasets.py
    test_video_access.py
  pipeline/
    test_base_nn_trainer.py
    test_deepdta.py
    test_domain_adapter.py
    test_fewshot_trainer.py
    test_mpca_trainer.py
    test_multi_domain_adapter.py
    test_multiomics_trainer.py
    test_uncertainty_qbin_pipeline.py
    test_video_domain_adapter.py
  predict/
    test_class_domain_nets.py
    test_decode.py
    test_isonet.py
    test_uncertainty_binning.py
  prepdata/
    test_chem_transform.py
    test_image_transform.py
    test_tabular_transform.py
  utils/
    test_download.py
    test_initialize_nn.py
    test_logger.py
    test_print.py
    test_seed.py
  conftest.py
  download_test_data.py
  README.md
.gitignore
.pre-commit-config.yaml
.readthedocs.yaml
LICENSE
pyproject.toml
README.md
setup.cfg
setup.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/ISSUE_TEMPLATE/action-fail.md">
---
name: "Action failed"
about: Raised when a GitHub action fails
title: A scheduled test failed - {{ date | date('MMMM Do YYYY, h:mm:ss a') }}
labels: action failed
assignees: ''

---

A scheduled test failed - {{ date | date('MMMM Do YYYY, h:mm:ss a') }}.
</file>

<file path=".github/ISSUE_TEMPLATE/bug-report.md">
---
name: "\U0001F41B Bug report"
about: Create a report to help us improve PyKale
title: "[Bug]"
labels: bug
assignees: ''

---

# üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To reproduce

Steps to reproduce the behavior:

1.
2.
3.

Or, code snippet to reproduce the behaviour

```python
# Your code goes here
# Please make sure it does not require any external dependencies (other than those required by PyKale!)
# (We much prefer small snippets rather than links to existing libraries!)
```

** Stack trace/error message **
```
// Paste the bad output here!
```

## Expected Behaviour

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (`pip`, source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context
Add any other context about the problem here.
</file>

<file path=".github/ISSUE_TEMPLATE/documentation.md">
---
name: "\U0001F4DA Documentation"
about: Report an issue related to https://pykale.readthedocs.io
title: "[Docs]"
labels: documentation
assignees: ''

---

# üìö Documentation

** Is there documentation missing? **
<!-- Let us know what modules have missing or incomplete documentation -->

** Is documentation wrong? **
<!-- Let us know if you find a mistake in the documentation! -->

** Is there a feature that needs some example code? **
<!-- Let us know if we're missing any example notebooks -->

** Think you know how to fix the docs? ** (If so, we'd love a pull request from you!)

- Link to [PyKale documentation](https://PyKale.readthedocs.io)
</file>

<file path=".github/ISSUE_TEMPLATE/feature-request.md">
---
name: "\U0001F680 Feature request"
about: Submit a proposal/request for a new PyKale feature
title: "[Feature Request]"
labels: enhancement
assignees: ''

---

# üöÄ Feature Request

<!-- A clear and concise description of the feature proposal -->

## Motivation

**Is your feature request related to a problem? Please describe.**
<!-- A clear and concise description of what the problem is, e.g. I'm always frustrated when [...] -->
<!-- Please link to any relevant issues or other PRs! -->

## Pitch

**Describe the solution you'd like**
<!-- A clear and concise description of what you want to happen. -->

**Describe alternatives you've considered**
<!-- A clear and concise description of any alternative solutions or features you've considered. -->

**Are you willing to open a pull request?** (We LOVE contributions!!!)

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
</file>

<file path=".github/ISSUE_TEMPLATE/refactor.md">
---
name: "\U0001F527 Refactor"
about: Propose a refactor/speedup/improvement to PyKale's internals
title: "[Refactor]"
labels: refactor
assignees: ''

---

<!-- A clear and concise description of what you wish to refactor. Please include the following: -->

- <!-- Modules that will be modified -->
- <!-- Impact on code structure -->
- <!-- Impact on speed -->
- <!-- Will this be a breaking change? -->
</file>

<file path=".github/workflows/changelog.yml">
# This workflow will generate a log of changes automatically upon a new release.
# See https://github.com/marketplace/actions/changelog-ci

name: changelog

on:
  pull_request:
    types: [opened]

jobs:
  log-changes:
    name: Log changes
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run changelog
        uses: saadmk11/changelog-ci@v1.1.2
        with:
          changelog_filename: .github/CHANGELOG.md
          config_file: .github/changelog-ci-config.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path=".github/workflows/codeql-analysis.yml">
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: codeql-analysis

on:
  push:
    branches: [ main ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ main ]
  schedule:
    - cron: '15 18 * * 5'

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'python' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Learn more about CodeQL language support at https://git.io/codeql-language-support

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.
        # queries: ./path/to/local/query, your-org/your-repo/queries@main

    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).
    # If this step fails, then you should remove it and run the build manually (see below)
    - name: Autobuild
      uses: github/codeql-action/autobuild@v2

    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.
    # üìö https://git.io/JvXDl

    # ‚úèÔ∏è If the Autobuild fails above, remove it and uncomment the following three lines
    #    and modify them (or add more) to build your code if your project
    #    uses a compiled language

    #- run: |
    #   make bootstrap
    #   make release

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2
</file>

<file path=".github/workflows/pre-commit.yml">
# This workflow will run lint and many other pre-commit hooks.
# https://pre-commit.com/

name: pre-commit-check

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  pre-commit-check:
    name: Pre-commit checks including linting
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: "3.8"
    - name: Install dependencies
      run: |
        pip install pre-commit isort
        pre-commit install
    - name: Run pre-commit checks including linting
      run: |
        pre-commit run --all-files
</file>

<file path=".github/workflows/project.yml">
# This workflow will automatically add an issue or pull request to the project defined on lines 22 and 32.
# The project needs to be updated when we move to a new project.
# https://github.com/marketplace/actions/add-to-github-projects
# The encrypted secret key "ADD_TO_PROJECT_PAT" has been created in accordance with the guidelines provided in https://docs.github.com/en/actions/security-guides/encrypted-secrets

name: assign-project

on:
  issues:
    types: [opened, labeled]
  pull_request:
    types: [opened, labeled]

jobs:
  add-issue-to-project:
    name: Add issue to project
    if: github.event_name == 'issues' && github.event.action == 'opened'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/add-to-project@v0.5.0
        with:
          project-url: https://github.com/orgs/pykale/projects/4
          github-token: ${{ secrets.ADD_TO_PROJECT_PAT }}

  add-pull-request-to-project:
    name: Add pull request to project
    if: github.event_name == 'pull_request' && github.event.action == 'opened'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/add-to-project@v0.5.0
        with:
          project-url: https://github.com/orgs/pykale/projects/4
          github-token: ${{ secrets.ADD_TO_PROJECT_PAT }}
</file>

<file path=".github/workflows/release.yml">
# This workflow will release a package on PyPI automatically when it is tagged/released.
# https://packaging.python.org/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/

name: release

on:
  release:
    types: [created]

jobs:
  build-n-publish:
    name: Release on PyPI
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@master
    - name: Set up Python 3.8
      uses: actions/setup-python@v1
      with:
        python-version: 3.8
    - name: Install pypa/build
      run: >-
        python -m
        pip install
        build
        --user
    - name: Build a binary wheel and a source tarball
      run: >-
        python -m
        build
        --sdist
        --wheel
        --outdir dist/
        .
    - name: Publish distribution to Test PyPI
      uses: pypa/gh-action-pypi-publish@master
      with:
        password: ${{ secrets.TEST_PYPI_API_TOKEN }}
        repository_url: https://test.pypi.org/legacy/
    - name: Publish distribution to PyPI
      uses: pypa/gh-action-pypi-publish@master
      with:
        password: ${{ secrets.PYPI_API_TOKEN }}
</file>

<file path=".github/workflows/stale.yml">
# This workflow will control the stale and close of prs and issues.
# For more information see: https://github.com/actions/stale

name: stale

on:
  schedule:
    # * is a special character in YAML, so you have to quote this string
    - cron:  '0 0 * * *' # every midnight

jobs:
  stale:
    name: Mark (and close) stale issues and pull requests
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write

    steps:
      - uses: actions/stale@v6
        with:
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          days-before-issue-stale: 30
          days-before-issue-close: 7
          days-before-pr-stale: 45
          days-before-pr-close: -1
          stale-issue-message: "This issue has been automatically marked as stale due to lack of activity. It will be closed if no further activity occurs. Thank you!"
          close-issue-message: "This issue is closed due to lack of activity. Feel free to reopen it if you still have questions."
          stale-pr-message: "This pull request has been automatically marked as stale due to lack of activity."
          stale-issue-label: "stale-issue"
          exempt-issue-labels: "bug, enhancement, new feature"
</file>

<file path=".github/workflows/test.yml">
# This workflow will install Python dependencies, run tests, and report the coverage with a variety of Python versions and OSs.
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # * is a special character in YAML, so you have to quote this string
    - cron:  '0 0 * * *' # every midnight

jobs:
  test:
    name: Test (${{ matrix.os }}, python version ${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ["3.8", "3.9", "3.10"] # list of Python versions to test
        exclude:
          - os: windows-latest
            python-version: "3.8"
          - os: windows-latest
            python-version: "3.9"
        include:
          - os: ubuntu-latest
            path: ~/.cache/pip
          - os: windows-latest
            path: ~\AppData\Local\pip\Cache

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python using Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: ${{ matrix.python-version }}
          miniconda-version: latest

      - name: Cache pip dependencies
        id: cache_pip
        uses: actions/cache@v4
        with:
          path: ${{ matrix.path }}
          key: ${{ runner.os }}-python${{ matrix.python-version }}-pip-20240705-${{ hashFiles('**/setup.py') }}
          restore-keys: |
            ${{ runner.os }}-python${{ matrix.python-version }}-pip-20240705-
          # We have used a softer matching strategy for the full hash of setup.py, as recommended by GitHub.
          # See: https://github.com/davronaliyev/Cache-dependencies-in-GitHub-Actions/blob/main/examples.md#python---pip
          # This restores the cache first and then downloads any changed packages to avoid updating the cache with
          # every change to the setup.py file, thus reducing the storage requirements of GitHub Action.
          # We set a date tag to the cache key to show the updated date of the cache. We can update this date tag to
          # generate new cache after every major changes in setup.py.

      - name: Install dependencies
        run: |
          pip install rdkit
          pip install gdown
          pip install pyparsing==3.0.9
          pip install torch==2.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
          pip install torchvision==0.18.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
          pip install torch_scatter==2.1.2 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html
          pip install torch_sparse==0.6.18 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html
          pip install torch_cluster==1.6.3 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html
          pip install torch_spline_conv==1.2.2 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html
          pip install torch_geometric==2.3.0
          pip install -e .[dev]
        shell: bash -l {0}

      - name: Cache downloaded test data
        id: cache_data
        uses: actions/cache@v4
        with:
          path: tests/test_data
          key: ${{ runner.os }}-python${{ matrix.python-version }}-data-${{ hashFiles('tests/download_test_data.py') }}
          restore-keys: |
            ${{ runner.os }}-python${{ matrix.python-version }}-data-${{ hashFiles('tests/download_test_data.py') }}
          # Use strict matching for the hash of download_test_data.py, as we want to update the cache whenever the file changes.

      - name: Download test data
        if: steps.cache_data.outputs.cache-hit != 'true'
        run : |
          python tests/download_test_data.py
        shell: bash -l {0}

      - name: Run tests
        id: run_tests
        run: |
          pytest --nbmake --nbmake-timeout=3000 --cov=kale
        shell: bash -l {0}

      - name: Determine coverage
        run: |
          coverage xml
        shell: bash -l {0}

      - name: Report coverage
        uses: codecov/codecov-action@v4
</file>

<file path=".github/CHANGELOG.md">
# Version  0.1.2

#### New Features

* [#348](https://github.com/pykale/pykale/pull/348): Create stale workflow
* [#356](https://github.com/pykale/pykale/pull/356): Update CIFAR CNNTransformer example to Lightning
* [#358](https://github.com/pykale/pykale/pull/358): Add option to log messages to terminal
* [#365](https://github.com/pykale/pykale/pull/365): Update project.yml for the new project board
* [#370](https://github.com/pykale/pykale/pull/370): Support py3.10 and remove py3.7

#### Bug Fixes

* [#372](https://github.com/pykale/pykale/pull/372): Update IPython version from <9.0 to <8.13 to support Python 3.8

#### Code Improvements

* [#345](https://github.com/pykale/pykale/pull/345): Add file path as a dicom dataset attribute
* [#346](https://github.com/pykale/pykale/pull/346): Update the polypharmacy example and GripNet implementation
* [#385](https://github.com/pykale/pykale/pull/385): Refactor input parameter of the data loader in PolypharmacyDataset class
* [#386](https://github.com/pykale/pykale/pull/386): Refactor documentation and resolve module visibility issues in PyKale

# Version  0.1.1

#### New Features

* [#338](https://github.com/pykale/pykale/pull/338): Improve GripNet implementation
* [#339](https://github.com/pykale/pykale/pull/339): Add setup options
* [#340](https://github.com/pykale/pykale/pull/340): Update reading DICOM and marker visualization

#### Code Improvements

* [#341](https://github.com/pykale/pykale/pull/341): Update Colab installation and add notebook hook
* [#342](https://github.com/pykale/pykale/pull/342): Add arguments to visualize and rename examples

#### Documentation Updates

* [#337](https://github.com/pykale/pykale/pull/337): Update GripNet example name and contributing guidelines
* [#343](https://github.com/pykale/pykale/pull/343): Clarify python version supported

# Version  0.1.0

#### New Features

* [#246](https://github.com/pykale/pykale/pull/246): Add MIDA, CoIRLS, distribution plot, and brain example

#### Bug Fixes

* [#322](https://github.com/pykale/pykale/pull/322): Add pre-commit dependency for black and click
* [#330](https://github.com/pykale/pykale/pull/330): Fix problems of tests for Python version 3.7, 3.8 and 3.9

#### Code Improvements

* [#284](https://github.com/pykale/pykale/pull/284): Update DICOM reading and image visualization
* [#320](https://github.com/pykale/pykale/pull/320): Add code scanning
* [#321](https://github.com/pykale/pykale/pull/321): Fix cardiac MRI example visualization number of columns
* [#331](https://github.com/pykale/pykale/pull/331): Update cmr example landmark visualization

#### Documentation Updates

* [#333](https://github.com/pykale/pykale/pull/333): Update docs and readme for 0.1.0 release

# Version  0.1.0rc5

#### New Features

* [#251](https://github.com/pykale/pykale/pull/251): MFSAN support 1D input
* [#273](https://github.com/pykale/pykale/pull/273): Add topk & multitask topk accuracies

#### Bug Fixes

* [#244](https://github.com/pykale/pykale/pull/244): Update getting indicies with torch.where
* [#254](https://github.com/pykale/pykale/pull/254): Fix bugs for upgrading PyTroch-lightning to 1.5
* [#256](https://github.com/pykale/pykale/pull/256) & [#257](https://github.com/pykale/pykale/pull/257): Update for PyTorch 1.10 and Torchvision 0.11.1
* [#286](https://github.com/pykale/pykale/pull/286): Update ipython requirement from <8.0 to <9.0

#### Code Improvements

* [#240](https://github.com/pykale/pykale/pull/240): Refractor the code to save the images instead of opening them at runtime
* [#271](https://github.com/pykale/pykale/pull/271): Fix doc build, improve docstrings and MPCA pipeline fit efficiency
* [#272](https://github.com/pykale/pykale/pull/272): Update progress_bar for PyTorch Lightning & change 'target' abbreviation
* [#283](https://github.com/pykale/pykale/pull/283): "val" in variable names to "valid"

#### Tests

* [#258](https://github.com/pykale/pykale/pull/258): Use pyparsing 2.4.7 in test

#### Documentation Updates

* [#228](https://github.com/pykale/pykale/pull/228): Zenodo json
* [#243](https://github.com/pykale/pykale/pull/243): Clarify PR template
* [#282](https://github.com/pykale/pykale/pull/282): Clarify when to request review and prefer just one label

# Version  0.1.0rc4

#### Code Improvements

* [#218](https://github.com/pykale/pykale/pull/218): Change logger in digits and action examples
* [#219](https://github.com/pykale/pykale/pull/219): Update three notebooks
* [#222](https://github.com/pykale/pykale/pull/222): Add multi source example
* [#224](https://github.com/pykale/pykale/pull/224): Merge all image accesses to a unique API

#### Tests

* [#221](https://github.com/pykale/pykale/pull/221): Add notebook "smoke tests" to CI

#### Documentation Updates

* [#225](https://github.com/pykale/pykale/pull/225): Update readme & fix colab imgaug
* [#229](https://github.com/pykale/pykale/pull/229): Add DOI to readme
* [#235](https://github.com/pykale/pykale/pull/235): Fix typo and hyperlink

# Version  0.1.0rc3

#### New Features

* [#196](https://github.com/pykale/pykale/pull/196): Add Google Drive Download API
* [#197](https://github.com/pykale/pykale/pull/197): Multi domain loader and office data access
* [#210](https://github.com/pykale/pykale/pull/210): Multi-source domain adaptation SOTA

#### Code Improvements

* [#201](https://github.com/pykale/pykale/pull/201): No "extras", only "normal" or "dev" installs

#### Tests

* [#178](https://github.com/pykale/pykale/pull/178): Reduce tests for video
* [#188](https://github.com/pykale/pykale/pull/188): Create download_path directory in conftest.py
* [#189](https://github.com/pykale/pykale/pull/189): Create test_sampler.py and update doc for tests
* [#200](https://github.com/pykale/pykale/pull/200): Nightly test run

#### Documentation Updates

* [#165](https://github.com/pykale/pykale/pull/165): Notebook tutorial for the bindingdb_deepdta example
* [#199](https://github.com/pykale/pykale/pull/199): CMR PAH notebook example
* [#207](https://github.com/pykale/pykale/pull/207): Restructure notebook tutorial docs
* [#212](https://github.com/pykale/pykale/pull/212): Describe use of YAML

#### Other Changes

* [#187](https://github.com/pykale/pykale/pull/187): Add dependabot
* [#205](https://github.com/pykale/pykale/pull/205): Update data dirs

# Version  0.1.0rc2

#### New Features

* [#149](https://github.com/pykale/pykale/pull/149): Add digits notebook with Binder and Colab
* [#151](https://github.com/pykale/pykale/pull/151): Add class subset selection
* [#159](https://github.com/pykale/pykale/pull/159): Add interpret module

#### Code Improvements

* [#132](https://github.com/pykale/pykale/pull/132): Create file download module
* [#138](https://github.com/pykale/pykale/pull/138): Change action_domain_adapter.py to video_domain_adapter.py
* [#144](https://github.com/pykale/pykale/pull/144): Move gait data to pykale/data
* [#157](https://github.com/pykale/pykale/pull/157): Add concord_index calculation into DeepDTA

#### Tests

* [#127](https://github.com/pykale/pykale/pull/127): Add video_access tests
* [#134](https://github.com/pykale/pykale/pull/134): Add tests for image and video CNNs
* [#136](https://github.com/pykale/pykale/pull/136): Add tests for domain adapter
* [#137](https://github.com/pykale/pykale/pull/137): Add tests for csv logger
* [#139](https://github.com/pykale/pykale/pull/139): Add tests for isonet
* [#145](https://github.com/pykale/pykale/pull/145): Add tests for video domain adapter
* [#150](https://github.com/pykale/pykale/pull/150): Add tests for gripnet
* [#156](https://github.com/pykale/pykale/pull/156): Remove empty tests and MNIST test

#### Documentation Updates

* [#133](https://github.com/pykale/pykale/pull/133): Add quote from Kevin@facebook
* [#143](https://github.com/pykale/pykale/pull/143): Add "new feature" group
* [#160](https://github.com/pykale/pykale/pull/160): Update docs w.r.t. CIKM submission

# Version  0.1.0rc1

**Important**: Rename `master` to `main`.

#### Code Improvements

* [#92](https://github.com/pykale/pykale/pull/92): Update action domain adaptation pipeline and modules (big PR)
* [#123](https://github.com/pykale/pykale/pull/123): Merge prep_cmr with image_transform plus tests

#### Tests

* [#104](https://github.com/pykale/pykale/pull/104): Test attention_cnn
* [#107](https://github.com/pykale/pykale/pull/107): Do only CI test multiple python versions on Linux
* [#122](https://github.com/pykale/pykale/pull/122): Test deep_dta

#### Documentation Updates

* [#106](https://github.com/pykale/pykale/pull/106): Update the readmes of docs, examples, and tests
* [#120](https://github.com/pykale/pykale/pull/120): Update PR for changelog, cherry pick, and test re-run tip
* [#121](https://github.com/pykale/pykale/pull/121): Update new logos
* [#125](https://github.com/pykale/pykale/pull/125): Update documentation, esp. guidance on how to use pykale

# Version  0.1.0b3

#### Code Improvements

* [#84](https://github.com/pykale/pykale/pull/84): Auto assign to the default project
* [#91](https://github.com/pykale/pykale/pull/91): MPCA pipeline
* [#93](https://github.com/pykale/pykale/pull/93): Fix black config and rerun
* [#97](https://github.com/pykale/pykale/pull/97): Add changelog CI and update logo

#### Dependencies

* [#82](https://github.com/pykale/pykale/pull/82): Remove requirements in examples and update setup

#### Tests

* [#70](https://github.com/pykale/pykale/pull/70): Add tests for utils.print
* [#80](https://github.com/pykale/pykale/pull/80): Extend automated test matrix and rename lint
* [#85](https://github.com/pykale/pykale/pull/85): Test utils logger
* [#87](https://github.com/pykale/pykale/pull/87): Test cifar/digit_access and downgrade black
* [#90](https://github.com/pykale/pykale/pull/90): Test mpca
* [#94](https://github.com/pykale/pykale/pull/94): Update test guidelines

#### Documentation Updates

* [#81](https://github.com/pykale/pykale/pull/81): Docs update version and installation
* [#88](https://github.com/pykale/pykale/pull/88): Automatically sort documented members by source order
* [#89](https://github.com/pykale/pykale/pull/89): Disable automatic docstring inheritance from parent class


# Changelog

This changelog is updated for each new release.
</file>

<file path=".github/CODE_OF_CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at pykale-group@sheffield.ac.uk. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq
</file>

<file path=".github/CONTRIBUTING.md">
# Contributing to PyKale

[Light involvements (viewers/users)](#light-involvements-viewersusers) |
[*Medium involvements (contributors)*](#medium-involvements-contributors) |
[**Heavy involvements (maintainers)**](#heavy-involvements-maintainers)

[Ask questions](#ask-questions) |
[Report bugs](#report-bugs) |
[Suggest improvements](#suggest-improvements) |
[*Branch, fork & pull*](#branch-fork-and-pull) |
[*Coding style*](#coding-style) |
[*Test*](#testing) |
[Review & merge](#review-and-merge-pull-requests) |
[Release & management](#release-and-management)

Thank you for your interest! You can contribute to the PyKale project in a wide range of ways listed above, from light to heavy involvements. You can also reach us via <a href="mailto:pykale-group&#64;sheffield.ac.uk">email</a> if needed. The participation in this open source project is subject to [Code of Conduct](https://github.com/pykale/pykale/blob/main/CODE_OF_CONDUCT.md).

## Light involvements (viewers/users)

See the [ReadMe](https://github.com/pykale/pykale/blob/main/README.md) for installation instructions. Your contribution can start as light as asking questions.

### Ask questions

Ask any questions about PyKale on the [PyKale's GitHub Discussions tab](https://github.com/pykale/pykale/discussions) and we will discuss and answer you there. Questions help us identify *blind spots* in our development and can greatly improve the code quality.

### Report bugs

Search current issues to see whether they are already reported. If not, report bugs by [creating issues](https://github.com/pykale/pykale/issues) using the provided template. Even better, if you know how to fix them, make the suggestions and/or propose changes with [pull requests](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/proposing-changes-to-your-work-with-pull-requests).

### Suggest improvements

Suggest possible improvements such as new features or code refactoring by [creating issues](https://github.com/pykale/pykale/issues) using the respective templates. Even better, you are welcome to propose such changes with [pull requests](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/proposing-changes-to-your-work-with-pull-requests).

## Medium involvements (contributors)

We follow PyTorch to use **US English spelling** and recommend spell check via [Grazie](https://github.com/JetBrains/intellij-community/tree/master/plugins/grazie) in PyCharm and [Code Spell Checker](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker) in VS code with US English setting.

### Branch, fork and pull

A maintainer with *write* access can [create a branch](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository) directly here in `pykale` to make changes under the [shared repository model](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-collaborative-development-models),  following the steps below while skipping the fork step.

Anyone can use the [*fork and pull* model](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-collaborative-development-models) to contribute code to PyKale:

- [**Fork**](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) pykale (also see the [guide on forking projects](https://guides.github.com/activities/forking/)).
  - Keep the fork main branch synced with `pykale:main` by [syncing a fork](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork).
  - Install `pre-commit` to enforce style via `pip install pre-commit` and `pre-commit install` at the root.
- [Create a branch](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository) based on the *latest main* in your fork with a *descriptive* name on what you plan to do, e.g. to fix an issue, starting with the issue ticket number.
  - Make changes to this branch using detailed commit messages and following the [coding style](#coding-style) below. In particular, do [**frequent commits**](https://docs.github.com/en/actions/guides/about-continuous-integration#about-continuous-integration) and **small-scale pull requests** to make them more focused and easier to review.
  - [Sync your branch](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/syncing-your-branch) with the main frequently so that potential problems can be identified earlier.
  - Document the update in [Google Style Python Docstrings](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html). Update `docs` following [docs update steps](https://github.com/pykale/pykale/tree/main/docs). Build `docs` via `make html` and verify locally built documentations under `docs\build\html`.
  - Build tests and do tests (not enforced yet, to be done).
- Create a [draft pull request](https://github.blog/2019-02-14-introducing-draft-pull-requests/) or [pull request](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) or from the task branch above to the main branch `pykale:main` explaining the changes, add an appropriate label or more, and choose one reviewer or more, using a [template](#pull-request-template).
  - If merged, the *title* of your PR (typically start with a *verb*) will automatically become part of the [changelog](https://github.com/pykale/pykale/blob/main/.github/CHANGELOG.md) in the next release, where the *label* of your PR will be used to group the changes into categories. Make the title and label precise and descriptive.
  - A draft pull request helps start a conversation with collaborators in a draft state. It will not be reviewed or merged until you change the status to ‚ÄúReady for review‚Äù near the bottom of your pull request.
  - View the [continuous integration (CI) status checks](https://github.com/pykale/pykale/actions) to fix the found problems. Some [`test` actions](https://github.com/pykale/pykale/actions/workflows/test.yml) may fail/cancel due to server reasons, particularly `Test (macos-latest, ...)`. In such cases, [re-run the `test` workflow](https://docs.github.com/en/actions/managing-workflow-runs/re-running-a-workflow) later can usually pass. Also, when the check messages say files are changed, they mean changes in the simulated environment, *NOT* on the branch.
  - You need to [address merge conflicts](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/addressing-merge-conflicts) if they arise. Resolve the conflicts locally.
  - After passing all CI checks and resolving the conflicts, you should [request a review](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/requesting-a-pull-request-review). If you know who is appropriate or like the [suggested reviewers](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/requesting-a-pull-request-review#:~:text=Suggested%20reviewers%20are%20based%20on,review%20from%20the%20same%20reviewer.), request/assign that person. Otherwise, we will assign one shortly.
  - A reviewer will follow the [review and merge guidelines](#review-and-merge-pull-requests). The reviewer may discuss with you and request explanations/changes before merging.
  - Merging to the main branch **requires** *ALL checks to pass* AND *at least one approving review*.
  - Small pull requests are preferred for easier review. In exceptional cases of a long branch with a large number of commits in a PR, you may consider breaking it into several smaller branches and PRs, e.g. via [git-cherry-pick](https://git-scm.com/docs/git-cherry-pick), for which a [video](https://youtu.be/h8XnBRZEPYI) is available to help.

#### Before pull requests: pre-commit hooks

We set up several  [`pre-commit`](https://pre-commit.com/) hooks to ensure code quality, including

- Linting tools: [flake8](https://gitlab.com/pycqa/flake8), [black](https://github.com/psf/black), and [isort](https://github.com/timothycrosley/isort).
- Static type analysis: [mypy](https://github.com/python/mypy) (to do, not yet active)
- Other hooks as specified in [`.pre-commit-config.yaml`](https://github.com/pykale/pykale/blob/main/.pre-commit-config.yaml), such as restricting the largest file size to 300KB.

You need to install pre-commit and the hooks from the root directory via

```bash
pip install pre-commit
pre-commit install
```

This will install the `pre-commit` hooks at `pykale\.git\hooks`, to be **triggered by each new commit** to automatically run them *over the files you commit*. In this way, problems can be detected and fixed early. Several **important** points to note:

- Pre-commit hooks are configured in [`.pre-commit-config.yaml`](https://github.com/pykale/pykale/blob/main/.pre-commit-config.yaml). Only administrator should modify it.
- These hooks, e.g.,  [black](https://black.readthedocs.io/en/stable/index.html) and [isort](https://pycqa.github.io/isort/), will **automatically fix** some problems for you by **changing the files**, so please check the changes after you trigger `commit`.
- If your commits can not pass the above checks, read the error message to see what has been automatically fixed and what needs your manual fix, e.g. flake8 errors. Some flake8 errors may be fixed by some hooks so you can rerun the pre-commit (e.g. re-commit to trigger it) or just run flake8 to see the updated flake8 errors.
- If your commits can not pass the check for added large files and see the error message of `json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)`, try to upgrade your `git` to a version >= 2.29.2 to fix it.

#### Manual checks and fixes (be *CAREFUL*)

Required libraries will be automatically installed but if you wish, you may install them manually and run them **from the root directory** (so that the PyKale configurations are used). For example,

```bash
pip install black # The first time
black ./kale/embed/new_module.py # "black ." do it for all files
pip install isort # The first time
isort ./kale/embed/new_module.py # "isort ." do it for all files
pip install flake8 # The first time
flake8 ./kale/embed/new_module.py # "flake8 ." do it for all files
```

Run [black](https://black.readthedocs.io/en/stable/index.html) and [isort](https://pycqa.github.io/isort/) will fix the found problems automatically by modifying the files but they will be automatically run and you do *not* need to do it manually. Remaining [flake8](https://flake8.pycqa.org/en/latest/) or other errors need to be manually fixed.

**Important**: Run these commands from the root directory so that the PyKale configuration files ([`setup.cfg`](https://github.com/pykale/pykale/blob/main/setup.cfg), [`pyproject.toml`](https://github.com/pykale/pykale/blob/main/pyproject.toml), and [`.pre-commit-config.yaml`](https://github.com/pykale/pykale/blob/main/.pre-commit-config.yaml)) are used for these tools. Otherwise, the default configurations will be used, which **differ** from the PyKale configurations and are consistent.

**IDE integration**: flake8 linting can be set up for both [VSCode](https://code.visualstudio.com/docs/python/linting) and [PyCharm](https://tirinox.ru/flake8-pycharm/) but you must use [`setup.cfg`](https://github.com/pykale/pykale/blob/main/setup.cfg) to configure it. In this way, you could fix linting errors on the go.

#### Automated GitHub workflows (continuous integration)

For continuous integration (CI) and continuous deployment (CD), we use several [GitHub workflows (actions)](https://github.com/pykale/pykale/actions) that will be triggered upon a push or pull request as specified at [`pykale/.github/workflows/`](https://github.com/pykale/pykale/tree/main/.github/workflows)

- Build: install Python dependencies (set up)
- Linting: run flake8 and pre-commit
- Tests: unit and regression tests (in progress)

We will make the above more complete and rigorous, e.g. with more tests and code coverage analysis etc.

#### Pull request template

We have a pull request template. Please use it for all pull requests and mark the status of your pull requests.

- **Ready**: ready for review and merge (if no problems found). Reviewers will be assigned.
- **Work in progress**: for core team's awareness of this development (e.g. to avoid duplicated efforts) and possible feedback (e.g. to find problems early, such as linting/CI issues). Not ready to merge yet. Change it to **Ready** when ready to merge.
- **Hold**: not for attention yet.

### Coding style

We aim to design the core `kale` modules to be highly **reusable**, generic, and customizable, and follow these guidelines:

- Follow the [continuous integration practice](https://docs.github.com/en/actions/guides/about-continuous-integration#about-continuous-integration) to make small changes and commit frequently with clear descriptions for others to understand what you have done. This can detect errors sooner, reduces debug need, make it easier to merge changes, and eventually save the overall time.
- Use highly *readable* names for variables, functions, and classes. Using *verbs* is preferred when feasible for compactness. Use spell check with **US** English setting, e.g., [Grazie](https://github.com/JetBrains/intellij-community/tree/master/plugins/grazie) in PyCharm and [Code Spell Checker](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker) in VS code.
- Use [`logging`](https://docs.python.org/3/howto/logging.html#logging-basic-tutorial) instead of `print` to log messages. Users can choose the level via, e.g., `logging.getLogger().setLevel(logging.INFO)`. See the [benefits](https://stackoverflow.com/questions/6918493/in-python-why-use-logging-instead-of-print).
- Include detailed docstrings in code for generating documentations, following the [Google Style Python Docstrings](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).
  - The compiling of `\\` and `\` in maths and Greek letters in docstring is messy. So, if contributors encounter problems or errors when using maths and Greek letters, please try to switch between `\\` and `\`.
- Highly reusable modules should go into `kale`. Highly data/example-specific code goes into `Examples`.
- Configure learning systems using [YAML](https://en.wikipedia.org/wiki/YAML) following [YACS](https://github.com/rbgirshick/yacs). See our [examples](https://github.com/pykale/pykale/tree/main/examples).
- Use [PyTorch](https://pytorch.org/tutorials/) and [PyTorch Lightning](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) ([Video](https://www.youtube.com/watch?v=QHww1JH7IDU)) as much as possible.
- If high-quality existing code from other sources are used, add credit and license information at the top of the file.
- Use pre-commit hooks to enforce consistent styles via [flake8](https://gitlab.com/pycqa/flake8), [black](https://github.com/psf/black), and [isort](https://github.com/timothycrosley/isort)), with common PyKale configuration files.

#### Recommended development software

- Python IDE: [Visual Studio Code](https://code.visualstudio.com/download), [PyCharm](https://www.jetbrains.com/pycharm/download/)
- GitHub: [GitHub Desktop (for Windows/Mac)](https://desktop.github.com/), [GitKraken (for Linux)](https://www.gitkraken.com/), [GitHub guides](https://guides.github.com/), [GitHub documentations](https://docs.github.com/en)

### Testing

All new code should be covered by tests following the [`pykale` test guidelines](https://github.com/pykale/pykale/blob/main/tests/README.md).

## Heavy involvements (maintainers)

### Review and merge pull requests

A maintainer assigned to review a pull request should follow GitHub guidelines on how to [review changes in pull requests](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/reviewing-changes-in-pull-requests) and [incorporate changes from a pull request](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/incorporating-changes-from-a-pull-request) to review and merge the pull requests. Merging can be automated (see [Automation](#automation)), in which case an approving review will trigger the merging. You should NOT approve the changes if they are not ready to merge.

If you think you are not the right person to review, let the administrator (haipinglu) know for a reassignment. If multiple reviewers are assigned, anyone can approve and merge unless more approvals are explicitly required.

For simple problems, such as typos, hyperlinks, the reviewers can fix it directly and push the changes rather than comment and wait for the author to fix. This will speed up the development.

### Release and management

The release will be done manually in GitHub, but with automatic upload to PyPI.

#### Versions

We follow the [Semantic Versioning](https://semver.org/) guidelines. Given a version number `MAJOR.MINOR.PATCH`, increment the:

- MAJOR version when you make incompatible API changes,
- MINOR version when you add functionality in a backwards compatible manner, and
- PATCH version when you make backwards compatible bug fixes.

Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.

#### Project boards

We set up [project boards](https://github.com/pykale/pykale/projects) to manage the progress of development. A single default project contains all active/planned works, with automation.

#### Automation

We have adopted the GitHub automations including

- [Automerge](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/automatically-merging-a-pull-request): merges automatically when 1) one approving review is completed; 2) all CI checks have passed; and 3) one maintainer has **enabled the auto-merge** for a PR.
- [Auto branch deletion](https://github.blog/changelog/2019-07-31-automatically-delete-head-branches-of-pull-requests/): deletes the head branches automatically after pull requests are merged. Deleted branches can be restored if needed.
- [Project board automation](https://docs.github.com/en/github/managing-your-work-on-github/about-automation-for-project-boards): automates project board card management.

## References

The following libraries from the [PyTorch ecosystem](https://pytorch.org/ecosystem/) are good resources to learn from:

- [**PyTorchLightning**](https://github.com/PyTorchLightning/pytorch-lightning): a lightweight PyTorch wrapper for high-performance AI research
- [GPyTorch](https://github.com/cornellius-gp/gpytorch): a highly efficient and modular implementation of Gaussian processes in PyTorch
- [Kornia](https://github.com/kornia/kornia): computer vision library for PyTorch by the OpenCV team
- [MONAI](https://github.com/Project-MONAI/MONAI): deep learning-based healthcare imaging workflows
- [PyTorch_Geometric](https://github.com/rusty1s/pytorch_geometric): deep learning library for graphs
- [TensorLy](https://github.com/tensorly/tensorly): a library for tensor learning in Python
- [Torchio](https://github.com/fepegar/torchio): medical image pre-processing and augmentation toolkit for deep learning
</file>

<file path=".github/dependabot.yml">
# https://docs.github.com/en/github/administering-a-repository/configuration-options-for-dependency-updates
version: 2
updates:
  - package-ecosystem: pip
    directory: /
    schedule:
      interval: weekly
</file>

<file path=".github/FUNDING.yml">
# These are supported funding model platforms

github: [haipinglu] # Replace with up to 4 GitHub Sponsors-enabled usernames e.g., [user1, user2]
</file>

<file path=".github/pull_request_template.md">
Fixes #{issue_number}.

### Description
A few sentences describing the changes proposed in this pull request.

### Status
**Ready/Work in progress/Hold**

If you've been given access to pykale with role Triage or above, on the right (delete these after selection):

- Select **one most appropriate** label.
- **When** your pull request is **ready** for review, select a reviewer. Use the suggested one if unsure.

If you cannot see an option to select a reviewer/label, that means one maintainer will get notified upon your pull request and then select for you.

### Types of changes
<!--- Put an `x` in all the boxes that apply, and remove the not applicable items -->
- [x] Non-breaking change (fix or new feature that would not break existing functionality).
- [ ] Breaking change (fix or new feature that would cause existing functionality to change).
- [ ] New tests added to cover the changes.
- [ ] In-line docstrings updated.
- [ ] [Source for documentation at `docs`](https://github.com/pykale/pykale/tree/main/docs/source) manually updated for new API.
</file>

<file path="docs/source/conf.py">
# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import re
import sys
from io import open
from os import path

sys.path.insert(0, path.abspath(path.join(path.dirname(__file__), "../..")))


# Get version
def read(*names, **kwargs):
    with open(path.join(path.dirname(__file__), "..", "..", *names), encoding=kwargs.get("encoding", "utf8")) as fp:
        return fp.read()


def find_version(*file_paths):
    version_file = read(*file_paths)
    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", version_file, re.M)
    if version_match:
        return version_match.group(1)
    raise RuntimeError("Unable to find version string.")


version = find_version("kale", "__init__.py")

# -- Project information -----------------------------------------------------

project = "PyKale"
copyright = "2020 - 2021 PyKale Contributors"
author = "PyKale Contributors"

# The full version, including alpha/beta/rc tags
release = version

# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.

source_suffix = {".rst": "restructuredtext", ".txt": "restructuredtext", ".md": "markdown"}

extensions = [
    "recommonmark",
    "sphinx_rtd_theme",
    "sphinx.ext.napoleon",
    "sphinx.ext.autosummary",
    "sphinx.ext.intersphinx",
    "nbsphinx",
    "nbsphinx_link",
    "sphinx.ext.mathjax",
    "sphinx.ext.autodoc",
    "sphinx_markdown_tables",
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ["_build", "**.ipynb_checkpoints"]


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
# html_theme = 'alabaster'
html_theme = "sphinx_rtd_theme"

# Automatically documented members are sorted by source order.
autodoc_member_order = "bysource"

# Disable automatic docstring inheritance from parents.
autodoc_inherit_docstrings = False

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]
</file>

<file path="docs/source/index.rst">
.. PyKale documentation master file, created by
   sphinx-quickstart on Wed Jul 29 22:39:23 2020.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.


PyKale Documentation
==================================

.. toctree::
    :maxdepth: 1
    :caption: Getting Started

    introduction
    installation
    tutorial
    notebooks
    yaml

.. toctree::
    :maxdepth: 1
    :caption: Kale API

    kale.loaddata
    kale.prepdata
    kale.embed
    kale.predict
    kale.evaluate
    kale.interpret
    kale.pipeline
    kale.utils

Kale APIs above are ordered following the machine learning pipeline, i.e., functionalities, rather than alphabetically.

.. toctree::
    :maxdepth: 1
    :caption: Example Projects

    Action Video - Domain Adaptation <https://github.com/pykale/pykale/tree/main/examples/action_dann>
    Autism fMRI - Domain Adaptation <https://github.com/pykale/pykale/tree/main/examples/multisite_neuroimg_adapt>
    AVMNIST - Multimodal Neural Network <https://github.com/pykale/pykale/tree/main/examples/avmnist_multimodal>
    BindingDB - DeepDTA <https://github.com/pykale/pykale/tree/main/examples/bindingdb_deepdta>
    CardiacMRI - MPCA <https://github.com/pykale/pykale/tree/main/examples/cmri_mpca>
    CIFAR - CNN Transformer <https://github.com/pykale/pykale/tree/main/examples/cifar_cnntransformer>
    CIFAR - ISONet <https://github.com/pykale/pykale/tree/main/examples/cifar_isonet>
    Digits - Domain Adaptation <https://github.com/pykale/pykale/tree/main/examples/digits_dann>
    Digits/Office - Multisource Adaptation <https://github.com/pykale/pykale/tree/main/examples/office_multisource_adapt>
    Multiomics - MOGONET <https://github.com/pykale/pykale/tree/main/examples/multiomics_mogonet>
    Toy Data - Domain Adaptation <https://github.com/pykale/pykale/tree/main/examples/toy_domain_adaptation>
    Polypharmacy - GripNet <https://github.com/pykale/pykale/tree/main/examples/polypharmacy_gripnet>
    Video - Data Loading <https://github.com/pykale/pykale/tree/main/examples/video_loading>

.. To study later the best way to document examples
.. examples/examples.cifar_cnntransformer
.. examples/examples.cifar_isonet
.. examples/examples.digits_dann

.. .. toctree::
..     :maxdepth: 1
..     :caption: Notebooks

.. examples/CMR_PAH.nblink
    "path": "../../examples/cmri_mpca/CMR_PAH.ipynb"

Indices and Tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

.. .. |digits_mybinder| image:: https://mybinder.org/badge_logo.svg
..     :target: https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Fdigits_dann%2Ftutorial.ipynb


.. .. |digits_colab| image:: https://colab.research.google.com/assets/colab-badge.svg
..     :target: https://colab.research.google.com/github/pykale/pykale/blob/main/examples/digits_dann/tutorial.ipynb
</file>

<file path="docs/source/installation.md">
# Installation

## Requirements

PyKale requires Python 3.8, 3.9, or 3.10. Before installing pykale, you should
- manually [install PyTorch](https://pytorch.org/get-started/locally/) matching your hardware first,
- if you will use APIs related to graphs, you need to manually install [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) first following its [official instructions](https://github.com/rusty1s/pytorch_geometric#installation) and matching your PyTorch installation, and
- If [RDKit](https://www.rdkit.org/) will be used, you need to install it via `pip install rdkit`.

## Pip install

Install PyKale using `pip` for the stable version:

```bash
pip install pykale  # for the core API only
```

## Install from source

Install from source for the latest version and/or development:

```sh
git clone https://github.com/pykale/pykale
cd pykale
pip install .  # for the core API only
pip install -e .[dev]  # editable install for developers including all dependencies and examples
```

## Installation options

PyKale provides six installation options for different user needs:

- `default`: `pip install pykale` for essential functionality
- `graph`: `pip install pykale[graph]` for graph-related functionality (e.g., [TDC](https://tdcommons.ai/))
- `image`: `pip install pykale[image]` for image-related functionality (e.g., [DICOM](https://en.wikipedia.org/wiki/DICOM))
- `example`: `pip install pykale[example]` for examples and tutorials
- `full`: `pip install pykale[full]` for all functionality, including examples and tutorials
- `dev`: `pip install pykale[dev]` for development, including all functionality, examples, and tutorials

Multiple options can be chosen by separating them with commas (without whitespace). See examples below.

```sh
pip install pykale[graph,example]
pip install pykale[graph,image]
pip install pykale[graph,image,example]
```

## Tests

For local unit tests on all `kale` API, you need to have PyTorch, PyTorch Geometric, and RDKit installed (see the top) and then run [pytest](https://pytest.org/) at the root directory:

```bash
pytest
```

You can also run pytest on individual module (see [pytest documentation](https://docs.pytest.org/en/6.2.x/)).
</file>

<file path="docs/source/introduction.md">
# Introduction

PyKale is a **Py**thon library for **k**nowledge-**a**ware machine **le**arning from multiple sources, particularly from multiple modalities for [multimodal learning](https://en.wikipedia.org/wiki/Multimodal_learning) and from multiple domains for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). This library was motivated by needs in healthcare applications (hence we choose the acronym *kale*, a healthy vegetable) and aims to enable and accelerate interdisciplinary research.

## Objectives

Our objectives are to build *green* machine learning systems.

- *Reduce repetition and redundancy*:  refactor code to standardize workflow and enforce styles, and identify and remove duplicated functionalities
- *Reuse existing resources*: reuse the same machine learning pipeline for different data, and reuse existing libraries for available functionalities
- *Recycle learning models across areas*: identify commonalities between applications, and recycle models for one application to another

## API design

To achieve the above objectives, we

- design our API to be pipeline-based to unify the workflow and increase the flexibility, and
- follow core principles of standardization and minimalism in the development.

This design helps us break barriers between different areas or applications and facilitate the fusion and nurture of ideas across discipline boundaries.

## Development

We have Research Software Engineers (RSEs) on our team to help us adopt the best software engineering practices in a research context. We have modern GitHub setup with project boards, discussion, and GitHub actions/workflows. Our repository has automation and continuous integration to build documentation, do linting of our code, perform pre-commit checks (e.g. maximum file size), use [pytest](https://docs.pytest.org/en/6.2.x/) for testing and [codecov](https://about.codecov.io/) for analysis of testing.
</file>

<file path="docs/source/kale.embed.rst">
.. kale.embed package

Embed
==================

Submodules
----------

kale.embed.attention\_cnn module
--------------------------------

.. automodule:: kale.embed.attention_cnn
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.factorization module
-------------------------------

.. automodule:: kale.embed.factorization
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.feature\_fusion module
---------------------------------

.. automodule:: kale.embed.feature_fusion
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.gcn module
---------------------

.. automodule:: kale.embed.gcn
   :members:
   :undoc-members:
   :show-inheritance:
   :exclude-members: message, update

kale.embed.gripnet module
-------------------------

.. automodule:: kale.embed.gripnet
   :members:
   :undoc-members:
   :show-inheritance:
   :exclude-members: message, update


kale.embed.image\_cnn module
----------------------------

.. automodule:: kale.embed.image_cnn
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.mogonet module
-------------------------

.. automodule:: kale.embed.mogonet
    :members:
    :undoc-members:
    :show-inheritance:


kale.embed.positional\_encoding module
--------------------------------------

.. automodule:: kale.embed.positional_encoding
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.seq\_nn module
-------------------------

.. automodule:: kale.embed.seq_nn
   :members:
   :undoc-members:
   :show-inheritance:
   :exclude-members:

kale.embed.uncertainty\_fitting module
--------------------------------------

.. automodule:: kale.embed.uncertainty_fitting
   :members:
   :undoc-members:
   :show-inheritance:
   :exclude-members:

kale.embed.video\_feature\_extractor module
-------------------------------------------

.. automodule:: kale.embed.video_feature_extractor
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.video\_i3d module
----------------------------

.. automodule:: kale.embed.video_i3d
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.video\_res3d module
------------------------------

.. automodule:: kale.embed.video_res3d
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.video\_se\_i3d module
--------------------------------

.. automodule:: kale.embed.video_se_i3d
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.video\_se\_res3d module
----------------------------------

.. automodule:: kale.embed.video_se_res3d
   :members:
   :undoc-members:
   :show-inheritance:

kale.embed.video\_selayer module
--------------------------------

.. automodule:: kale.embed.video_selayer
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: kale.embed
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.evaluate.rst">
.. kale.evaluate package

Evaluate
==================

Submodules
----------

kale.evaluate.cross\_validation module
--------------------------------------

.. automodule:: kale.evaluate.cross_validation
   :members:
   :undoc-members:
   :show-inheritance:

kale.evaluate.metrics module
----------------------------

.. automodule:: kale.evaluate.metrics
   :members:
   :undoc-members:
   :show-inheritance:

kale.evaluate.similarity\_metrics
---------------------------------

.. automodule:: kale.evaluate.similarity_metrics
   :members:
   :undoc-members:
   :show-inheritance:

kale.evaluate.uncertainty\_metrics
----------------------------------

.. automodule:: kale.evaluate.uncertainty_metrics
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: kale.evaluate
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.interpret.rst">
.. kale.interpret package

Interpret
=====================

Submodules
----------

kale.interpret.model\_weights module
------------------------------------

.. automodule:: kale.interpret.model_weights
   :members:
   :undoc-members:
   :show-inheritance:


kale.interpret.uncertainty\_quantiles module
--------------------------------------------

.. automodule:: kale.interpret.uncertainty_quantiles
   :members:
   :undoc-members:
   :show-inheritance:

kale.interpret.visualize module
-------------------------------

.. automodule:: kale.interpret.visualize
   :members:
   :undoc-members:
   :show-inheritance:



Module contents
---------------

.. automodule:: kale.interpret
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.loaddata.rst">
.. kale.loaddata package

Load Data
=====================

Submodules
----------

kale.loaddata.avmnist\_datasets module
--------------------------------------

.. automodule:: kale.loaddata.avmnist_datasets
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.dataset\_access module
------------------------------------

.. automodule:: kale.loaddata.dataset_access
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.image\_access module
----------------------------------

.. automodule:: kale.loaddata.image_access
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.mnistm module
---------------------------

.. automodule:: kale.loaddata.mnistm
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.multi\_domain module
----------------------------------

.. automodule:: kale.loaddata.multi_domain
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.multiomics\_datasets module
-----------------------------------------

.. automodule:: kale.loaddata.multiomics_datasets
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.polypharmacy\_datasets module
-------------------------------------------

.. automodule:: kale.loaddata.polypharmacy_datasets
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.sampler module
----------------------------

.. automodule:: kale.loaddata.sampler
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.tabular\_access module
------------------------------------

.. automodule:: kale.loaddata.tabular_access
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.tdc\_datasets module
----------------------------------

.. automodule:: kale.loaddata.tdc_datasets
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.usps module
-------------------------

.. automodule:: kale.loaddata.usps
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.video\_access module
----------------------------------

.. automodule:: kale.loaddata.video_access
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.video\_datasets module
------------------------------------

.. automodule:: kale.loaddata.video_datasets
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.video\_multi\_domain module
-----------------------------------------

.. automodule:: kale.loaddata.video_multi_domain
   :members:
   :undoc-members:
   :show-inheritance:

kale.loaddata.videos module
---------------------------

.. automodule:: kale.loaddata.videos
   :members:
   :exclude-members: VideoRecord
   :show-inheritance:

kale.loaddata.few\_shot module
---------------------------

.. automodule:: kale.loaddata.few_shot
   :members:
   :exclude-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: kale.loaddata
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.pipeline.rst">
.. kale.pipeline package

Pipeline
=====================

Submodules
----------

kale.pipeline.base\_nn\_trainer module
--------------------------------------

.. automodule:: kale.pipeline.base_nn_trainer
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.deepdta module
----------------------------

.. automodule:: kale.pipeline.deepdta
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.domain\_adapter module
------------------------------------

.. automodule:: kale.pipeline.domain_adapter
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.mpca\_trainer module
----------------------------------

.. automodule:: kale.pipeline.mpca_trainer
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.multi\_domain\_adapter module
-------------------------------------------

.. automodule:: kale.pipeline.multi_domain_adapter
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.multiomics\_trainer module
----------------------------------------

.. automodule:: kale.pipeline.multiomics_trainer
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.video\_domain\_adapter module
-------------------------------------------

.. automodule:: kale.pipeline.video_domain_adapter
   :members:
   :undoc-members:
   :show-inheritance:

kale.pipeline.fewshot\_trainer module
-------------------------------------------

.. automodule:: kale.pipeline.fewshot_trainer
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: kale.pipeline
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.predict.rst">
.. kale.predict package

Predict
====================

Submodules
----------

kale.predict.class\_domain\_nets module
---------------------------------------

.. automodule:: kale.predict.class_domain_nets
   :members:
   :undoc-members:
   :show-inheritance:

kale.predict.decode module
--------------------------

.. automodule:: kale.predict.decode
   :members:
   :undoc-members:
   :show-inheritance:

kale.predict.isonet module
--------------------------

.. automodule:: kale.predict.isonet
   :members:
   :undoc-members:
   :show-inheritance:


kale.predict.uncertainty\_binning
----------------------------------

.. automodule:: kale.predict.uncertainty_binning
   :members:
   :undoc-members:
   :show-inheritance:

Module contents
---------------

.. automodule:: kale.predict
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/kale.prepdata.rst">
.. kale.prepdata package

Preprocess Data
=====================

Submodules
----------

kale.prepdata.chem\_transform module
------------------------------------

.. automodule:: kale.prepdata.chem_transform
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.graph\_negative\_sampling module
----------------------------------------------

.. automodule:: kale.prepdata.graph_negative_sampling
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.image\_transform module
-------------------------------------

.. automodule:: kale.prepdata.image_transform
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.string\_transform module
--------------------------------------

.. automodule:: kale.prepdata.string_transform
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.supergraph\_construct module
------------------------------------------

.. automodule:: kale.prepdata.supergraph_construct
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.tabular\_transform module
---------------------------------------

.. automodule:: kale.prepdata.tabular_transform
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.tensor\_reshape module
------------------------------------

.. automodule:: kale.prepdata.tensor_reshape
   :members:
   :undoc-members:
   :show-inheritance:

kale.prepdata.video\_transform module
-------------------------------------

.. automodule:: kale.prepdata.video_transform
    :members:
    :exclude-members:
    :show-inheritance:

Module contents
---------------

.. automodule:: kale.prepdata
    :members:
    :undoc-members:
    :show-inheritance:
</file>

<file path="docs/source/kale.utils.rst">
.. kale.utils package

Utilities
==================

Submodules
----------

kale.utils.download module
--------------------------

.. automodule:: kale.utils.download
   :members:
   :undoc-members:
   :show-inheritance:

kale.utils.initialize_nn module
-------------------------------

.. automodule:: kale.utils.initialize_nn
   :members:
   :undoc-members:
   :show-inheritance:

kale.utils.logger module
------------------------

.. automodule:: kale.utils.logger
   :members:
   :undoc-members:
   :show-inheritance:

kale.utils.print module
-----------------------

.. automodule:: kale.utils.print
   :members:
   :undoc-members:
   :show-inheritance:

kale.utils.save\_xlsx module
----------------------------

.. automodule:: kale.utils.save_xlsx
   :members:
   :undoc-members:
   :show-inheritance:

kale.utils.seed module
----------------------

.. automodule:: kale.utils.seed
   :members:
   :undoc-members:
   :show-inheritance:


Module contents
---------------

.. automodule:: kale.utils
   :members:
   :undoc-members:
   :show-inheritance:
</file>

<file path="docs/source/notebooks.md">
# Jupyter Notebook Tutorials

- **Autism detection**: [Domain adaptation on multi-site imaging data for autism detection](https://github.com/pykale/pykale/blob/main/examples/multisite_neuroimg_adapt/tutorial.ipynb)

- **Cardiovascular disease diagnosis**: [Cardiac MRI for MPCA-based diagnosis](https://github.com/pykale/pykale/blob/main/examples/cmri_mpca/tutorial.ipynb)

- **Drug discovery**: [BindingDB drug-target interaction prediction](https://github.com/pykale/pykale/blob/main/examples/bindingdb_deepdta/tutorial.ipynb)

- **Image classification**: [Digits domain adaptation](https://github.com/pykale/pykale/blob/main/examples/digits_dann/tutorial.ipynb)

- **Toy data classification**: [Toy data domain adaptation](https://github.com/pykale/pykale/blob/main/examples/toy_domain_adaptation/tutorial.ipynb)
</file>

<file path="docs/source/tutorial.md">
# Tutorial

For *interactive* tutorials, see [Jupyter Notebook tutorials](notebooks.md).

## Usage of Pipeline-based API in Examples

The `kale` API has a unique pipeline-based API design. Each example typically has three essential modules (`main.py`, `config.py`, `model.py`), one optional directory (`configs`), and possibly other modules (`trainer.py`):

- `main.py` is the main module to be run, showing the main workflow.
- `config.py` is the configuration module that sets up the data, prediction problem, and hyper-parameters, etc. The settings in this module is the default configuration.
  - `configs` is the directory to place *customized* configurations for individual runs. We use `.yaml` files for this purpose.
- `model.py` is the model module to define the machine learning model and configure its training parameters.
  - `trainer.py` is the trainer module to define the training and testing workflow. This module is *only needed when NOT using `PyTorch Lightning`*.

Next, we explain the usage of the pipeline-based API in the modules above, mainly using the [domain adaptation for digits classification example](https://github.com/pykale/pykale/tree/main/examples/digits_dann).

- The `kale.pipeline` module provides mature, off-the-shelf machine learning pipelines for plug-in usage, e.g. `import kale.pipeline.domain_adapter as domain_adapter` in [`digits_dann`'s `model` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/model.py).
- The `kale.utils` module provides common utility functions, such as `from kale.utils.seed import set_seed` in [`digits_dann`'s `main` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/main.py).
- The `kale.loaddata` module provides the input to the machine learning system, such as`from kale.loaddata.image_access import DigitDatase` in  [`digits_dann`'s `main` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/main.py).
- The `kale.prepdata` module provides pre-processing functions to transform the raw input data into a suitable form for machine learning, such as `import kale.prepdata.image_transform as image_transform` in `kale.loaddata.image_access` used in  [`digits_dann`'s `main` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/main.py) for image data augmentation.
- The `kale.embed` module provides *embedding* functions (the *encoder*) to *learn* suitable representations from the (pre-processed) input data, such as `from kale.embed.image_cnn import SmallCNNFeature` in [`digits_dann`'s `model` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/model.py). This is a machine learning module.
- The `kale.predict` module provides prediction functions (the *decoder*) to *learn* a mapping from the input representation to a target prediction, such as `from kale.predict.class_domain_nets import ClassNetSmallImage` in [`digits_dann`'s `model` module](https://github.com/pykale/pykale/blob/main/examples/digits_dann/model.py). This is also a machine learning module.
- The `kale.evaluate` module implements evaluation metrics not yet available, such as the Concordance Index (CI) for measuring the proportion of [concordant pairs](https://en.wikipedia.org/wiki/Concordant_pair).
- The `kale.interpret` module aims to provide functions for interpretation of the learned model or the prediction results, such as visualization. This module has no implementation yet.

## Building New Modules or Projects

New modules/projects can be built following the steps below.

- Step 1 - Examples: Choose one of the [examples](https://github.com/pykale/pykale/tree/main/examples) of your interest (e.g., most relevant to your project) to
  - browse through the configuration, main, and model modules
  - download the data if needed
  - run the example following instructions in the example's README
- Step 2a - New model: To develop new machine learning models under PyKale,
  - define the blocks in your pipeline to figure out what the methods are for data loading, pre-processing data, embedding (encoder/representation), prediction (decoder), evaluation, and interpretation (if needed)
  - modify existing pipelines with your customized blocks or build a new pipeline with PyKale blocks and blocks from other libraries
- Step 2b - New applications: To develop new applications using PyKale,
  - clarify the input data and the prediction target to find matching functionalities in PyKale (request if not found)
  - tailor data loading, pre-processing, and evaluation (and interpretation if needed) to your application

## The Scope of Support

### Data

PyKale currently supports graphs, images, and videos, using PyTorch Dataloaders wherever possible. Audios are not supported yet (welcome your contribution).

### Machine learning models

PyKale supports modules from the following areas of machine learning

- Deep learning: convolutional neural networks (CNNs), graph neural networks (GNNs) GNN including graph convolutional networks (GCNs), transformers
- Transfer learning: domain adaptation
- Multimodal learning: integration of heterogeneous data
- Dimensionality reduction: multilinear subspace learning, such as multilinear principal component analysis (MPCA)

### Example applications

PyKale includes example application from three areas below

- Image/video recognition: imaging recognition with CIFAR10/100, digits (MNIST, USPS, SVHN), action videos (EPIC Kitchen)
- Bioinformatics/graph analysis: link prediction problems in BindingDB and knowledge graphs
- Medical imaging: cardiac MRI classification
</file>

<file path="docs/source/yaml.md">
# Configuration using YAML

## Why YAML?

PyKale has been designed such that users can configure machine learning models and experiments without writing any new Python code. This is achieved via a human and machine readable language called [YAML](https://en.wikipedia.org/wiki/YAML). Well thought out default configuration values are first stored using the [YACS](https://github.com/rbgirshick/yacs) Python module in a `config.py` file. Several customized configurations can then be created in respective `.yaml` files.

This also enables more advanced users to establish their own default and add new configuration parameters with minimal coding. By separating code and configuration, this approach can lead to better [reproducibility](https://en.wikipedia.org/wiki/Reproducibility).

## A simple example

The following example is a simple [YAML file `tutorial.yaml`](https://github.com/pykale/pykale/blob/main/examples/digits_dann/configs/tutorial.yaml) used by the [digits tutorial notebook](https://github.com/pykale/pykale/blob/main/examples/digits_dann/tutorial.ipynb):

```{YAML}
DAN:
  METHOD: "CDAN"

DATASET:
  NUM_REPEAT: 1
  SOURCE: "svhn"
  VALID_SPLIT_RATIO: 0.5

SOLVER:
  MIN_EPOCHS: 0
  MAX_EPOCHS: 3

OUTPUT:
  PB_FRESH: None
```

Related configuration settings are grouped together. The group headings and allowed values are stored in a [separate Python file `config.py`](https://github.com/pykale/pykale/blob/main/examples/digits_dann/config.py) which many users will not need to refer to. The headings and parameters in this example are explained below:

| Heading / Parameter | Meaning | Default |
| --- | --- | --- |
| **DAN** | Domain Adaptation Net | *None* |
| METHOD | Type of DAN: `CDAN`, `CDAN-E`, or `DANN` | `CDAN` |
|**DATASET** | Dataset (for training, testing and validation ) | *None* |
| NUM_REPEAT | Number of times the training and validation cycle will be run | `10` |
| SOURCE | The source dataset name | `mnist` |
| VALID_SPLIT_RATIO | The proportion of training data used for validation | `0.1` |
| **SOLVER** | Model training parameters | *None* |
| MIN_EPOCHS | The minimum number of training epochs | `20` |
| MAX_EPOCHS | The maximum number of training epochs | `120` |
| **OUTPUT** | Output configuration | *None* |
| PB_FRESH | Progress bar refresh option | `0` (disabled) |

The tutorial YAML file `tutorial.yaml` above overrides certain defaults in `config.py` to make the machine learning process faster and clearer for demonstration purposes.

## Customization for your applications

Application of an example to your data can be as simple as creating a new YAML file to (change the defaults to) specify your data location, and other preferred configuration customization, e.g., in the choice of models and/or the number of iterations.
</file>

<file path="docs/make.bat">
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=source
set BUILDDIR=build

if "%1" == "" goto help

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.http://sphinx-doc.org/
	exit /b 1
)

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%

:end
popd
</file>

<file path="docs/Makefile">
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = source
BUILDDIR      = build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
</file>

<file path="docs/README.md">
# Documentation

Learn more about [Python Docstrings](https://www.datacamp.com/community/tutorials/docstrings-python) to contribute high-quality documentation while coding. We follow [Google Style Python Docstrings](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).

## Workflow

We call `kale` and `examples` the **root**-level modules, `kale.xxx` and `examples.xxx` the **first**-level modules, and `kale.xxx.xxx` and `examples.xxx.xxx` the **second**-level modules (and so on, if necessary).

1. **First-level module update**: If the `kale.xxx` module is not under `source` yet, run `sphinx-apidoc -o source/ ../kale` from `docs` to generate the `.rst` file under `source` first. If the `examples.xxx` module is not under `source/examples` yet, run `sphinx-apidoc -o source/examples ../examples` from `docs` to generate the `.rst` file under `source/examples` first. *Note: This should be rarely needed.*

2. **Second-level module update**: If the `kale.xxx.xxx` model is not in the documentation yet or has been *renamed*, add or revise it manually to `kale.xxx.rst` in an alphabetically-sorted position, e.g. adding the `linformer` module by inserting the following in `kale.embed.rst` right before the `mpca` module:
    ```
    kale.embed.linformer module
    ----------------------

    .. automodule:: kale.embed.linformer
    :members:
    :undoc-members:
    :show-inheritance:
    ```
    *Caution*: Alternatively (e.g., lots of modules are renamed), remove all relevant `.rst` files under `source` and then recreate them, e.g., via running `sphinx-apidoc -o source/ ../kale`. After creation, edit the heading of `.rst` files, e.g., from **kale.embed package** to **Embed** (see those in an earlier version).

3. **Final update step**: Run `make html` from `docs` to update the `.html` files under the `build` folder using the source files under the `source` folder and verify the updated documentation in a browser at `pykale/docs/build/html/index.html`. Run `make clean` will clean the `build` folder for a fresh build. Do **NOT** commit `docs/build` (see `.gitignore`). Build and view offline to check.

4. Other standardization

* Put a docstring at the top of each `.py` to summarize the module
* Docstring for a class should be at the top of the class definition, above `__init__`
* See `examples/digits_dann` and related modules for reference.

If you are aware of a better way to auto-generate documentations, create an issue or push your suggested changes.

## References

### Sphinx autodocumentation and Read the Doc

* We use [Read the Doc](https://readthedocs.org/) with [Sphinx](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/sphinx-quickstart.html).
* Documentations can be [automatically generated]([https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html](https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html)). [Tutorial: Autodocumenting your Python code with Sphinx](https://romanvm.pythonanywhere.com/post/autodocumenting-your-python-code-sphinx-part-i-5/)
* [Python Docstring Generator for VScode](https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring)

### Key references from [JMLR Machine Learning Open Source Software](http://www.jmlr.org/mloss/)

* [Tensor Train Decomposition on TensorFlow (T3F)](https://github.com/Bihaqo/t3f)
* [A Graph Kernel Library in Python](https://github.com/ysig/GraKeL)
* [A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning](https://github.com/scikit-learn-contrib/imbalanced-learn)
* [Deep Universal Probabilistic Programming](https://github.com/pyro-ppl/pyro)
* [A Python Toolbox for Scalable Outlier Detection](https://github.com/yzhao062/pyod)

Another documentation to learn from: the [MONAI highlights](https://docs.monai.io/en/latest/highlights.html).
</file>

<file path="docs/requirements.txt">
# Sorted
--find-links https://download.pytorch.org/whl/torch_stable.html

# Learn from https://github.com/rusty1s/pytorch_geometric/blob/master/docs/requirements.txt
# Remove if switching to DGL
cython
https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.1%2Bpt20cpu-cp310-cp310-linux_x86_64.whl
https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.17%2Bpt20cpu-cp310-cp310-linux_x86_64.whl
https://download.pytorch.org/whl/cpu/torch-2.0.0%2Bcpu-cp310-cp310-linux_x86_64.whl

ipython<=8.12.0

nbsphinx
nbsphinx-link
numpy

pandas
pillow>=6.2.0
pwlf
pydicom
PyTDC
pytorch-lightning
rdkit
recommonmark
scikit-image
scikit-learn
sphinx-markdown-tables
sphinx-rtd-theme
tensorly
# torch>=2.0.0
torch-geometric

torchsummary>=1.5.0
torchvision
urllib3<2.0
yacs>=0.1.7
</file>

<file path="examples/action_dann/configs/EPIC-D12D2-CDAN.yaml">
DATASET:
#  ROOT:  '/shared/tale2/Shared/data/EgoAction/'
  SOURCE: 'EPIC'
  SRC_TRAINLIST: 'epic_D1_train.pkl'
  SRC_TESTLIST: 'epic_D1_test.pkl'
  TARGET: 'EPIC'
  TGT_TRAINLIST: 'epic_D2_train.pkl'
  TGT_TESTLIST: 'epic_D2_test.pkl'
  IMAGE_MODALITY: 'rgb'
DAN:
  METHOD: 'CDAN'
MODEL:
  METHOD: 'r3d_18'
OUTPUT:
  OUT_DIR: 'outputs/EPIC-d12d2-cdan/'
</file>

<file path="examples/action_dann/configs/EPIC-D12D2-DAN.yaml">
DATASET:
#  ROOT:  '/shared/tale2/Shared/data/EgoAction/'
  SOURCE: 'EPIC'
  SRC_TRAINLIST: 'epic_D1_train.pkl'
  SRC_TESTLIST: 'epic_D1_test.pkl'
  TARGET: 'EPIC'
  TGT_TRAINLIST: 'epic_D2_train.pkl'
  TGT_TESTLIST: 'epic_D2_test.pkl'
  IMAGE_MODALITY: 'rgb'
DAN:
  METHOD: 'DAN'
MODEL:
  METHOD: 'r3d_18'
OUTPUT:
  OUT_DIR: 'outputs/EPIC-d12d2-dan/'
</file>

<file path="examples/action_dann/configs/EPIC-D12D2-DANN.yaml">
DATASET:
#  ROOT:  '/shared/tale2/Shared/data/EgoAction/'
  SOURCE: 'EPIC'
  SRC_TRAINLIST: 'epic_D1_train.pkl'
  SRC_TESTLIST: 'epic_D1_test.pkl'
  TARGET: 'EPIC'
  TGT_TRAINLIST: 'epic_D2_train.pkl'
  TGT_TESTLIST: 'epic_D2_test.pkl'
  IMAGE_MODALITY: 'rgb'
DAN:
  METHOD: 'DANN'
MODEL:
  METHOD: 'r3d_18'
OUTPUT:
  OUT_DIR: 'outputs/EPIC-d12d2-dann/'
</file>

<file path="examples/action_dann/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML)
</file>

<file path="examples/action_dann/config.py">
"""
Default configurations for action recognition domain adaptation
"""

import os

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "I:/Datasets/EgoAction/"  # "/shared/tale2/Shared"
_C.DATASET.SOURCE = "EPIC"  # dataset options=["EPIC", "GTEA", "ADL", "KITCHEN"]
_C.DATASET.SRC_TRAINLIST = "epic_D1_train.pkl"
_C.DATASET.SRC_TESTLIST = "epic_D1_test.pkl"
_C.DATASET.TARGET = "EPIC"  # dataset options=["EPIC", "GTEA", "ADL", "KITCHEN"]
_C.DATASET.TGT_TRAINLIST = "epic_D2_train.pkl"
_C.DATASET.TGT_TESTLIST = "epic_D2_test.pkl"
_C.DATASET.IMAGE_MODALITY = "rgb"  # mode options=["rgb", "flow", "joint"]
# _C.DATASET.NUM_CLASSES = 8
_C.DATASET.FRAMES_PER_SEGMENT = 16
_C.DATASET.NUM_REPEAT = 5  # 10
_C.DATASET.WEIGHT_TYPE = "natural"
_C.DATASET.SIZE_TYPE = "max"  # options=["source", "max"]
# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2020
_C.SOLVER.BASE_LR = 0.01  # Initial learning rate
_C.SOLVER.MOMENTUM = 0.9
_C.SOLVER.WEIGHT_DECAY = 0.0005  # 1e-4
_C.SOLVER.NESTEROV = True

_C.SOLVER.TYPE = "SGD"
_C.SOLVER.MAX_EPOCHS = 30  # "nb_adapt_epochs": 100,
# _C.SOLVER.WARMUP = True
_C.SOLVER.MIN_EPOCHS = 5  # "nb_init_epochs": 20,
_C.SOLVER.TRAIN_BATCH_SIZE = 16  # 150
# _C.SOLVER.TEST_BATCH_SIZE = 32  # No difference in ADA

# Adaptation-specific solver config
_C.SOLVER.AD_LAMBDA = True
_C.SOLVER.AD_LR = True
_C.SOLVER.INIT_LAMBDA = 1.0

# ---------------------------------------------------------------------------- #
# Domain Adaptation Net (DAN) configs
# ---------------------------------------------------------------------------- #
_C.MODEL = CfgNode()
_C.MODEL.METHOD = "i3d"  # options=["r3d_18", "r2plus1d_18", "mc3_18", "i3d"]
_C.MODEL.ATTENTION = "None"  # options=["None", "SELayer"]

# ---------------------------------------------------------------------------- #
# Domain Adaptation Net (DAN) configs
# ---------------------------------------------------------------------------- #
_C.DAN = CfgNode()
_C.DAN.METHOD = "CDAN"  # options=["CDAN", "CDAN-E", "DANN", "DAN"]
_C.DAN.USERANDOM = False
_C.DAN.RANDOM_DIM = 1024
# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.VERBOSE = False  # To discuss, for HPC jobs
_C.OUTPUT.FAST_DEV_RUN = False  # True for debug
_C.OUTPUT.PB_FRESH = 0  # 0 # 50 # 0 to disable  ; MAYBE make it a command line option
_C.OUTPUT.OUT_DIR = os.path.join("outputs", _C.DATASET.SOURCE + "2" + _C.DATASET.TARGET)


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/action_dann/main.py">
"""This example is about domain adaptation for action recognition, using PyTorch Lightning.

Reference: https://github.com/thuml/CDAN/blob/master/pytorch/train_image.py
"""

import argparse
import logging

import pytorch_lightning as pl
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, TQDMProgressBar

from kale.loaddata.video_access import VideoDataset
from kale.loaddata.video_multi_domain import VideoMultiDomainDatasets
from kale.utils.seed import set_seed

# from pytorch_lightning.callbacks.early_stopping import EarlyStopping


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Domain Adversarial Networks on Action Datasets")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    parser.add_argument("--resume", default="", type=str)
    args = parser.parse_args()
    return args


def main():
    """The main for this domain adaptation example, showing the workflow"""
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    print(cfg)

    # ---- setup output ----
    format_str = "@%(asctime)s %(name)s [%(levelname)s] - (%(message)s)"
    logging.basicConfig(format=format_str)
    # ---- setup dataset ----
    seed = cfg.SOLVER.SEED
    source, target, num_classes = VideoDataset.get_source_target(
        VideoDataset(cfg.DATASET.SOURCE.upper()), VideoDataset(cfg.DATASET.TARGET.upper()), seed, cfg
    )
    dataset = VideoMultiDomainDatasets(
        source,
        target,
        image_modality=cfg.DATASET.IMAGE_MODALITY,
        seed=seed,
        config_weight_type=cfg.DATASET.WEIGHT_TYPE,
        config_size_type=cfg.DATASET.SIZE_TYPE,
    )

    # ---- training/test process ----
    ### Repeat multiple times to get std
    for i in range(0, cfg.DATASET.NUM_REPEAT):
        seed = seed + i * 10
        set_seed(seed)  # seed_everything in pytorch_lightning did not set torch.backends.cudnn
        print(f"==> Building model for seed {seed} ......")
        # ---- setup model and logger ----
        model, train_params = get_model(cfg, dataset, num_classes)
        tb_logger = pl_loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name="seed{}".format(seed))
        checkpoint_callback = ModelCheckpoint(
            filename="{epoch}-{step}-{valid_loss:.4f}",
            # save_last=True,
            # save_top_k=1,
            monitor="valid_loss",
            mode="min",
        )

        ### Set early stopping
        # early_stop_callback = EarlyStopping(monitor="valid_target_acc", min_delta=0.0000, patience=100, mode="max")

        lr_monitor = LearningRateMonitor(logging_interval="epoch")
        progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)

        ### Set the lightning trainer.
        trainer = pl.Trainer(
            min_epochs=cfg.SOLVER.MIN_EPOCHS,
            max_epochs=cfg.SOLVER.MAX_EPOCHS,
            # resume_from_checkpoint=last_checkpoint_file,
            accelerator="gpu" if args.devices != 0 else "cpu",
            devices=args.devices if args.devices != 0 else "auto",
            logger=tb_logger,
            fast_dev_run=cfg.OUTPUT.FAST_DEV_RUN,
            callbacks=[lr_monitor, checkpoint_callback, progress_bar],
        )

        ### Find learning_rate
        # lr_finder = trainer.tuner.lr_find(model, max_lr=0.1, min_lr=1e-6)
        # fig = lr_finder.plot(suggest=True)
        # fig.show()
        # logging.info(lr_finder.suggestion())

        ### Training/validation process
        trainer.fit(model)

        ### Test process
        trainer.test()


if __name__ == "__main__":
    main()
</file>

<file path="examples/action_dann/model.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""
Define the learning model and configure training parameters.
References from https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/utils/experimentation.py
"""

from copy import deepcopy

from kale.embed.video_feature_extractor import get_video_feat_extractor
from kale.pipeline import domain_adapter, video_domain_adapter
from kale.predict.class_domain_nets import ClassNetVideo, DomainNetVideo


def get_config(cfg):
    """
    Sets the hyper parameter for the optimizer and experiment using the config file

    Args:
        cfg: A YACS config object.
    """

    config_params = {
        "train_params": {
            "adapt_lambda": cfg.SOLVER.AD_LAMBDA,
            "adapt_lr": cfg.SOLVER.AD_LR,
            "lambda_init": cfg.SOLVER.INIT_LAMBDA,
            "nb_adapt_epochs": cfg.SOLVER.MAX_EPOCHS,
            "nb_init_epochs": cfg.SOLVER.MIN_EPOCHS,
            "init_lr": cfg.SOLVER.BASE_LR,
            "batch_size": cfg.SOLVER.TRAIN_BATCH_SIZE,
            "optimizer": {
                "type": cfg.SOLVER.TYPE,
                "optim_params": {
                    "momentum": cfg.SOLVER.MOMENTUM,
                    "weight_decay": cfg.SOLVER.WEIGHT_DECAY,
                    "nesterov": cfg.SOLVER.NESTEROV,
                },
            },
        },
        "data_params": {
            # "dataset_group": cfg.DATASET.NAME,
            "dataset_name": cfg.DATASET.SOURCE + "2" + cfg.DATASET.TARGET,
            "source": cfg.DATASET.SOURCE,
            "target": cfg.DATASET.TARGET,
            "size_type": cfg.DATASET.SIZE_TYPE,
            "weight_type": cfg.DATASET.WEIGHT_TYPE,
        },
    }
    return config_params


# Based on https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/utils/experimentation.py
def get_model(cfg, dataset, num_classes):
    """
    Builds and returns a model and associated hyperparameters according to the config object passed.

    Args:
        cfg: A YACS config object.
        dataset: A multi domain dataset consisting of source and target datasets.
        num_classes: The class number of specific dataset.
    """

    # setup feature extractor
    feature_network, class_feature_dim, domain_feature_dim = get_video_feat_extractor(
        cfg.MODEL.METHOD.upper(), cfg.DATASET.IMAGE_MODALITY, cfg.MODEL.ATTENTION, num_classes
    )
    # setup classifier
    classifier_network = ClassNetVideo(input_size=class_feature_dim, n_class=num_classes)

    config_params = get_config(cfg)
    train_params = config_params["train_params"]
    train_params_local = deepcopy(train_params)
    method_params = {}

    method = domain_adapter.Method(cfg.DAN.METHOD)

    if method.is_mmd_method():
        model = video_domain_adapter.create_mmd_based_video(
            method=method,
            dataset=dataset,
            image_modality=cfg.DATASET.IMAGE_MODALITY,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            **method_params,
            **train_params_local,
        )
    else:
        critic_input_size = domain_feature_dim
        # setup critic network
        if method.is_cdan_method():
            if cfg.DAN.USERANDOM:
                critic_input_size = cfg.DAN.RANDOM_DIM
            else:
                critic_input_size = domain_feature_dim * num_classes
        critic_network = DomainNetVideo(input_size=critic_input_size)

        if cfg.DAN.METHOD == "CDAN":
            method_params["use_random"] = cfg.DAN.USERANDOM

        # The following calls kale.loaddata.dataset_access for the first time
        model = video_domain_adapter.create_dann_like_video(
            method=method,
            dataset=dataset,
            image_modality=cfg.DATASET.IMAGE_MODALITY,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            critic=critic_network,
            **method_params,
            **train_params_local,
        )

    return model, train_params
</file>

<file path="examples/action_dann/README.md">
# Video Classification: Domain Adaptation for Action Recognition with Lightning

### 1. Description

This example is constructed by refactoring the [ADA: (Yet) Another Domain Adaptation library](https://github.com/criteo-research/pytorch-ada), with many domain adaptation algorithms included and is modified from `digits_dann`, by replacing the feature extractor and the data loaders for video data.

### 2. Usage

* Datasets: GTEA, KITCHEN, ADL(P4, P6, P11), EPIC(D1, D2, D3)
* Algorithms: DANN, CDAN, DAN, ...
* Example:

For training (using 1 GPU):

`python main.py --cfg configs/EPIC-D12D2-DANN.yaml --devices 1`

`python main.py --cfg configs/EPIC-D12D2-CDAN.yaml --devices 1`

`python main.py --cfg configs/EPIC-D12D2-DAN.yaml --devices 1`

For test:

`python test.py --cfg configs/EPIC-D12D2-DANN.yaml --devices 1 --ckpt your_pretrained_model.ckpt `

### 3. Related `kale` API

`kale.embed.video_feature_extractor`: Get video feature extractor networks (Res3D, I3D, etc.).

`kale.embed.video_i3d`: Inflated 3D ConvNets (I3D) model for action recognition.

`kale.embed.video_res3d`: MC3_18, R3D_18, and R2plus1D_18 models for action recognition.

`kale.embed.video_se_i3d`: I3D model with SELayers.

`kale.embed.video_se_res3d`: MC3_18, R3D_18, and R2plus1D_18 models with SELayers.

`kale.embed.video_selayer`: Basic SELayers.

`kale.loaddata.action_multi_domain`: Construct the dataset for action videos with (multiple) source and target domains.

`kale.loaddata.video_access`: Data loaders for video datasets.

`kale.loaddata.video_datasets`: Based on `kale.loaddata.videos`. `BasicVideoDataset` for loading data from GTEA, KITCHEN and ADL Datasets. `EPIC` inherited from `BasicVideoDataset` for EPIC-Kitchen dataset.

`kale.loaddata.videos`: `VideoRecord` represents a video sample's metadata. `VideoFrameDataset` For loading video data effieciently.

`kale.pipeline.action_domain_adapter`: Domain adaptation pipelines for action video on action recognition.

`kale.predict.class_domain_nets`: Classifiers for data or domain.

`kale.prepdata.video_transform`: Transforms for video data.
</file>

<file path="examples/action_dann/test.py">
"""This example is testing domain adaptation for action recognition, using PyTorch Lightning.
We can load and test different trained models without training.

"""

import argparse
import logging

import pytorch_lightning as pl
import torch
from config import get_cfg_defaults
from model import get_model

from kale.loaddata.video_access import VideoDataset
from kale.loaddata.video_multi_domain import VideoMultiDomainDatasets


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Domain Adversarial Networks on Action Datasets")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. None/int(0) for cpu. list[x,y] for xth, yth GPU. str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    parser.add_argument("--resume", default="", type=str)
    parser.add_argument("--ckpt", default="", help="pre-trained parameters for the model (ckpt files)", type=str)
    args = parser.parse_args()
    return args


def weights_update(model, checkpoint):
    """Load the pre-trained parameters to the model."""
    model_dict = model.state_dict()
    pretrained_dict = {k: v for k, v in checkpoint["state_dict"].items() if k in model_dict}
    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)
    return model


def main():
    """The main for this domain adaptation example, showing the workflow"""
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    print(cfg)

    # ---- setup output ----
    format_str = "@%(asctime)s %(name)s [%(levelname)s] - (%(message)s)"
    logging.basicConfig(format=format_str)
    # ---- setup dataset ----
    seed = cfg.SOLVER.SEED
    source, target, num_classes = VideoDataset.get_source_target(
        VideoDataset(cfg.DATASET.SOURCE.upper()), VideoDataset(cfg.DATASET.TARGET.upper()), seed, cfg
    )
    dataset = VideoMultiDomainDatasets(
        source,
        target,
        image_modality=cfg.DATASET.IMAGE_MODALITY,
        seed=seed,
        config_weight_type=cfg.DATASET.WEIGHT_TYPE,
        config_size_type=cfg.DATASET.SIZE_TYPE,
    )

    # ---- setup model and logger ----
    model, train_params = get_model(cfg, dataset, num_classes)
    trainer = pl.Trainer(
        logger=False,
        resume_from_checkpoint=args.ckpt,
        accelerator="gpu" if args.devices != 0 else "cpu",
        devices=args.devices if args.devices != 0 else "auto",
    )

    model_test = weights_update(model=model, checkpoint=torch.load(args.ckpt))

    # test scores
    trainer.test(model=model_test)


if __name__ == "__main__":
    main()
</file>

<file path="examples/avmnist_multimodal/configs/bimodal_interaction_fusion.yaml">
MODEL:
  FUSION: "bimodal_interaction_fusion"

OUTPUT:
  OUT_DIR: './outputs/bimodal_interaction_fusion'
</file>

<file path="examples/avmnist_multimodal/configs/late_fusion.yaml">
MODEL:
  FUSION: "late"

OUTPUT:
  OUT_DIR: './outputs/late_fusion'
</file>

<file path="examples/avmnist_multimodal/configs/low_rank_tensor_fusion.yaml">
MODEL:
  FUSION: "low_rank_tensor"

OUTPUT:
  OUT_DIR: './outputs/low_rank_tensor_fusion'
</file>

<file path="examples/avmnist_multimodal/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/avmnist_multimodal/config.py">
"""
Default configurations for the AVMNIST dataset using Multimodal Neural Network (MMNN).
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "avmnist"
_C.DATASET.NAME = "avmnist.zip"
_C.DATASET.GDRIVE_ID = "1N5k-LvLwLbPBgn3GdVg6fXMBIR6pYrKb"
_C.DATASET.FILE_FORMAT = "zip"
_C.DATASET.BATCH_SIZE = 40
_C.DATASET.SHUFFLE = True
_C.DATASET.FLATTEN_AUDIO = True
_C.DATASET.FLATTEN_IMAGE = True
_C.DATASET.UNSQUEEZE_CHANNEL = False
_C.DATASET.NORMALIZE_IMAGE = False
_C.DATASET.NORMALIZE_AUDIO = False

# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2020
_C.SOLVER.BASE_LR = 0.01
_C.SOLVER.WEIGHT_DECAY = 0.0001
_C.SOLVER.MAX_EPOCHS = 1
_C.SOLVER.EARLY_STOP = True
_C.SOLVER.CLIP_VAL = 8

# ---------------------------------------------------------------------------- #
# Model configs
# ---------------------------------------------------------------------------- #
_C.MODEL = CfgNode()
_C.MODEL.LENET_IN_CHANNELS = 1
_C.MODEL.LENET_ADD_LAYERS_IMG = 3
_C.MODEL.LENET_ADD_LAYERS_AUD = 5
_C.MODEL.CHANNELS = 6
_C.MODEL.MLP_IN_DIM = _C.MODEL.CHANNELS * 40
_C.MODEL.MLP_LOW_RANK_IN_DIM = _C.MODEL.CHANNELS * 20
_C.MODEL.MLP_HIDDEN_DIM = 100
_C.MODEL.OUT_DIM = 2
_C.MODEL.FUSION = "late"
_C.MODEL.MULTIPLICATIVE_FUSION_IN_DIM = [_C.MODEL.CHANNELS * 8, _C.MODEL.CHANNELS * 32]
_C.MODEL.MULTIPLICATIVE_FUSION_OUT_DIM = _C.MODEL.CHANNELS * 40
_C.MODEL.MULTIPLICATIVE_OUTPUT = "matrix"
_C.MODEL.LOW_RANK_TENSOR_IN_DIM = [_C.MODEL.CHANNELS * 8, _C.MODEL.CHANNELS * 32]
_C.MODEL.LOW_RANK_TENSOR_OUT_DIM = _C.MODEL.CHANNELS * 20
_C.MODEL.LOW_RANK_TENSOR_RANK = 40

# -----------------------------------------------------------------------------
# Comet Logger (optional) - https://www.comet.ml/site/
# -----------------------------------------------------------------------------
_C.COMET = CfgNode()
_C.COMET.ENABLE = False  # Set True to enable Comet logging (requires an API key).
_C.COMET.API_KEY = ""  # Your Comet API key
_C.COMET.PROJECT_NAME = "Multimodal Neural Network"
_C.COMET.EXPERIMENT_NAME = "AVMNIST_MMNN"

# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"
_C.OUTPUT.PB_FRESH = 50  # Number of steps before a new progress bar is printed. Set 0 to disable the progress bar.


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/avmnist_multimodal/main.py">
"""This example is about the application of Multimodal Neural Network (MMNN) for digit classification (0 and 1) on the AVMNIST dataset.

Reference: https://github.com/pliang279/MultiBench/tree/main/examples/multimedia
"""

import argparse
import os
import time

import pytorch_lightning as pl
import torch
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar

from kale.loaddata.avmnist_datasets import AVMNISTDataset
from kale.utils.download import download_file_gdrive
from kale.utils.logger import construct_logger
from kale.utils.seed import set_seed


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="PyTorch AVMNIST Training")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument("--output", default="default", help="folder to save output", type=str)
    parser.add_argument(
        "--gpus",
        default=0,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    args = parser.parse_args()
    return args


def main():
    args = arg_parse()
    # ---- setup device ----
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("==> Using device " + device)

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    set_seed(cfg.SOLVER.SEED)

    # ---- setup logger and output ----
    output_dir = os.path.join(cfg.OUTPUT.OUT_DIR, cfg.DATASET.NAME, args.output)
    os.makedirs(output_dir, exist_ok=True)
    logger = construct_logger("avmnist", output_dir)
    logger.info("Using " + device)
    logger.info("\n" + cfg.dump())

    download_file_gdrive(cfg.DATASET.GDRIVE_ID, cfg.DATASET.ROOT, cfg.DATASET.NAME, cfg.DATASET.FILE_FORMAT)

    dataset = AVMNISTDataset(data_dir=cfg.DATASET.ROOT, batch_size=cfg.DATASET.BATCH_SIZE)
    traindata = dataset.get_train_loader()
    validdata = dataset.get_valid_loader()
    testdata = dataset.get_test_loader()

    model = get_model(cfg, device)

    # Choose one logger (CometLogger or TensorBoardLogger) using cfg.COMET.ENABLE
    if cfg.COMET.ENABLE:
        suffix = str(int(time.time() * 1000))[6:]
        logger = pl_loggers.CometLogger(
            api_key=cfg.COMET.API_KEY,
            project_name=cfg.COMET.PROJECT_NAME,
            save_dir=cfg.OUTPUT.OUT_DIR,
            experiment_name="{}_{}".format(cfg.COMET.EXPERIMENT_NAME, suffix),
        )
    else:
        logger = pl_loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR)

    # ---- setup callbacks ----
    # setup progress bar
    progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)

    # setup learning rate monitor
    lr_monitor = LearningRateMonitor(logging_interval="epoch")

    # ---- setup trainers ----
    trainer = pl.Trainer(
        default_root_dir=cfg.OUTPUT.OUT_DIR,
        max_epochs=cfg.SOLVER.MAX_EPOCHS,
        accelerator="gpu" if args.devices != 0 else "cpu",
        devices=args.devices if args.devices != 0 else "auto",
        logger=logger,
        callbacks=[progress_bar, lr_monitor],
        log_every_n_steps=1,
    )
    # ---- start training ----
    trainer.fit(model, traindata, validdata)

    # ---- start testing ----
    print("Testing:")
    trainer.test(model, testdata)


if __name__ == "__main__":
    main()
</file>

<file path="examples/avmnist_multimodal/model.py">
"""
Define and build the Multimodal Neural Network (MMNN) model based on chosen hyperparameters..
References: 1. https://github.com/pliang279/MultiBench/blob/main/examples/multimedia/avmnist_simple_late_fusion.py
            2. https://github.com/pliang279/MultiBench/blob/main/examples/multimedia/avmnist_low_rank_tensor.py
            3. https://github.com/pliang279/MultiBench/blob/main/examples/multimedia/avmnist_multi_interac_matrix.py
"""

from kale.embed.feature_fusion import BimodalInteractionFusion, Concat, LowRankTensorFusion
from kale.embed.image_cnn import LeNet
from kale.pipeline.base_nn_trainer import MultimodalNNTrainer
from kale.predict.decode import MLPDecoder


def get_model(cfg, device):
    """
    Builds and returns an MMNN model according to the config object passed.

    Args:
        cfg: A YACS config object.
    """
    encoders = [
        LeNet(cfg.MODEL.LENET_IN_CHANNELS, cfg.MODEL.CHANNELS, cfg.MODEL.LENET_ADD_LAYERS_IMG),
        LeNet(cfg.MODEL.LENET_IN_CHANNELS, cfg.MODEL.CHANNELS, cfg.MODEL.LENET_ADD_LAYERS_AUD),
    ]

    if cfg.MODEL.FUSION == "late":
        fusion = Concat()
        head = MLPDecoder(
            cfg.MODEL.MLP_IN_DIM, cfg.MODEL.MLP_HIDDEN_DIM, cfg.MODEL.OUT_DIM, include_decoder_layers=False
        )
    elif cfg.MODEL.FUSION == "bimodal_interaction_fusion":
        fusion = BimodalInteractionFusion(
            cfg.MODEL.MULTIPLICATIVE_FUSION_IN_DIM,
            cfg.MODEL.MULTIPLICATIVE_FUSION_OUT_DIM,
            cfg.MODEL.MULTIPLICATIVE_OUTPUT,
        )
        head = MLPDecoder(
            cfg.MODEL.MLP_IN_DIM, cfg.MODEL.MLP_HIDDEN_DIM, cfg.MODEL.OUT_DIM, include_decoder_layers=False
        )
    elif cfg.MODEL.FUSION == "low_rank_tensor":
        fusion = LowRankTensorFusion(
            cfg.MODEL.LOW_RANK_TENSOR_IN_DIM, cfg.MODEL.LOW_RANK_TENSOR_OUT_DIM, cfg.MODEL.LOW_RANK_TENSOR_RANK
        )
        head = MLPDecoder(
            cfg.MODEL.MLP_LOW_RANK_IN_DIM, cfg.MODEL.MLP_HIDDEN_DIM, cfg.MODEL.OUT_DIM, include_decoder_layers=False
        )

    model = MultimodalNNTrainer(encoders, fusion, head, lr=cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)
    return model
</file>

<file path="examples/avmnist_multimodal/README.md">
# Multimodal Digit Classification: Multimodal Neural Network (MMNN) Method on AVMNIST

### 1. Description

This example is constructed by refactoring the code of the [MultiBench repository](https://github.com/pliang279/MultiBench) for the [PyKale library](https://github.com/pykale/pykale).

### 2. Usage
* Dataset: [AVMNIST](https://drive.google.com/file/d/1KvKynJJca5tDtI5Mmp6CoRh9pQywH8Xp/view)
<br>The Audio-Visual MNIST (AVMNIST) dataset is an innovative and unique extension on the classic MNIST dataset, intended to facilitate multimodal learning, where models are trained to interpret information from both visual and auditory inputs simultaneously. This is an interesting area of research, as many real-world applications involve multiple modalities of input, and models that can integrate and learn from these different modalities of data can potentially achieve more robust and accurate performance. AVMNIST was created by pairing audio clips from the Free Spoken Digit Dataset (FSDD) with the corresponding written digit images from the MNIST dataset. The FSDD is a collection of 3,000 recordings of spoken digits in English, where each clip consists of a person speaking a digit between 0 and 9. The MNIST dataset, on the other hand, consists of 70,000 grayscale images of hand-written digits between 0 and 9. Each sample in the AVMNIST dataset thus consists of an image-audio pair where the image is a handwritten digit from the MNIST dataset, and the corresponding audio is a clip of a person saying the same digit from the FSDD dataset. The task, therefore, is to predict the digit that will be between 0 and 9.
* Algorithm: Multimodal Neural Network (MMNN)
* Example: AVMNIST with MMNN

Note: In this example, we used a [small subset of the AVMNIST dataset](https://drive.google.com/file/d/1N5k-LvLwLbPBgn3GdVg6fXMBIR6pYrKb/view), specifically including only the samples with labels 0 and 1, to illustrate the usage of MMNN in predicting digit.

We provided some `yaml` config file for a quick testing in the `configs` folder. To use it, run:
```python
python main.py --cfg configs/late_fusion.yaml --output AVMNIST-LATE
```
or
```python
python main.py --cfg configs/low_rank_tensor_fusion.yaml --output AVMNIST_LOW_RANK_TENSOR
```
or
```python
python main.py --cfg configs/bimodal_interaction_fusion.yaml --output AVMNIST-BIMODAL_INTERACTION
```

### 3. Related `kale` API

`kale.loaddata.avmnist_datasets`: This is a data loading module specifically designed for handling the AVMNIST dataset. The AVMNIST dataset is a multimodal version of the traditional MNIST, containing both audio and visual data corresponding to the original MNIST data. This module provides efficient ways to load, preprocess, and format this data in a manner that makes it ready for training a multimodal neural network model.

`kale.embed.feature_fusion`: This module provides a comprehensive set of feature fusion methods, designed for integrating distinct modalities like image and audio. It utilizes a wide range of methods, from simple concatenation, addition, and multiplication fusion, to more sophisticated approaches such as low-rank tensor fusion. The choice of fusion method can greatly influence the performance of the resulting multimodal model.

`kale.embed.image_cnn.LeNet`: This is a base neural network trainer that allows the extraction of features from different modalities like images and audios.

`kale.pipeline.base_nn_trainer.MultimodalNNTrainer`: This module includes a model trainer that consists of a training loop, optimizer configuration, and evaluation metrics.

`kale.predict.decode`: This module is responsible for generating final predictions from the fused features. A two-layered MLPDecoder has been used to generate the final output.
</file>

<file path="examples/bindingdb_deepdta/configs/IC50-DeepDTA.yaml">
DATASET:
  NAME: "BindingDB_IC50"
  PATH: "./data"

SOLVER:
  MAX_EPOCHS: 100
</file>

<file path="examples/bindingdb_deepdta/configs/Kd-DeepDTA.yaml">
DATASET:
  NAME: 'BindingDB_Kd'
</file>

<file path="examples/bindingdb_deepdta/configs/Ki-DeepDTA.yaml">
DATASET:
  NAME: 'BindingDB_Ki'
</file>

<file path="examples/bindingdb_deepdta/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/bindingdb_deepdta/configs/tutorial.yaml">
DATASET:
  NAME: 'BindingDB_Kd'
SOLVER:
  MAX_EPOCHS: 3
  MIN_EPOCHS: 0
</file>

<file path="examples/bindingdb_deepdta/config.py">
"""
Default configurations for the BindingDB datasets using DeepDTA.
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.PATH = "./data"
_C.DATASET.NAME = "BindingDB_IC50"
_C.DATASET.Y_LOG = True

# -----------------------------------------------------------------------------
# Model component
# -----------------------------------------------------------------------------
_C.MODEL = CfgNode()
_C.MODEL.DRUG_DIM = 128
_C.MODEL.TARGET_DIM = 128
_C.MODEL.DRUG_LENGTH = 85
_C.MODEL.TARGET_LENGTH = 1200
_C.MODEL.NUM_FILTERS = 32  # for cnn only
_C.MODEL.DRUG_FILTER_LENGTH = 8  # for drug cnn only
_C.MODEL.TARGET_FILTER_LENGTH = 8  # for target cnn only
_C.MODEL.MLP_IN_DIM = 192  # for mlp only, the concat of drug decoder output and target decode output
_C.MODEL.NUM_SMILE_CHAR = 64
_C.MODEL.NUM_ATOM_CHAR = 25
_C.MODEL.MLP_HIDDEN_DIM = 1024
_C.MODEL.MLP_OUT_DIM = 512
_C.MODEL.MLP_DROPOUT_RATE = 0.2

# -----------------------------------------------------------------------------
# Solver
# -----------------------------------------------------------------------------
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2020
_C.SOLVER.LR = 0.001
_C.SOLVER.MIN_EPOCHS = 0
_C.SOLVER.MAX_EPOCHS = 100
_C.SOLVER.TRAIN_BATCH_SIZE = 256
_C.SOLVER.TEST_BATCH_SIZE = 256


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/bindingdb_deepdta/main.py">
import argparse

import pytorch_lightning as pl
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from torch.utils.data import DataLoader

from kale.loaddata.tdc_datasets import BindingDBDataset


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="DeepDTA on BindingDB dataset")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    parser.add_argument("--resume", default="", type=str)
    args = parser.parse_args()
    return args


def main():
    args = arg_parse()

    # ---- set configs, logger and device ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    tb_logger = TensorBoardLogger("outputs", name=cfg.DATASET.NAME)

    # ---- set dataset ----
    train_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split="train", path=cfg.DATASET.PATH)
    valid_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split="valid", path=cfg.DATASET.PATH)
    test_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split="test", path=cfg.DATASET.PATH)
    train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=cfg.SOLVER.TRAIN_BATCH_SIZE)
    valid_loader = DataLoader(dataset=valid_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)
    test_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)

    # ---- set model ----
    model = get_model(cfg)

    # ---- training and evaluation ----
    checkpoint_callback = ModelCheckpoint(filename="{epoch}-{step}-{valid_loss:.4f}", monitor="valid_loss", mode="min")
    trainer = pl.Trainer(
        max_epochs=cfg.SOLVER.MAX_EPOCHS,
        accelerator="gpu" if args.devices != 0 else "cpu",
        devices=args.devices if args.devices != 0 else "auto",
        logger=tb_logger,
        callbacks=[checkpoint_callback],
    )
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)
    trainer.test(dataloaders=test_loader)


if __name__ == "__main__":
    main()
</file>

<file path="examples/bindingdb_deepdta/model.py">
from kale.embed.seq_nn import CNNEncoder
from kale.pipeline.deepdta import DeepDTATrainer
from kale.predict.decode import MLPDecoder


def get_model(cfg):
    # ---- encoder hyper-parameter ----
    num_drug_embeddings = cfg.MODEL.NUM_SMILE_CHAR
    num_target_embeddings = cfg.MODEL.NUM_ATOM_CHAR
    drug_dim = cfg.MODEL.DRUG_DIM
    target_dim = cfg.MODEL.TARGET_DIM
    drug_length = cfg.MODEL.DRUG_LENGTH
    target_length = cfg.MODEL.TARGET_LENGTH
    num_filters = cfg.MODEL.NUM_FILTERS
    drug_filter_length = cfg.MODEL.DRUG_FILTER_LENGTH
    target_filter_length = cfg.MODEL.TARGET_FILTER_LENGTH

    drug_encoder = CNNEncoder(
        num_embeddings=num_drug_embeddings,
        embedding_dim=drug_dim,
        sequence_length=drug_length,
        num_kernels=num_filters,
        kernel_length=drug_filter_length,
    )

    target_encoder = CNNEncoder(
        num_embeddings=num_target_embeddings,
        embedding_dim=target_dim,
        sequence_length=target_length,
        num_kernels=num_filters,
        kernel_length=target_filter_length,
    )

    # ---- decoder hyper-parameter ----
    decoder_in_dim = cfg.MODEL.MLP_IN_DIM
    decoder_hidden_dim = cfg.MODEL.MLP_HIDDEN_DIM
    decoder_out_dim = cfg.MODEL.MLP_OUT_DIM
    dropout_rate = cfg.MODEL.MLP_DROPOUT_RATE

    decoder = MLPDecoder(
        in_dim=decoder_in_dim,
        hidden_dim=decoder_hidden_dim,
        out_dim=decoder_out_dim,
        dropout_rate=dropout_rate,
        include_decoder_layers=True,
    )

    # ---- learning rate ----
    lr = cfg.SOLVER.LR

    model = DeepDTATrainer(drug_encoder, target_encoder, decoder, lr, **cfg.MODEL, **cfg.SOLVER)

    return model
</file>

<file path="examples/bindingdb_deepdta/README.md">
# Drug-Target Interaction Prediction using DeepDTA

### 1. Description
Drug-target interaction prediction is an important research area in the field of drug discovery. It refers to predicting the binding affinity between the given chemical compounds and protein targets. In this example we train a standard DeepDTA model as a baseline in BindingDB, a public, web-accessible dataset of measured binding affinities.

### 2. DeepDTA
[DeepDTA](https://academic.oup.com/bioinformatics/article/34/17/i821/5093245) is the modeling of protein sequences and compound 1D
representations with convolutional neural networks (CNNs). The whole architecture of DeepDTA is shown below.

![DeepDTA](https://github.com/hkmztrk/DeepDTA/blob/master/docs/figures/deepdta.PNG)

### 3. Datasets
We construct **three datasets** from BindingDB distinguished by different affinity measurement metrics
(**Kd, IC50 and Ki**). They are acquired from [Therapeutics Data Commons](https://tdcommons.ai/) (TDC), which is a collection of machine learning
tasks spread across different domains of therapeutics. The data statistics is shown:

|  Metrics   | Drugs | Targets | Pairs |
|  :----:  | :----:  |   :----:  | :----:  |
| Kd  | 10,655 | 1,413 | 52,284 |
| IC50  | 549,205 | 5,078 | 991,486 |
| Ki | 174,662 | 3,070 | 375,032 |

This figure is the binding affinity distribution for the three datasets respectively, and the metrics values (x-axis) have been transformed into
log space.
![Binding affinity distribution](figures/bindingdb.jpg)

### 4. Requirements
You'll need to install the external [RDKit](https://github.com/rdkit/rdkit) package for running the example codes.

```
pip install rdkit
```

### 5. Usage
Run model for BindingDB datasets with IC50, Kd and Ki metrics respectively.
```
python main.py --cfg configs/IC50-DeepDTA.yaml
python main.py --cfg configs/Kd-DeepDTA.yaml
python main.py --cfg configs/Ki-DeepDTA.yaml
```

### 6. Results
Here are the MSE loss results for the three BindingDB datasets, and the minimal validation loss's epoch is saved as the
best checkpoint, which is applied to calculate test dataset loss. All default maximum epochs are 100.

|  Datasets   | valid_loss | test_loss | best_epoch |
|  :----:  | :----:  |   :----:  | :----:  |
| Kd  | 0.7898 | 0.7453 | 47 |
| IC50  | 0.9264 | 0.9198 | 83 |
| Ki | 1.071 | 1.072 | 91 |

### 7. Architecture
Below is the architecture of DeepDTA with default hyperparameters settings.

<pre>
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
‚îú‚îÄCNNEncoder: 1-1                        [256, 96]                 --
|    ‚îî‚îÄEmbedding: 2-1                    [256, 85, 128]            8,320
|    ‚îî‚îÄConv1d: 2-2                       [256, 32, 121]            21,792
|    ‚îî‚îÄConv1d: 2-3                       [256, 64, 114]            16,448
|    ‚îî‚îÄConv1d: 2-4                       [256, 96, 107]            49,248
|    ‚îî‚îÄAdaptiveMaxPool1d: 2-5            [256, 96, 1]              --
‚îú‚îÄCNNEncoder: 1-2                        [256, 96]                 --
|    ‚îî‚îÄEmbedding: 2-6                    [256, 1200, 128]          3,328
|    ‚îî‚îÄConv1d: 2-7                       [256, 32, 121]            307,232
|    ‚îî‚îÄConv1d: 2-8                       [256, 64, 114]            16,448
|    ‚îî‚îÄConv1d: 2-9                       [256, 96, 107]            49,248
|    ‚îî‚îÄAdaptiveMaxPool1d: 2-10           [256, 96, 1]              --
‚îú‚îÄMLPDecoder: 1-3                        [256, 1]                  --
|    ‚îî‚îÄLinear: 2-11                      [256, 1024]               197,632
|    ‚îî‚îÄDropout: 2-12                     [256, 1024]               --
|    ‚îî‚îÄLinear: 2-13                      [256, 1024]               1,049,600
|    ‚îî‚îÄDropout: 2-14                     [256, 1024]               --
|    ‚îî‚îÄLinear: 2-15                      [256, 512]                524,800
|    ‚îî‚îÄLinear: 2-16                      [256, 1]                  513
==========================================================================================
Total params: 2,244,609
Trainable params: 2,244,609
Non-trainable params: 0
Total mult-adds (M): 58.08
==========================================================================================
Input size (MB): 1.32
Forward/backward pass size (MB): 429.92
Params size (MB): 8.98
Estimated Total Size (MB): 440.21
</file>

<file path="examples/bindingdb_deepdta/tutorial.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# PyKale Tutorial: Drug-Target Interaction Prediction using DeepDTA\n",
        "\n",
        "| [Open In Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/bindingdb_deepdta/tutorial.ipynb) (click `Runtime`\u2006\u2192\u2006`Run all (Ctrl+F9)` |\n",
        "\n",
        "If using [Google Colab](https://colab.research.google.com), a free GPU can be enabled to save time via setting `Runtime`\u2006\u2192\u2006`Change runtime type` \u2192 `Hardware accelerator: GPU`"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Introduction\n",
        "Drug-target interaction prediction is an important research area in the field of drug discovery. It refers to predicting the binding affinity between the given chemical compounds and protein targets. In this example we train a standard DeepDTA model as a baseline in BindingDB, a public, web-accessible dataset of measured binding affinities.\n",
        "\n",
        "### DeepDTA\n",
        "[DeepDTA](https://academic.oup.com/bioinformatics/article/34/17/i821/5093245) is the modeling of protein sequences and compound 1D representations with convolutional neural networks (CNNs). The whole architecture of DeepDTA is shown below.\n",
        "\n",
        "![DeepDTA](https://raw.githubusercontent.com/hkmztrk/DeepDTA/master/docs/figures/deepdta.PNG)\n",
        "\n",
        "### Datasets\n",
        "We construct **three datasets** from BindingDB distinguished by different affinity measurement metrics\n",
        "(**Kd, IC50 and Ki**). They are acquired from [Therapeutics Data Commons](https://tdcommons.ai/) (TDC), which is a collection of machine learning tasks spreading across different domains of therapeutics. The data statistics is shown below:\n",
        "\n",
        "|  Metrics   | Drugs | Targets | Pairs |\n",
        "|  :----:  | :----:  |   :----:  | :----:  |\n",
        "| Kd  | 10,655 | 1,413 | 52,284 |\n",
        "| IC50  | 549,205 | 5,078 | 991,486 |\n",
        "| Ki | 174,662 | 3,070 | 375,032 |\n",
        "\n",
        "This figure is the binding affinity distribution for the three datasets respectively, where the metric values (x-axis) have been transformed into log space.\n",
        "![Binding affinity distribution](figures/bindingdb.jpg)\n",
        "This tutorial uses the (smallest) **Kd** dataset.\n",
        "\n",
        "## Setup\n",
        "\n",
        "The first few blocks of code are necessary to set up the notebook execution environment and import the required modules, including PyKale.\n",
        "\n",
        "This checks if the notebook is running on Google Colab and installs required packages."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes imgaug && pip uninstall --yes albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "    !pip install rdkit\n",
        "    !git clone https://github.com/pykale/pykale.git\n",
        "    %cd pykale\n",
        "    !pip install .[graph,example] \n",
        "    %cd examples/bindingdb_deepdta\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch==2.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -q torch_scatter==2.1.2 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html\n",
        "!pip install -q torch_sparse==0.6.18 -f https://data.pyg.org/whl/torch-2.3.0+cpu.html\n",
        "!pip install -q torch-geometric==2.3.0\n",
        "!pip install tensorboard"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This imports required modules."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from config import get_cfg_defaults\n",
        "from model import get_model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from kale.loaddata.tdc_datasets import BindingDBDataset\n",
        "from kale.utils.seed import set_seed"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "The customized configuration used in this tutorial is stored in `./configs/tutorial.yaml`, this file overwrites defaults in `config.py` where a value is specified.\n",
        "\n",
        "For saving time to run a whole pipeline in this tutorial, we sample small train/valid/test (8,000/1,000/1,000) subsets from the original BindingDB dataset."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "cfg_path = \"./configs/tutorial.yaml\"\n",
        "train_subset_size, valid_subset_size, test_subset_size = 8000, 1000, 1000\n",
        "\n",
        "cfg = get_cfg_defaults()\n",
        "cfg.merge_from_file(cfg_path)\n",
        "cfg.freeze()\n",
        "print(cfg)\n",
        "\n",
        "set_seed(cfg.SOLVER.SEED)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Check if a GPU is available\n",
        "\n",
        "If a CUDA GPU is available, this should be used to accelerate the training process. The code below checks and reports on this.\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using: \" + device)\n",
        "devices = 1 if device == \"cuda\" else 0"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Select Datasets\n",
        "\n",
        "Source and target datasets are specified using the `BindingDBDataset()` function and loaded using the `DataLoader()` function."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "train_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"train\", path=cfg.DATASET.PATH)\n",
        "valid_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"valid\", path=cfg.DATASET.PATH)\n",
        "test_dataset = BindingDBDataset(name=cfg.DATASET.NAME, split=\"test\", path=cfg.DATASET.PATH)\n",
        "train_size, valid_size, test_size = len(train_dataset), len(valid_dataset), len(test_dataset)\n",
        "train_sample_indices, valid_sample_indices, test_sample_indices = torch.randperm(train_size)[:train_subset_size].tolist(), torch.randperm(valid_size)[:valid_subset_size].tolist(), torch.randperm(test_size)[:test_subset_size].tolist()\n",
        "train_dataset, valid_dataset, test_dataset = Subset(train_dataset, train_sample_indices), Subset(valid_dataset, valid_sample_indices), Subset(test_dataset, test_sample_indices)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "cfg.DATASET.PATH"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=cfg.SOLVER.TRAIN_BATCH_SIZE)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=cfg.SOLVER.TEST_BATCH_SIZE)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup model\n",
        "\n",
        "Here, we use the previously defined configuration and dataset to set up the model we will subsequently train."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "model = get_model(cfg)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup Logger\n",
        "\n",
        "A logger is used to store output generated during and after model training. This information can be used to assess the effectiveness of the training and to identify problems."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "tb_logger = TensorBoardLogger(\"outputs\", name=cfg.DATASET.NAME)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup Trainer\n",
        "\n",
        "A trainer object is used to determine and store model parameters. Here, one is configured with information on how a model should be trained, and what hardware will be used."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "checkpoint_callback = ModelCheckpoint(filename=\"{epoch}-{step}-{valid_loss:.4f}\", monitor=\"valid_loss\", mode=\"min\")\n",
        "trainer = pl.Trainer(min_epochs=cfg.SOLVER.MIN_EPOCHS, \n",
        "                     max_epochs=cfg.SOLVER.MAX_EPOCHS,\n",
        "                     accelerator=\"gpu\" if devices != 0 else \"cpu\",\n",
        "                     logger=tb_logger,\n",
        "                     callbacks=[checkpoint_callback])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Train Model\n",
        "\n",
        "Optimize model parameters using the trainer."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "%time trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Test Optimized Model\n",
        "\n",
        "Check performance of model optimized with training data against test data which was not used in training."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "trainer.test(dataloaders=test_loader)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "You should get a test loss of $7.3\\cdots$ in root mean square error (RMSE). The target value ($y$) has a range of [-13, 20] (in log space). Thus, with only three epochs, we have learned to predict the target value with an RMSE of 7.3 over a range of [-13, 20].\n",
        "\n",
        "We set the maximum epochs to 3 and extract a subset (8000/1000/1000) to save time in running this tutorial. You may change these settings. Setting the max epochs to 100 and using the full dataset will get a much better result (<1)."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Architecture\n",
        "Below is the architecture of DeepDTA with default hyperparameters settings.\n",
        "\n",
        "<pre>\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "\u251c\u2500CNNEncoder: 1-1                        [256, 96]                 --\n",
        "|    \u2514\u2500Embedding: 2-1                    [256, 85, 128]            8,320\n",
        "|    \u2514\u2500Conv1d: 2-2                       [256, 32, 121]            21,792\n",
        "|    \u2514\u2500Conv1d: 2-3                       [256, 64, 114]            16,448\n",
        "|    \u2514\u2500Conv1d: 2-4                       [256, 96, 107]            49,248\n",
        "|    \u2514\u2500AdaptiveMaxPool1d: 2-5            [256, 96, 1]              --\n",
        "\u251c\u2500CNNEncoder: 1-2                        [256, 96]                 --\n",
        "|    \u2514\u2500Embedding: 2-6                    [256, 1200, 128]          3,328\n",
        "|    \u2514\u2500Conv1d: 2-7                       [256, 32, 121]            307,232\n",
        "|    \u2514\u2500Conv1d: 2-8                       [256, 64, 114]            16,448\n",
        "|    \u2514\u2500Conv1d: 2-9                       [256, 96, 107]            49,248\n",
        "|    \u2514\u2500AdaptiveMaxPool1d: 2-10           [256, 96, 1]              --\n",
        "\u251c\u2500MLPDecoder: 1-3                        [256, 1]                  --\n",
        "|    \u2514\u2500Linear: 2-11                      [256, 1024]               197,632\n",
        "|    \u2514\u2500Dropout: 2-12                     [256, 1024]               --\n",
        "|    \u2514\u2500Linear: 2-13                      [256, 1024]               1,049,600\n",
        "|    \u2514\u2500Dropout: 2-14                     [256, 1024]               --\n",
        "|    \u2514\u2500Linear: 2-15                      [256, 512]                524,800\n",
        "|    \u2514\u2500Linear: 2-16                      [256, 1]                  513\n",
        "==========================================================================================\n",
        "Total params: 2,244,609\n",
        "Trainable params: 2,244,609\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (M): 58.08\n",
        "==========================================================================================\n",
        "Input size (MB): 1.32\n",
        "Forward/backward pass size (MB): 429.92\n",
        "Params size (MB): 8.98\n",
        "Estimated Total Size (MB): 440.21"
      ],
      "cell_type": "markdown"
    }
  ]
}
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-CNNTransformer-huge.yaml">
TRANSFORMER:
  NUM_LAYERS: 4

CNN:
  CONV_LAYERS: [[16,3], [32,3], [64,3], [32,1], [64,3], [128,3], [256,3], [256, 1]]
  OUTPUT_SHAPE: (-1, 256, 8, 8)

OUTPUT:
  OUT_DIR: './outputs/CNNTransformer-huge'
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-CNNTransformer-large.yaml">
TRANSFORMER:
  NUM_LAYERS: 4

OUTPUT:
  OUT_DIR: './outputs/CNNTransformer-large'
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-CNNTransformer-small.yaml">
TRANSFORMER:
  NUM_LAYERS: 2

OUTPUT:
  OUT_DIR: './outputs/CNNTransformer-small'
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-CNNTransformer.yaml">
OUTPUT:
  OUT_DIR: './outputs/CNNTransformer'
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-StandardCNN-huge.yaml">
TRANSFORMER:
  USE_TRANSFORMER: False

CNN:
  CONV_LAYERS: [[16,3], [32,3], [64,3], [32,1], [64,3], [128,3], [256,3], [256, 1]]
  OUTPUT_SHAPE: (-1, 256, 8, 8)

OUTPUT:
  OUT_DIR: './outputs/StandardCNN-huge'
</file>

<file path="examples/cifar_cnntransformer/configs/CF10-StandardCNN.yaml">
TRANSFORMER:
  USE_TRANSFORMER: False

OUTPUT:
  OUT_DIR: './outputs/StandardCNN'
</file>

<file path="examples/cifar_cnntransformer/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/cifar_cnntransformer/config.py">
"""
Hyperparameter configuration file based on the YACS library.
"""
from yacs.config import CfgNode

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "./data"  # Root directory of dataset, "data" is in the same directory as this file
_C.DATASET.NAME = "CIFAR10"  # Dataset name
_C.DATASET.NUM_CLASSES = 10  # Number of classes in the dataset
_C.DATASET.NUM_WORKERS = 0  # Number of workers for data loading
_C.DATASET.DOWNLOAD = True  # Manually choose whether to download the dataset. Default set to True. Set to False for functional testing only.

# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 42
_C.SOLVER.BASE_LR = 0.05
_C.SOLVER.AD_LR = True  # default to enable multi-step learning rate decay
_C.SOLVER.LR_MILESTONES = [30, 60, 90]
_C.SOLVER.LR_GAMMA = 0.1

_C.SOLVER.TYPE = "SGD"
_C.SOLVER.WEIGHT_DECAY = 1e-4
_C.SOLVER.MOMENTUM = 0.9
_C.SOLVER.NESTEROV = False

_C.SOLVER.MAX_EPOCHS = 100
_C.SOLVER.TRAIN_BATCH_SIZE = 128
_C.SOLVER.TEST_BATCH_SIZE = 200

# ---------------------------------------------------------------------------- #
# CNN configs
# ---------------------------------------------------------------------------- #
_C.CNN = CfgNode()

# A list for each convolutional layer is given as (num_channels, kernel_size). Nested lists log to file prettier
# than nested tuples do.
_C.CNN.CONV_LAYERS = [[16, 3], [32, 3], [64, 3], [32, 1], [64, 3], [128, 3], [256, 3], [64, 1]]

# The index after which pooling layers should be positioned in the convolutional layer list. (0,3) applies 2 pooling
# layers, resulting in an image size of 8x8.
_C.CNN.POOL_LOCATIONS = (0, 3)

_C.CNN.USE_BATCHNORM = True
_C.CNN.ACTIVATION_FUN = "relu"  # one of ('relu', 'elu', 'leaky_relu')
_C.CNN.OUTPUT_SHAPE = (-1, 64, 8, 8)

# ---------------------------------------------------------------------------- #
# Transformer configs
# ---------------------------------------------------------------------------- #
_C.TRANSFORMER = CfgNode()
_C.TRANSFORMER.USE_TRANSFORMER = True  # Set False to use the CNN only
_C.TRANSFORMER.NUM_LAYERS = 3
_C.TRANSFORMER.NUM_HEADS = 2
_C.TRANSFORMER.DIM_FEEDFORWARD = 128
_C.TRANSFORMER.DROPOUT = 0.1
_C.TRANSFORMER.OUTPUT_TYPE = "spatial"

# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.PB_FRESH = 50  # Number of steps before a new progress bar is printed. Set 0 to disable the progress bar.
_C.OUTPUT.OUT_DIR = "./outputs"  # Directory to save outputs to (logs, checkpoints, etc.)

# -----------------------------------------------------------------------------
# Comet Logger (optional) - https://www.comet.ml/site/
# -----------------------------------------------------------------------------
_C.COMET = CfgNode()
_C.COMET.ENABLE = False  # Set True to enable Comet logging (requires an API key).
_C.COMET.API_KEY = ""  # Your Comet API key
_C.COMET.PROJECT_NAME = "CNN Transformer"
_C.COMET.EXPERIMENT_NAME = "CNNTransformer"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/cifar_cnntransformer/main.py">
"""This example is about the use of a CNN and a Transformer-Encoder for image classification on CIFAR10 Dataset,
using PyTorch Lightning.

Reference:
    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., & Polosukhin, I.  (2017).
    Attention Is All You Need.  In Proceedings of the Advances in Neural Information Processing Systems(pp. 6000-6010).
    https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
"""

import argparse
import time

import pytorch_lightning as pl
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar

import kale.utils.seed as seed
from kale.loaddata.image_access import get_cifar


def arg_parse():
    parser = argparse.ArgumentParser(description="CNN Transformer on CIFAR10 Dataset")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    parser.add_argument("--ckpt_resume", default=None, help="path to train checkpoint file", type=str)
    parser.add_argument("--ckpt_test", default=None, help="path to test checkpoint file", type=str)
    args = parser.parse_args()
    return args


def main():
    args = arg_parse()

    # ---- setup config ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    seed.set_seed(cfg.SOLVER.SEED)

    # ---- setup dataset ----
    train_loader, valid_loader = get_cifar(cfg)

    # ---- setup model ----
    print("==> Building model..")
    model = get_model(cfg)

    # ---- setup logger ----
    # Choose one logger (CometLogger or TensorBoardLogger) using cfg.COMET.ENABLE
    if cfg.COMET.ENABLE:
        suffix = str(int(time.time() * 1000))[6:]
        logger = pl_loggers.CometLogger(
            api_key=cfg.COMET.API_KEY,
            project_name=cfg.COMET.PROJECT_NAME,
            save_dir=cfg.OUTPUT.OUT_DIR,
            experiment_name="{}_{}".format(cfg.COMET.EXPERIMENT_NAME, suffix),
        )
    else:
        logger = pl_loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR)

    # ---- setup callbacks ----
    # setup progress bar
    progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)

    # setup learning rate monitor
    lr_monitor = LearningRateMonitor(logging_interval="epoch")

    # ---- setup trainers ----
    trainer = pl.Trainer(
        default_root_dir=cfg.OUTPUT.OUT_DIR,
        max_epochs=cfg.SOLVER.MAX_EPOCHS,
        accelerator="gpu" if args.devices != 0 else "cpu",
        devices=args.devices if args.devices != 0 else "auto",
        logger=logger,
        callbacks=[progress_bar, lr_monitor],
        strategy="ddp",  # comment this line on Windows, because Windows does not support CCL backend
        log_every_n_steps=1,
    )

    # ---- start training ----
    trainer.fit(model, train_loader, valid_loader, ckpt_path=args.ckpt_resume)

    # ---- start testing ----
    trainer.test(model, valid_loader, ckpt_path=args.ckpt_test)


if __name__ == "__main__":
    main()
</file>

<file path="examples/cifar_cnntransformer/model.py">
"""
Define and build the model based on chosen hyperparameters.
"""
from copy import deepcopy

import torch
import torch.nn as nn

from kale.embed.attention_cnn import CNNTransformer, ContextCNNGeneric
from kale.embed.image_cnn import SimpleCNNBuilder
from kale.pipeline.base_nn_trainer import CNNTransformerTrainer
from kale.predict.class_domain_nets import ClassNet


def get_config(cfg):
    """
    Sets the hyperparameter for the optimizer and experiment using the config file
    Args:
        cfg: A YACS config object.
    """
    config_params = {
        "train_params": {
            "init_lr": cfg.SOLVER.BASE_LR,
            "adapt_lr": cfg.SOLVER.AD_LR,
            "lr_milestones": cfg.SOLVER.LR_MILESTONES,
            "lr_gamma": cfg.SOLVER.LR_GAMMA,
            "max_epochs": cfg.SOLVER.MAX_EPOCHS,
            "optimizer": {
                "type": cfg.SOLVER.TYPE,
                "optim_params": {
                    "momentum": cfg.SOLVER.MOMENTUM,
                    "weight_decay": cfg.SOLVER.WEIGHT_DECAY,
                },
            },
        },
        "data_params": {
            "num_classes": cfg.DATASET.NUM_CLASSES,
        },
        "cnn_params": {
            "conv_layers_spec": cfg.CNN.CONV_LAYERS,
            "activation_fun": cfg.CNN.ACTIVATION_FUN,
            "use_batchnorm": cfg.CNN.USE_BATCHNORM,
            "pool_locations": cfg.CNN.POOL_LOCATIONS,
        },
        "transformer_params": {
            "cnn_output_shape": cfg.CNN.OUTPUT_SHAPE,
            "num_layers": cfg.TRANSFORMER.NUM_LAYERS,
            "num_heads": cfg.TRANSFORMER.NUM_HEADS,
            "dim_feedforward": cfg.TRANSFORMER.DIM_FEEDFORWARD,
            "dropout": cfg.TRANSFORMER.DROPOUT,
            "output_type": cfg.TRANSFORMER.OUTPUT_TYPE,
        },
    }

    return config_params


def get_model(cfg):
    """
    Builds and returns a model according to the config object passed.

    Args:
        cfg: A YACS config object.
    """
    config_params = get_config(cfg)
    train_params = config_params["train_params"]
    train_params_local = deepcopy(train_params)
    data_params = config_params["data_params"]
    data_params_local = deepcopy(data_params)
    cnn_params = config_params["cnn_params"]
    cnn_params_local = deepcopy(cnn_params)
    transformer_params = config_params["transformer_params"]
    transformer_params_local = deepcopy(transformer_params)

    cnn = SimpleCNNBuilder(**cnn_params_local)

    if cfg.TRANSFORMER.USE_TRANSFORMER:
        context_cnn = CNNTransformer(cnn, **transformer_params_local)
    else:
        context_cnn = ContextCNNGeneric(
            cnn,
            transformer_params_local["cnn_output_shape"],
            contextualizer=lambda x: x,
            output_type=transformer_params_local["output_type"],
        )

    classifier = ClassNet(data_params_local["num_classes"], transformer_params_local["cnn_output_shape"])
    model = CNNTransformerTrainer(feature_extractor=context_cnn, task_classifier=classifier, **train_params_local)

    return model
</file>

<file path="examples/cifar_cnntransformer/README.md">
# Image Classification: Standard CNN vs. CNN+Attention

### 1. Description
In this example we train a standard 8 layer CNN on CIFAR10 as a baseline. We then take the same CNN architecture and stack a Transformer-Encoder ontop and train this new CNNTransformer model from scratch. We present several different variants of this model where we only alter the Transformer size. Below, the validation accuracy of each model is compared.

[//]: # (![Model Comparisons]&#40;CIFAR10-ModelComparison-ValAcc-Lightning.png&#41;)
<div align="center">
	<img src="CIFAR10-ModelComparison-ValAcc-Lightning.png" alt="Editor" width="80%">
</div>
(This example is not intended to achieve any state-of-the-art result, but only a demonstration of the use of CNNTransformer.)

### 2. Observations
We observe that all models ultimately converge to similar accuracy, except for CNNTransformer-huge, which is slightly inferior. There is no distinctive difference between the learning speed of all models.

### 3. Usage
`python main.py --cfg configs/one_of_the_config_files.yaml`

### 4. Architecture
For clarity and as an example, the architecture of the CNNTransformer-Medium is shown below.

<pre>
---------------------------------------------------------------------------------------------------------
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
‚îú‚îÄCNNTransformer: 1-1                                   [-1, 64, 8, 8]            --
|    ‚îî‚îÄSimpleCNN: 2-1                                   [-1, 64, 8, 8]            --
|    |    ‚îî‚îÄModuleList: 3                               []                        --
|    |    |    ‚îî‚îÄConv2d: 4-1                            [-1, 16, 32, 32]          448
|    |    |    ‚îî‚îÄBatchNorm2d: 4-2                       [-1, 16, 32, 32]          32
|    |    |    ‚îî‚îÄReLU: 4-3                              [-1, 16, 32, 32]          --
|    |    |    ‚îî‚îÄMaxPool2d: 4-4                         [-1, 16, 16, 16]          --
|    |    |    ‚îî‚îÄConv2d: 4-5                            [-1, 32, 16, 16]          4,640
|    |    |    ‚îî‚îÄBatchNorm2d: 4-6                       [-1, 32, 16, 16]          64
|    |    |    ‚îî‚îÄReLU: 4-7                              [-1, 32, 16, 16]          --
|    |    |    ‚îî‚îÄConv2d: 4-8                            [-1, 64, 16, 16]          18,496
|    |    |    ‚îî‚îÄBatchNorm2d: 4-9                       [-1, 64, 16, 16]          128
|    |    |    ‚îî‚îÄReLU: 4-10                             [-1, 64, 16, 16]          --
|    |    |    ‚îî‚îÄConv2d: 4-11                           [-1, 32, 16, 16]          2,080
|    |    |    ‚îî‚îÄBatchNorm2d: 4-12                      [-1, 32, 16, 16]          64
|    |    |    ‚îî‚îÄReLU: 4-13                             [-1, 32, 16, 16]          --
|    |    |    ‚îî‚îÄMaxPool2d: 4-14                        [-1, 32, 8, 8]            --
|    |    |    ‚îî‚îÄConv2d: 4-15                           [-1, 64, 8, 8]            18,496
|    |    |    ‚îî‚îÄBatchNorm2d: 4-16                      [-1, 64, 8, 8]            128
|    |    |    ‚îî‚îÄReLU: 4-17                             [-1, 64, 8, 8]            --
|    |    |    ‚îî‚îÄConv2d: 4-18                           [-1, 128, 8, 8]           73,856
|    |    |    ‚îî‚îÄBatchNorm2d: 4-19                      [-1, 128, 8, 8]           256
|    |    |    ‚îî‚îÄReLU: 4-20                             [-1, 128, 8, 8]           --
|    |    |    ‚îî‚îÄConv2d: 4-21                           [-1, 256, 8, 8]           295,168
|    |    |    ‚îî‚îÄBatchNorm2d: 4-22                      [-1, 256, 8, 8]           512
|    |    |    ‚îî‚îÄReLU: 4-23                             [-1, 256, 8, 8]           --
|    |    |    ‚îî‚îÄConv2d: 4-24                           [-1, 64, 8, 8]            16,448
|    |    |    ‚îî‚îÄBatchNorm2d: 4-25                      [-1, 64, 8, 8]            128
|    |    |    ‚îî‚îÄReLU: 4-26                             [-1, 64, 8, 8]            --
|    ‚îî‚îÄSequential: 2-2                                  [-1, 2, 64]               --
|    |    ‚îî‚îÄPositionalEncoding: 3-1                     [-1, 2, 64]               --
|    |    ‚îî‚îÄDropout: 3-2                                [-1, 2, 64]               --
|    |    ‚îî‚îÄTransformerEncoder: 3-3                     [-1, 2, 64]               --
|    |    |    ‚îî‚îÄModuleList: 4                          []                        --
|    |    |    |    ‚îî‚îÄTransformerEncoderLayer: 5-1      [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄMultiheadAttention: 6-1      [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄDropout: 6-2                 [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-3               [-1, 2, 64]               128
|    |    |    |    |    ‚îî‚îÄLinear: 6-4                  [-1, 2, 128]              8,320
|    |    |    |    |    ‚îî‚îÄDropout: 6-5                 [-1, 2, 128]              --
|    |    |    |    |    ‚îî‚îÄLinear: 6-6                  [-1, 2, 64]               8,256
|    |    |    |    |    ‚îî‚îÄDropout: 6-7                 [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-8               [-1, 2, 64]               128
|    |    |    |    ‚îî‚îÄTransformerEncoderLayer: 5-2      [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄMultiheadAttention: 6-9      [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄDropout: 6-10                [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-11              [-1, 2, 64]               128
|    |    |    |    |    ‚îî‚îÄLinear: 6-12                 [-1, 2, 128]              8,320
|    |    |    |    |    ‚îî‚îÄDropout: 6-13                [-1, 2, 128]              --
|    |    |    |    |    ‚îî‚îÄLinear: 6-14                 [-1, 2, 64]               8,256
|    |    |    |    |    ‚îî‚îÄDropout: 6-15                [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-16              [-1, 2, 64]               128
|    |    |    |    ‚îî‚îÄTransformerEncoderLayer: 5-3      [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄMultiheadAttention: 6-17     [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄDropout: 6-18                [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-19              [-1, 2, 64]               128
|    |    |    |    |    ‚îî‚îÄLinear: 6-20                 [-1, 2, 128]              8,320
|    |    |    |    |    ‚îî‚îÄDropout: 6-21                [-1, 2, 128]              --
|    |    |    |    |    ‚îî‚îÄLinear: 6-22                 [-1, 2, 64]               8,256
|    |    |    |    |    ‚îî‚îÄDropout: 6-23                [-1, 2, 64]               --
|    |    |    |    |    ‚îî‚îÄLayerNorm: 6-24              [-1, 2, 64]               128
|    |    |    ‚îî‚îÄLayerNorm: 4-27                        [-1, 2, 64]               128
‚îú‚îÄPredictionHead: 1-2                                   [-1, 10]                  --
|    ‚îî‚îÄAvgPool2d: 2-3                                   [-1, 64, 1, 1]            --
|    ‚îî‚îÄLinear: 2-4                                      [-1, 10]                  650
=========================================================================================================
Total params: 482,218
Trainable params: 482,218
Non-trainable params: 0
Total mult-adds (M): 34.04
---------------------------------------------------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 1.27
Params size (MB): 1.84
Estimated Total Size (MB): 3.12</pre>


(The only model that uses a different base CNN is CNN-TR-Huge. It uses CNN-Huge which is exactly the same but the last convolutional layer has 256 channels instead of 64)
</file>

<file path="examples/cifar_isonet/configs/CF10-ISO38.yaml">
# Created by Haiping Lu by modifying https://github.com/HaozhiQi/ISONet/blob/master/configs/IN1k-ISO18.yaml (under the MIT License)
SOLVER:
  BASE_LR: 0.01
  WARMUP: True
  MAX_EPOCHS: 10  # For quick testing
ISON:
  DEPTH: 38 # From an ECCV18 paper: ResNet with depth 38, 74, 110, 152 (6n+2) for CIFAR
  DROPOUT: True
  DROPOUT_RATE: 0.1
</file>

<file path="examples/cifar_isonet/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/cifar_isonet/config.py">
"""
Default configurations for image classification using ISONet,
based on https://github.com/HaozhiQi/ISONet/blob/master/isonet/utils/config.py
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "../data"
_C.DATASET.NAME = "CIFAR10"
_C.DATASET.NUM_CLASSES = 10
_C.DATASET.NUM_WORKERS = 1

# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2020
_C.SOLVER.BASE_LR = 0.05
_C.SOLVER.LR_MILESTONES = [30, 60, 90]
_C.SOLVER.LR_GAMMA = 0.1
_C.SOLVER.WEIGHT_DECAY = 1e-4
_C.SOLVER.MOMENTUM = 0.9
_C.SOLVER.DAMPENING = False
_C.SOLVER.NESTEROV = False

_C.SOLVER.TRAIN_BATCH_SIZE = 128
_C.SOLVER.TEST_BATCH_SIZE = 200

_C.SOLVER.MAX_EPOCHS = 100

_C.SOLVER.WARMUP = False
_C.SOLVER.WARMUP_EPOCH = 5
_C.SOLVER.WARMUP_FACTOR = 0.2
# ---------------------------------------------------------------------------- #
# ISONet configs
# ---------------------------------------------------------------------------- #
_C.ISON = CfgNode()
_C.ISON.DEPTH = 34
_C.ISON.ORTHO_COEFF = 1e-4
_C.ISON.HAS_BN = False
_C.ISON.HAS_ST = False
_C.ISON.SReLU = True
_C.ISON.DIRAC_INIT = True
_C.ISON.HAS_RES_MULTIPLIER = False
_C.ISON.RES_MULTIPLIER = 1.0
_C.ISON.DROPOUT = False
_C.ISON.DROPOUT_RATE = 0.0

_C.ISON.TRANS_FUN = "basic_transform"


# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/cifar_isonet/main.py">
"""ISONet (an extension of ResNet) on CIFAR image classification

Reference: https://github.com/HaozhiQi/ISONet/blob/master/train.py
"""

import argparse
import os

import torch
from config import get_cfg_defaults
from model import get_model
from trainer import Trainer

from kale.loaddata.image_access import get_cifar
from kale.utils.logger import construct_logger
from kale.utils.seed import set_seed


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="PyTorch CIFAR10 Training")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument("--output", default="default", help="folder to save output", type=str)
    parser.add_argument("--resume", default="", type=str)
    args = parser.parse_args()
    return args


def main():
    """The main for this domain adapation example, showing the workflow"""
    args = arg_parse()

    # ---- setup device ----
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("==> Using device " + device)

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    set_seed(cfg.SOLVER.SEED)

    # ---- setup logger and output ----
    output_dir = os.path.join(cfg.OUTPUT.OUT_DIR, cfg.DATASET.NAME, args.output)
    os.makedirs(output_dir, exist_ok=True)
    logger = construct_logger("isonet", output_dir)
    logger.info("Using " + device)
    logger.info("\n" + cfg.dump())

    # ---- setup dataset ----
    train_loader, valid_loader = get_cifar(cfg)

    print("==> Building model..")
    net = get_model(cfg)
    # print(net)
    net = net.to(device)
    # model_stats = summary(net, (3, 32, 32))
    # logger.info('\n'+str(model_stats))

    # Needed even for single GPU https://discuss.pytorch.org/t/attributeerror-net-object-has-no-attribute-module/45652
    if device == "cuda":
        net = torch.nn.DataParallel(net)

    optim = torch.optim.SGD(
        net.parameters(),
        lr=cfg.SOLVER.BASE_LR,
        momentum=cfg.SOLVER.MOMENTUM,
        weight_decay=cfg.SOLVER.WEIGHT_DECAY,
        dampening=cfg.SOLVER.DAMPENING,
        nesterov=cfg.SOLVER.NESTEROV,
    )

    trainer = Trainer(device, train_loader, valid_loader, net, optim, logger, output_dir, cfg)

    if args.resume:
        # Load checkpoint
        print("==> Resuming from checkpoint..")
        cp = torch.load(args.resume)
        trainer.model.load_state_dict(cp["net"])
        trainer.optim.load_state_dict(cp["optim"])
        trainer.epochs = cp["epoch"]
        trainer.train_acc = cp["train_accuracy"]
        trainer.valid_acc = cp["test_accuracy"]

    trainer.train()


if __name__ == "__main__":
    main()
</file>

<file path="examples/cifar_isonet/model.py">
"""
Define the ISONet model for the CIFAR datasets.
"""

import kale.predict.isonet as isonet


def get_config(cfg):
    """
    Sets the hypermeters (architecture) for ISONet using the config file

    Args:
        cfg: A YACS config object.
    """
    config_params = {
        "net_params": {
            "use_dirac": cfg.ISON.DIRAC_INIT,
            "use_dropout": cfg.ISON.DROPOUT,
            "dropout_rate": cfg.ISON.DROPOUT_RATE,
            "nc": cfg.DATASET.NUM_CLASSES,
            "depths": cfg.ISON.DEPTH,
            "has_bn": cfg.ISON.HAS_BN,
            "use_srelu": cfg.ISON.SReLU,
            "transfun": cfg.ISON.TRANS_FUN,
            "has_st": cfg.ISON.HAS_ST,
        }
    }
    return config_params


# Inherite and override
class CifarIsoNet(isonet.ISONet):
    """Constructs the ISONet for CIFAR datasets

    Args:
        cfg: A YACS config object.
    """

    def __init__(self, net_params):
        super(CifarIsoNet, self).__init__(net_params)
        # define network structures (override)
        self._construct(net_params)
        # initialization
        self._network_init(net_params["use_dirac"])

    def _construct(self, net_params):
        assert (
            net_params["depths"] - 2
        ) % 6 == 0, "Model depth should be of the format 6n + 2 for cifar"  # Seems because this is a ResNet
        # Each stage has the same number of blocks for cifar

        d = int((net_params["depths"] - 2) / 6)
        # Stem: (N, 3, 32, 32) -> (N, 16, 32, 32)
        self.stem = isonet.ResStem(w_in=3, w_out=16, net_params=net_params, kernelsize=3, stride=1, padding=1)
        # Stage 1: (N, 16, 32, 32) -> (N, 16, 32, 32)
        self.s1 = isonet.ResStage(w_in=16, w_out=16, stride=1, net_params=net_params, d=d)
        # Stage 2: (N, 16, 32, 32) -> (N, 32, 16, 16)
        self.s2 = isonet.ResStage(w_in=16, w_out=32, stride=2, net_params=net_params, d=d)
        # Stage 3: (N, 32, 16, 16) -> (N, 64, 8, 8)
        self.s3 = isonet.ResStage(w_in=32, w_out=64, stride=2, net_params=net_params, d=d)
        # Head: (N, 64, 8, 8) -> (N, num_classes)
        self.head = isonet.ResHead(w_in=64, net_params=net_params)


def get_model(cfg):
    """
    Builds and returns an ISONet model for CIFAR datasets according to the config
    object passed.

    Args:
        cfg: A YACS config object.
    """

    config_params = get_config(cfg)
    net_params = config_params["net_params"]
    net = CifarIsoNet(net_params)
    return net
</file>

<file path="examples/cifar_isonet/README.md">
# Image Classification: ISONet on CIFAR10/100

### 1. Description

This example is constructed by refactoring the [Deep Isometric Learning for Visual Recognition repository](https://github.com/HaozhiQi/ISONet) for an [ICML 2020 paper](http://proceedings.mlr.press/v119/qi20a.html).

### 2. Usage

* Dataset: [CIFAR10, CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html)
* Algorithm: ISONet
* Example: CIFAR10 with ISONet38

`python main.py --cfg configs/CF10-ISO38.yaml --output CF10-ISO38`

### 3. Related `kale` API

`kale.embed.image_cnn`: Extract features from small-size (32x32) images using CNN.

`kale.loaddata.image_access`: Data loaders for CIFAR datasets.

`kale.predict.isonet`: ISONet classifier.

`kale.prepdata.image_transform`: Transforms for image data.
</file>

<file path="examples/cifar_isonet/trainer.py">
"""A standard trainer for ISONet

From: https://github.com/HaozhiQi/ISONet/blob/master/isonet/trainer.py
"""

import time

import torch
import torch.nn as nn

from kale.utils.print import pprint_without_newline, tprint


class Trainer(object):
    """Sets up a standard trainer

    Args:
        device: gpu or cpu
        train_loader: the training data loader
        valid_loader: the validation data loader
        model: the (network) model
        optim: the optimizer
        logger:: the logger to log info
        output_dir: the path to save info
        cfg: a YACS config object.
    """

    def __init__(self, device, train_loader, valid_loader, model, optim, logger, output_dir, cfg):
        # misc
        self.device = device
        self.output_dir = output_dir
        # data loader
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        # nn setting
        self.model = model
        self.optim = optim
        # lr setting
        self.criterion = nn.CrossEntropyLoss()
        # training loop settings
        self.epochs = 1
        # loss settings
        self.train_acc, self.valid_acc = [], []
        self.best_valid_acc = 0
        self.ce_loss, self.ortho_loss = 0, 0
        # others
        self.ave_time = 0
        self.logger = logger
        self.cfg = cfg

    def train(self):
        """The validation step"""
        while self.epochs <= self.cfg.SOLVER.MAX_EPOCHS:
            self.adjust_learning_rate()
            self.train_epoch()
            self.valid()
            self.epochs += 1

    def train_epoch(self):
        """One training epoch"""
        self.model.train()
        self.ce_loss = 0
        self.ortho_loss = 0
        self.ave_time = 0
        correct = 0
        total = 0
        epoch_t = time.time()
        for batch_idx, (inputs, targets) in enumerate(self.train_loader):
            iter_t = time.time()
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            self.optim.zero_grad()
            batch_size = inputs.shape[0]

            outputs = self.model(inputs)
            loss = self.loss(outputs, targets)
            loss.backward()
            self.optim.step()

            _, predicted = outputs.max(1)
            total += batch_size

            correct += predicted.eq(targets).sum().item()

            self.ave_time += time.time() - iter_t
            tprint(
                f"train Epoch: {self.epochs} | {batch_idx + 1} / {len(self.train_loader)} | "
                f"Acc: {100. * correct / total:.3f} | CE: {self.ce_loss / (batch_idx + 1):.3f} | "
                f"O: {self.ortho_loss / (batch_idx + 1):.3f} | time: {self.ave_time / (batch_idx + 1):.3f}s"
            )

        info_str = (
            f"train Epoch: {self.epochs} | Acc: {100. * correct / total:.3f} | "
            f"CE: {self.ce_loss / (batch_idx + 1):.3f} | "
            f"time: {time.time() - epoch_t:.2f}s |"
        )
        self.logger.info(info_str)
        pprint_without_newline(info_str)
        self.train_acc.append(100.0 * correct / total)

    def valid(self):
        """The validation step"""
        self.model.eval()
        self.ce_loss = 0
        self.ortho_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(self.valid_loader):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = self.model(inputs)
                # loss = self.loss(outputs, targets)

                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        if 100.0 * correct / total > self.best_valid_acc:
            self.snapshot("best")
        self.snapshot("latest")
        self.best_valid_acc = max(self.best_valid_acc, 100.0 * correct / total)
        info_str = (
            f"valid | Acc: {100. * correct / total:.3f} | "
            f"CE: {self.ce_loss / len(self.valid_loader):.3f} | "
            f"O: {self.ortho_loss / len(self.valid_loader):.3f} | "
            f"best: {self.best_valid_acc:.3f} | "
        )
        print(info_str)
        self.logger.info(info_str)
        self.valid_acc.append(100.0 * correct / total)

    def loss(self, outputs, targets):
        """Computes the loss between learning outputs and the targets (ground truth)"""
        loss = self.criterion(outputs, targets)
        self.ce_loss += loss.item()

        if self.cfg.ISON.ORTHO_COEFF > 0:
            o_loss = self.model.module.ortho(self.device)
            self.ortho_loss += o_loss.item()
            loss += o_loss * self.cfg.ISON.ORTHO_COEFF
        return loss

    def adjust_learning_rate(self):
        """Adjust the learning rate according to the configuration"""
        # if do linear warmup
        if self.cfg.SOLVER.WARMUP and self.epochs < self.cfg.SOLVER.WARMUP_EPOCH:
            lr = self.cfg.SOLVER.BASE_LR * self.epochs / self.cfg.SOLVER.WARMUP_EPOCH
        else:
            # normal (step) scheduling
            lr = self.cfg.SOLVER.BASE_LR
            for m_epoch in self.cfg.SOLVER.LR_MILESTONES:
                if self.epochs > m_epoch:
                    lr *= self.cfg.SOLVER.LR_GAMMA

        for param_group in self.optim.param_groups:
            param_group["lr"] = lr
            if "scaling" in param_group:
                param_group["lr"] *= param_group["scaling"]

    def snapshot(self, name=None):
        """Saves the current model"""
        state = {
            "net": self.model.state_dict(),
            "optim": self.optim.state_dict(),
            "epoch": self.epochs,
            "train_accuracy": self.train_acc,
            "test_accuracy": self.valid_acc,
        }
        if name is None:
            torch.save(state, f"{self.output_dir}/{self.epochs}.pt")
        else:
            torch.save(state, f"{self.output_dir}/{name}.pt")
</file>

<file path="examples/cmri_mpca/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/cmri_mpca/configs/tutorial_lr.yaml">
DATASET:
  LANDMARK_FILE: "landmarks_64x64.csv"
PIPELINE:
  CLASSIFIER: "lr"
OUTPUT:
  SAVE_FIG: True
</file>

<file path="examples/cmri_mpca/configs/tutorial_svc.yaml">
DATASET:
  LANDMARK_FILE: "landmarks_64x64.csv"
PIPELINE:
  CLASSIFIER: "svc"
OUTPUT:
  SAVE_FIG: True
</file>

<file path="examples/cmri_mpca/config.py">
"""
Default configurations for cardiac MRI data (ShefPAH) processing and classification
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.SOURCE = "https://github.com/pykale/data/raw/main/images/ShefPAH-179/SA_64x64_v2.0.zip"
_C.DATASET.ROOT = "../data"
_C.DATASET.IMG_DIR = "DICOM"
_C.DATASET.BASE_DIR = "SA_64x64_v2.0"
_C.DATASET.FILE_FORAMT = "zip"
_C.DATASET.LANDMARK_FILE = "landmarks.csv"
_C.DATASET.MASK_DIR = "Mask"
# ---------------------------------------------------------------------------- #
# Image processing
# ---------------------------------------------------------------------------- #
_C.PROC = CfgNode()
_C.PROC.SCALE = 2

# ---------------------------------------------------------------------------- #
# Visualization
# ---------------------------------------------------------------------------- #
_C.PLT_KWS = CfgNode()
_C.PLT_KWS.PLT = CfgNode()
_C.PLT_KWS.PLT.n_cols = 10

_C.PLT_KWS.IM = CfgNode()
_C.PLT_KWS.IM.cmap = "gray"

_C.PLT_KWS.MARKER = CfgNode()
_C.PLT_KWS.MARKER.marker = "+"
_C.PLT_KWS.MARKER.color = "r"
_C.PLT_KWS.MARKER.s = 100
_C.PLT_KWS.MARKER.linewidths = 1.5
_C.PLT_KWS.MARKER.edgecolors = "face"
# see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html for more options

_C.PLT_KWS.WEIGHT = CfgNode()
_C.PLT_KWS.WEIGHT.markersize = 6
_C.PLT_KWS.WEIGHT.alpha = 0.7

# ---------------------------------------------------------------------------- #
# Machine learning pipeline
# ---------------------------------------------------------------------------- #
_C.PIPELINE = CfgNode()
_C.PIPELINE.CLASSIFIER = "linear_svc"  # ["svc", "linear_svc", "lr"]

# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"  # output_dir
_C.OUTPUT.SAVE_FIG = True

_C.SAVE_FIG_KWARGS = CfgNode()
_C.SAVE_FIG_KWARGS.format = "pdf"
_C.SAVE_FIG_KWARGS.bbox_inches = "tight"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/cmri_mpca/main.py">
"""
PAH Diagnosis from Cardiac MRI via a Multilinear PCA-based Pipeline

Reference:
Swift, A. J., Lu, H., Uthoff, J., Garg, P., Cogliano, M., Taylor, J., ... & Kiely, D. G. (2021). A machine learning
cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial hypertension diagnosis.
European Heart Journal-Cardiovascular Imaging. https://academic.oup.com/ehjcimaging/article/22/2/236/5717931
"""
import argparse
import os

import numpy as np
import pandas as pd
from config import get_cfg_defaults
from sklearn.model_selection import cross_validate

from kale.interpret import model_weights, visualize
from kale.loaddata.image_access import dicom2arraylist, read_dicom_dir
from kale.pipeline.mpca_trainer import MPCATrainer
from kale.prepdata.image_transform import mask_img_stack, normalize_img_stack, reg_img_stack, rescale_img_stack
from kale.utils.download import download_file_by_url


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Machine learning pipeline for PAH diagnosis")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)

    args = parser.parse_args()
    return args


def main():
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    print(cfg)

    save_figs = cfg.OUTPUT.SAVE_FIG
    fig_format = cfg.SAVE_FIG_KWARGS.format
    print(f"Save Figures: {save_figs}")

    # ---- initialize folder to store images ----
    save_figures_location = cfg.OUTPUT.OUT_DIR
    print(f"Save Figures: {save_figures_location}")

    if not os.path.exists(save_figures_location):
        os.makedirs(save_figures_location)

    # ---- setup dataset ----
    base_dir = cfg.DATASET.BASE_DIR
    file_format = cfg.DATASET.FILE_FORAMT
    download_file_by_url(cfg.DATASET.SOURCE, cfg.DATASET.ROOT, "%s.%s" % (base_dir, file_format), file_format)

    img_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.IMG_DIR)
    patient_dcm_list = read_dicom_dir(img_path, sort_instance=True, sort_patient=True)
    images, patient_ids = dicom2arraylist(patient_dcm_list, return_patient_id=True)
    patient_ids = np.array(patient_ids, dtype=int)
    n_samples = len(images)

    mask_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.MASK_DIR)
    mask_dcm = read_dicom_dir(mask_path, sort_instance=True)
    mask = dicom2arraylist(mask_dcm, return_patient_id=False)[0][0, ...]

    landmark_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.LANDMARK_FILE)
    landmark_df = pd.read_csv(landmark_path, index_col="Subject").loc[patient_ids]  # read .csv file as dataframe
    landmarks = landmark_df.iloc[:, :-1].values
    y = landmark_df["Group"].values
    y[np.where(y != 0)] = 1  # convert to binary classification problem, i.e. no PH vs PAH

    # plot the first phase of images with landmarks
    marker_names = list(landmark_df.columns[1::2])
    markers = []
    for marker in marker_names:
        marker_name = marker.split(" ")
        marker_name.pop(-1)
        marker_name = " ".join(marker_name)
        markers.append(marker_name)

    if save_figs:
        n_img_per_fig = 45
        n_figures = int(n_samples / n_img_per_fig) + 1
        for k in range(n_figures):
            visualize.plot_multi_images(
                [images[i][0, ...] for i in range(k * n_img_per_fig, min((k + 1) * n_img_per_fig, n_samples))],
                marker_locs=landmarks[k * n_img_per_fig : min((k + 1) * n_img_per_fig, n_samples), :],
                im_kwargs=dict(cfg.PLT_KWS.IM),
                marker_cmap="Set1",
                marker_kwargs=dict(cfg.PLT_KWS.MARKER),
                marker_titles=markers,
                image_titles=list(patient_ids[k * n_img_per_fig : min((k + 1) * n_img_per_fig, n_samples)]),
                n_cols=5,
            ).savefig(
                str(save_figures_location) + "/0)landmark_visualization_%s_of_%s.%s" % (k + 1, n_figures, fig_format),
                **dict(cfg.SAVE_FIG_KWARGS),
            )

    # ---- data pre-processing ----
    # ----- image registration -----
    img_reg, max_dist = reg_img_stack(images.copy(), landmarks, landmarks[0])
    plt_kawargs = {**{"im_kwargs": dict(cfg.PLT_KWS.IM), "image_titles": list(patient_ids)}, **dict(cfg.PLT_KWS.PLT)}
    if save_figs:
        visualize.plot_multi_images([img_reg[i][0, ...] for i in range(n_samples)], **plt_kawargs).savefig(
            str(save_figures_location) + "/1)image_registration.%s" % fig_format, **dict(cfg.SAVE_FIG_KWARGS)
        )

    # ----- masking -----
    img_masked = mask_img_stack(img_reg.copy(), mask)
    if save_figs:
        visualize.plot_multi_images([img_masked[i][0, ...] for i in range(n_samples)], **plt_kawargs).savefig(
            str(save_figures_location) + "/2)masking.%s" % fig_format, **dict(cfg.SAVE_FIG_KWARGS)
        )

    # ----- resize -----
    img_rescaled = rescale_img_stack(img_masked.copy(), scale=1 / cfg.PROC.SCALE)
    if save_figs:
        visualize.plot_multi_images([img_rescaled[i][0, ...] for i in range(n_samples)], **plt_kawargs).savefig(
            str(save_figures_location) + "/3)resize.%s" % fig_format, **dict(cfg.SAVE_FIG_KWARGS)
        )

    # ----- normalization -----
    img_norm = normalize_img_stack(img_rescaled.copy())
    if save_figs:
        visualize.plot_multi_images([img_norm[i][0, ...] for i in range(n_samples)], **plt_kawargs).savefig(
            str(save_figures_location) + "/4)normalize.%s" % fig_format, **dict(cfg.SAVE_FIG_KWARGS)
        )

    # ---- evaluating machine learning pipeline ----
    x = np.concatenate([img_norm[i].reshape((1,) + img_norm[i].shape) for i in range(n_samples)], axis=0)
    trainer = MPCATrainer(classifier=cfg.PIPELINE.CLASSIFIER, n_features=200)
    cv_results = cross_validate(trainer, x, y, cv=10, scoring=["accuracy", "roc_auc"], n_jobs=1)

    print("Averaged training time: {:.4f} seconds".format(np.mean(cv_results["fit_time"])))
    print("Averaged testing time: {:.4f} seconds".format(np.mean(cv_results["score_time"])))
    print("Averaged Accuracy: {:.4f}".format(np.mean(cv_results["test_accuracy"])))
    print("Averaged AUC: {:.4f}".format(np.mean(cv_results["test_roc_auc"])))

    # ---- model weights interpretation ----
    trainer.fit(x, y)

    weights = trainer.mpca.inverse_transform(trainer.clf.coef_) - trainer.mpca.mean_
    weights = rescale_img_stack(weights, cfg.PROC.SCALE)  # rescale weights to original shape
    weights = mask_img_stack(weights, mask)  # masking weights
    top_weights = model_weights.select_top_weight(weights, select_ratio=0.02)  # select top 2% weights
    if save_figs:
        visualize.plot_weights(
            top_weights[0][0],
            background_img=images[0][0],
            im_kwargs=dict(cfg.PLT_KWS.IM),
            marker_kwargs=dict(cfg.PLT_KWS.WEIGHT),
        ).savefig(str(save_figures_location) + "/5)weights.%s" % fig_format, **dict(cfg.SAVE_FIG_KWARGS))


if __name__ == "__main__":
    main()
</file>

<file path="examples/cmri_mpca/README.md">
# PAH Diagnosis from Cardiac MRI via Multilinear PCA

## 1. Description

This example demonstrates the multilinear PCA-based machine learning pipeline for cardiac MRI analysis [1], with application in pulmonary arterial hypertension (PAH) diagnosis.

**Reference:**

[1] Swift, A. J., Lu, H., Uthoff, J., Garg, P., Cogliano, M., Taylor, J., ... & Kiely, D. G. (2021). [A machine learning cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial hypertension diagnosis](https://academic.oup.com/ehjcimaging/article/22/2/236/5717931). European Heart Journal-Cardiovascular Imaging.

## 2. Usage

* Datasets: [ShefPAH-179 v2.0 (short-axis) cardiac MRI dataset](https://github.com/pykale/data/tree/main/images/ShefPAH-179)
* Algorithms: MPCA + Linear SVM / Kernel SVM / Logistic Regression,...
* Example: Classification using SVM

`python main.py --cfg configs/tutorial_svc.yaml`

## 3. Related `kale` API

`kale.interpret.model_weights`: Get model weights for interpretation.

`kale.interpret.visualize`: Plot model weights or images.

`kale.loaddata.image_access`: Load DICOM images as ndarray data.

`kale.pipeline.mpca_trainer`: Pipeline of MPCA + feature selection + classification.

`kale.prepdata.image_transform`: CMR images pre-processing.

`kale.utils.download`: Download data.
</file>

<file path="examples/cmri_mpca/tutorial.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# PyKale Tutorial: PAH Diagnosis from Cardiac MRI (CMR) via a Multilinear PCA-based Pipeline\n",
        "| [Open in Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/cmri_mpca/tutorial.ipynb) (click `Runtime`\u2006\u2192\u2006`Run all (Ctrl+F9)` |  [Launch Binder](https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Fcmri_mpca%2Ftutorial.ipynb) (click `Run`\u2006\u2192\u2006`Run All Cells`) |"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "- Pre-processing:\n",
        "    - Registration\n",
        "    - Masking\n",
        "    - Rescaling\n",
        "    - Normalization\n",
        "- Machine learning pipeline:\n",
        "    - Multilinear principal component analysis\n",
        "    - Discriminative feature selection\n",
        "    - Linear classification model training    \n",
        "\n",
        "[See the full pipeline illustration for more details.](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/ehjcimaging/22/2/10.1093_ehjci_jeaa001/2/m_jeaa001f2.jpeg?Expires=1631272906&Signature=PKl6KLDSoNyiTy~GNtXayJCucGKhweXvGz~svHY~ThjqjbokMVCnyJMMjoGQ4C81HdUcdnJA-rcNaqmDjspUs5eAX7avG~ckkIGXqGbPWrfnaMfwywWG3EXvvH0tw9ZcFeelnWgF4lkT0RFDVgzvzhHBvefNgS0ZGwLqiGJduANJFmWIXvYgNiU6M6kRbdpOJZBltknUO~Jv43-ghqmEX7dTfOKAx6~14quDC5cgzVFfwNFRKSn0P5JZRf~wIhQ6GQ4tprl7eXuzuRHcKnFbM~UkZOtcQvVhJofCCgSDnExyS6bns9Dop39OlfQHUdY4cwn1WaSnMKEqAqQaKZ715w__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "Swift, A. J., Lu, H., Uthoff, J., Garg, P., Cogliano, M., Taylor, J., ... & Kiely, D. G. (2021). [A machine learning cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial hypertension diagnosis](https://academic.oup.com/ehjcimaging/article/22/2/236/5717931). European Heart Journal-Cardiovascular Imaging."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes imgaug && pip uninstall --yes albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "    !git clone https://github.com/pykale/pykale.git\n",
        "    %cd pykale\n",
        "    !pip install .[image,example] \n",
        "    %cd examples/cmri_mpca\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This imports required modules."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from config import get_cfg_defaults\n",
        "\n",
        "from kale.utils.download import download_file_by_url\n",
        "from kale.loaddata.image_access import dicom2arraylist, read_dicom_dir\n",
        "from kale.interpret import visualize"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Get CMR Images, Landmark Locations, and Labels"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "cfg_path = \"configs/tutorial_svc.yaml\" # Path to `.yaml` config file\n",
        "\n",
        "cfg = get_cfg_defaults()\n",
        "cfg.merge_from_file(cfg_path)\n",
        "cfg.freeze()\n",
        "print(cfg)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Download Data"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "base_dir = cfg.DATASET.BASE_DIR\n",
        "file_format = cfg.DATASET.FILE_FORAMT\n",
        "download_file_by_url(cfg.DATASET.SOURCE, cfg.DATASET.ROOT, \"%s.%s\" % (base_dir, file_format), file_format)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Read DICOM Images"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "img_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.IMG_DIR)\n",
        "patient_dcm_list = read_dicom_dir(img_path, sort_instance=True, sort_patient=True)\n",
        "images, patient_ids = dicom2arraylist(patient_dcm_list, return_patient_id=True)\n",
        "patient_ids = np.array(patient_ids, dtype=int)\n",
        "n_samples = len(images)\n",
        "\n",
        "mask_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.MASK_DIR)\n",
        "mask_dcm = read_dicom_dir(mask_path, sort_instance=True)\n",
        "mask = dicom2arraylist(mask_dcm, return_patient_id=False)[0][0, ...]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Read Landmarks and Get Labels"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "landmark_path = os.path.join(cfg.DATASET.ROOT, base_dir, cfg.DATASET.LANDMARK_FILE)\n",
        "landmark_df = pd.read_csv(landmark_path, index_col=\"Subject\").loc[patient_ids]  # read .csv file as dataframe\n",
        "landmarks = landmark_df.iloc[:, :-1].values\n",
        "y = landmark_df[\"Group\"].values\n",
        "y[np.where(y != 0)] = 1  # convert to binary classification problem, i.e. no PH vs PAH"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Visualizing Data and Landmarks"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Get landmark names from column names\n",
        "marker_names = list(landmark_df.columns[1::2])\n",
        "markers = []\n",
        "for marker in marker_names:\n",
        "    marker_name = marker.split(\" \")\n",
        "    marker_name.pop(-1)\n",
        "    marker_name = \" \".join(marker_name)\n",
        "    markers.append(marker_name)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# plot the first phase of images with landmarks\n",
        "\n",
        "n_img_per_fig = 35\n",
        "n_figures = int(n_samples / n_img_per_fig) + 1\n",
        "for k in range(n_figures):\n",
        "    visualize.plot_multi_images(\n",
        "        [images[i][0, ...] for i in range(k * n_img_per_fig, min((k + 1) * n_img_per_fig, n_samples))],\n",
        "        marker_locs=landmarks[k * n_img_per_fig: min((k + 1) * n_img_per_fig, n_samples), :],\n",
        "        im_kwargs=dict(cfg.PLT_KWS.IM),\n",
        "        marker_cmap=\"Set1\",\n",
        "        marker_kwargs=dict(cfg.PLT_KWS.MARKER),\n",
        "        marker_titles=markers,\n",
        "        image_titles=list(patient_ids[k * n_img_per_fig: min((k + 1) * n_img_per_fig, n_samples)]),\n",
        "        n_cols=5,\n",
        "    ).show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## CMR Pre-processing"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "from kale.prepdata.image_transform import mask_img_stack, normalize_img_stack, reg_img_stack, rescale_img_stack"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Image Registration"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "img_reg, max_dist = reg_img_stack(images.copy(), landmarks, landmarks[0])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "plt_kawargs = {**{\"im_kwargs\": dict(cfg.PLT_KWS.IM), \"image_titles\": list(patient_ids)}, **dict(cfg.PLT_KWS.PLT)}\n",
        "visualize.plot_multi_images([img_reg[i][0, ...] for i in range(n_samples)], **plt_kawargs).show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Masking"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "img_masked = mask_img_stack(img_reg.copy(), mask)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "visualize.plot_multi_images([img_masked[i][0, ...] for i in range(n_samples)], **plt_kawargs).show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Rescaling"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "img_rescaled = rescale_img_stack(img_masked.copy(), scale=1 / cfg.PROC.SCALE)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "visualize.plot_multi_images([img_rescaled[i][0, ...] for i in range(n_samples)], **plt_kawargs).show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Normalization"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "img_norm = normalize_img_stack(img_rescaled.copy())"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "visualize.plot_multi_images([img_norm[i][0, ...] for i in range(n_samples)], **plt_kawargs).show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## PAH Classification"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from kale.pipeline.mpca_trainer import MPCATrainer\n",
        "\n",
        "x = np.concatenate([img_norm[i].reshape((1,) + img_norm[i].shape) for i in range(n_samples)], axis=0)\n",
        "trainer = MPCATrainer(classifier=cfg.PIPELINE.CLASSIFIER, n_features=200)\n",
        "cv_results = cross_validate(trainer, x, y, cv=10, scoring=[\"accuracy\", \"roc_auc\"], n_jobs=1)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "cv_results"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "print(\"Averaged training time: {:.4f} seconds\" .format(np.mean(cv_results['fit_time'])))\n",
        "print(\"Averaged testing time: {:.4f} seconds\" .format(np.mean(cv_results['score_time'])))\n",
        "print(\"Averaged Accuracy: {:.4f}\" .format(np.mean(cv_results[\"test_accuracy\"])))\n",
        "print(\"Averaged AUC: {:.4f}\" .format(np.mean(cv_results[\"test_roc_auc\"])))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Model Interpretation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "from kale.interpret import model_weights\n",
        "\n",
        "trainer.fit(x, y)\n",
        "\n",
        "weights = trainer.mpca.inverse_transform(trainer.clf.coef_) - trainer.mpca.mean_\n",
        "weights = rescale_img_stack(weights, cfg.PROC.SCALE)  # rescale weights to original shape\n",
        "weights = mask_img_stack(weights, mask)  # masking weights\n",
        "top_weights = model_weights.select_top_weight(weights, select_ratio=0.02)  # select top 2% weights\n",
        "visualize.plot_weights(\n",
        "    top_weights[0][0],\n",
        "    background_img=images[0][0],\n",
        "    im_kwargs=dict(cfg.PLT_KWS.IM),\n",
        "    marker_kwargs=dict(cfg.PLT_KWS.WEIGHT),\n",
        ").show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
</file>

<file path="examples/digits_dann/configs/MN2UP-CDAN.yaml">
DAN:
  METHOD: "CDAN"

DATASET:
  SOURCE: "svhn"
</file>

<file path="examples/digits_dann/configs/MN2UP-DAN.yaml">
DAN:
  METHOD: "DAN"

DATASET:
  SOURCE: "svhn"
</file>

<file path="examples/digits_dann/configs/MN2UP-DANN.yaml">
DAN:
  METHOD: "DANN"

DATASET:
  SOURCE: "svhn"
</file>

<file path="examples/digits_dann/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/digits_dann/configs/tutorial.yaml">
DAN:
  METHOD: "CDAN"

DATASET:
  NUM_REPEAT: 1
  SOURCE: "svhn"
  VALID_SPLIT_RATIO: 0.5

SOLVER:
  MIN_EPOCHS: 0
  MAX_EPOCHS: 3

OUTPUT:
  PB_FRESH: 0
</file>

<file path="examples/digits_dann/config.py">
"""
Default configurations for domain adaptation
"""

import os

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "./data"  # Root directory of dataset, "data" is in the same directory as this file
_C.DATASET.NAME = "digits"  # Dataset type name
_C.DATASET.SOURCE = "mnist"  # The source dataset name
_C.DATASET.TARGET = "usps"  # The target dataset name
_C.DATASET.NUM_CLASSES = 10  # Number of classes in the dataset
_C.DATASET.NUM_REPEAT = 10  # Number of times to repeat the experiment
_C.DATASET.DIMENSION = 784
_C.DATASET.WEIGHT_TYPE = "natural"
_C.DATASET.SIZE_TYPE = "source"
_C.DATASET.VALID_SPLIT_RATIO = 0.1  # Ratio of validation set to the training dataset

# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2020
_C.SOLVER.BASE_LR = 0.001
_C.SOLVER.MOMENTUM = 0.9
_C.SOLVER.WEIGHT_DECAY = 0.0005
_C.SOLVER.NESTEROV = True

_C.SOLVER.TYPE = "SGD"
_C.SOLVER.MAX_EPOCHS = 120
# _C.SOLVER.WARMUP = True
_C.SOLVER.MIN_EPOCHS = 20
_C.SOLVER.TRAIN_BATCH_SIZE = 150
_C.SOLVER.TEST_BATCH_SIZE = 200  # No difference in ADA

# Adaptation-specific solver config
_C.SOLVER.AD_LAMBDA = True  # Set True to use adaptive lambda
_C.SOLVER.AD_LR = True  # Set True to use adaptive learning rate
_C.SOLVER.INIT_LAMBDA = 1.0  # Initial value of lambda

# ---------------------------------------------------------------------------- #
# Domain Adaptation Net (DAN) configs
# ---------------------------------------------------------------------------- #
_C.DAN = CfgNode()
_C.DAN.METHOD = "CDAN"  # choices=['CDAN', 'CDAN-E', 'DANN', 'DAN', 'JAN']
_C.DAN.USERANDOM = False
_C.DAN.RANDOM_DIM = 1024

# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.VERBOSE = False  # To discuss, for HPC jobs
_C.OUTPUT.PB_FRESH = 0  # Number of steps before a new progress bar is printed. Set 0 to disable the progress bar
_C.OUTPUT.OUT_DIR = os.path.join("outputs", _C.DATASET.SOURCE + "2" + _C.DATASET.TARGET)

# -----------------------------------------------------------------------------
# Comet Logger (optional) - https://www.comet.ml/site/
# -----------------------------------------------------------------------------
_C.COMET = CfgNode()
_C.COMET.ENABLE = False  # Set True to enable Comet logging (requires an API key).
_C.COMET.API_KEY = ""  # Your Comet API key
_C.COMET.PROJECT_NAME = "Digit DANN"
_C.COMET.EXPERIMENT_NAME = "DigitDANN"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/digits_dann/main.py">
"""This example is about domain adaptation for digit image datasets, using PyTorch Lightning.

Reference: https://github.com/thuml/CDAN/blob/master/pytorch/train_image.py
"""

import argparse
import logging
import time

import pytorch_lightning as pl
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar

from kale.loaddata.image_access import DigitDataset
from kale.loaddata.multi_domain import MultiDomainDatasets
from kale.utils.seed import set_seed


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Domain Adversarial Networks on Digits Datasets")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    args = parser.parse_args()
    return args


def main():
    """The main for this domain adaptation example, showing the workflow"""
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    print(cfg)

    # ---- setup output ----
    format_str = "@%(asctime)s %(name)s [%(levelname)s] - (%(message)s)"
    logging.basicConfig(format=format_str)

    # ---- setup dataset ----
    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(cfg.DATASET.SOURCE.upper()), DigitDataset(cfg.DATASET.TARGET.upper()), cfg.DATASET.ROOT
    )
    dataset = MultiDomainDatasets(
        source,
        target,
        config_weight_type=cfg.DATASET.WEIGHT_TYPE,
        config_size_type=cfg.DATASET.SIZE_TYPE,
        valid_split_ratio=cfg.DATASET.VALID_SPLIT_RATIO,
    )

    # Repeat multiple times to get std
    for i in range(0, cfg.DATASET.NUM_REPEAT):
        seed = cfg.SOLVER.SEED + i * 10
        # seed_everything in pytorch_lightning did not set torch.backends.cudnn
        set_seed(seed)
        print(f"==> Building model for seed {seed} ......")

        # ---- setup model ----
        model, train_params = get_model(cfg, dataset, num_channels)

        # ---- setup logger ----
        if cfg.COMET.ENABLE:
            suffix = str(int(time.time() * 1000))[6:]
            logger = pl_loggers.CometLogger(
                api_key=cfg.COMET.API_KEY,
                project_name=cfg.COMET.PROJECT_NAME,
                save_dir=cfg.OUTPUT.OUT_DIR,
                experiment_name="{}_{}".format(cfg.COMET.EXPERIMENT_NAME, suffix),
            )
        else:
            logger = pl_loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name="seed{}".format(seed))

        # ---- setup callbacks ----
        # setup checkpoint callback
        checkpoint_callback = ModelCheckpoint(
            filename="{epoch}-{step}-{valid_loss:.4f}",
            monitor="valid_loss",
            mode="min",
        )

        # setup progress bar
        progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)

        # ---- setup trainer ----
        trainer = pl.Trainer(
            min_epochs=cfg.SOLVER.MIN_EPOCHS,
            max_epochs=cfg.SOLVER.MAX_EPOCHS,
            accelerator="gpu" if args.devices != 0 else "cpu",
            devices=args.devices if args.devices != 0 else "auto",
            callbacks=[checkpoint_callback, progress_bar],
            logger=logger,
        )

        # ---- start training ----
        trainer.fit(model)

        # ---- start testing ----
        trainer.test(ckpt_path="best")


if __name__ == "__main__":
    main()
</file>

<file path="examples/digits_dann/model.py">
"""
Define the learning model and configure training parameters.
"""
# Author: Haiping Lu
# Initial Date: 27 July 2020

from copy import deepcopy

import kale.pipeline.domain_adapter as domain_adapter
from kale.embed.image_cnn import SmallCNNFeature
from kale.predict.class_domain_nets import ClassNetSmallImage, DomainNetSmallImage


def get_config(cfg):
    """
    Set the hyperparameters for the optimizer and experiment using the config file

    Args:
        cfg: A YACS config object.
    """
    config_params = {
        "train_params": {
            "adapt_lambda": cfg.SOLVER.AD_LAMBDA,
            "adapt_lr": cfg.SOLVER.AD_LR,
            "lambda_init": cfg.SOLVER.INIT_LAMBDA,
            "nb_adapt_epochs": cfg.SOLVER.MAX_EPOCHS,
            "nb_init_epochs": cfg.SOLVER.MIN_EPOCHS,
            "init_lr": cfg.SOLVER.BASE_LR,
            "batch_size": cfg.SOLVER.TRAIN_BATCH_SIZE,
            "optimizer": {
                "type": cfg.SOLVER.TYPE,
                "optim_params": {
                    "momentum": cfg.SOLVER.MOMENTUM,
                    "weight_decay": cfg.SOLVER.WEIGHT_DECAY,
                    "nesterov": cfg.SOLVER.NESTEROV,
                },
            },
        },
        "data_params": {
            "dataset_group": cfg.DATASET.NAME,
            "dataset_name": cfg.DATASET.SOURCE + "2" + cfg.DATASET.TARGET,
            "source": cfg.DATASET.SOURCE,
            "target": cfg.DATASET.TARGET,
            "size_type": cfg.DATASET.SIZE_TYPE,
            "weight_type": cfg.DATASET.WEIGHT_TYPE,
        },
    }
    return config_params


# Based on https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/utils/experimentation.py
def get_model(cfg, dataset, num_channels):
    """
    Builds and returns a model and associated hyperparameters according to the config object passed.

    Args:
        cfg: A YACS config object.
        dataset: A multidomain dataset consisting of source and target datasets.
        num_channels: The number of image channels.
    """

    # setup feature extractor
    feature_network = SmallCNNFeature(num_channels)
    # setup classifier
    feature_dim = feature_network.output_size()
    classifier_network = ClassNetSmallImage(feature_dim, cfg.DATASET.NUM_CLASSES)

    config_params = get_config(cfg)
    train_params = config_params["train_params"]
    train_params_local = deepcopy(train_params)
    method_params = {}

    method = domain_adapter.Method(cfg.DAN.METHOD)

    if method.is_mmd_method():
        model = domain_adapter.create_mmd_based(
            method=method,
            dataset=dataset,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            **method_params,
            **train_params_local,
        )
    else:
        critic_input_size = feature_dim
        # setup critic network
        if method.is_cdan_method():
            if cfg.DAN.USERANDOM:
                critic_input_size = cfg.DAN.RANDOM_DIM
            else:
                critic_input_size = feature_dim * cfg.DATASET.NUM_CLASSES
        critic_network = DomainNetSmallImage(critic_input_size)

        if cfg.DAN.METHOD == "CDAN":
            method_params["use_random"] = cfg.DAN.USERANDOM

        # The following calls kale.loaddata.dataset_access for the first time
        model = domain_adapter.create_dann_like(
            method=method,
            dataset=dataset,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            critic=critic_network,
            **method_params,
            **train_params_local,
        )
    return model, train_params
</file>

<file path="examples/digits_dann/README.md">
# Image Classification: Domain Adaptation on Digits with Lightning

### 1. Description

This example is constructed by refactoring the [ADA: (Yet) Another Domain Adaptation library](https://github.com/criteo-research/pytorch-ada), with many domain adaptation algorithms included.

### 2. Usage

* Datasets: MNIST, Modified MNIST (MNISTM), UPSP, SVHN
* Algorithms: DANN, CDAN, CDAN+E, ...
* Example: MNIST (source) to UPSP (target) using CDAN and DANN

`python main.py --cfg configs/MN2UP-CDAN.yaml --devices 1`

`python main.py --cfg configs/MN2UP-DANN.yaml --devices 1`

### 3. Related `kale` API

`kale.embed.image_cnn`: Extract features from small-size (32x32) images using CNN.

`kale.loaddata.image_access`: Data loaders for digits datasets.

`kale.loaddata.mnistm`: Data loader for the [modified MNIST] data](https://github.com/zumpchke/keras_mnistm).

`kale.loaddata.multi_domain`: Construct the dataset for (multiple) source and target domains.

`kale.loaddata.usps`: Data loader for the [USPS data](https://git-disl.github.io/GTDLBench/datasets/usps_dataset/).

`kale.pipeline.domain_adapter`: Domain adaptation pipelines for image classification.

`kale.predict.class_domain_nets`: Classifiers for data or domain.

`kale.prepdata.image_transform`: Transforms for image data.


## 4. *Sample* output CSV of 10 runs for reference

|    |                     |                     |                     |      |        |            |
|----|---------------------|---------------------|---------------------|------|--------|------------|
|    | source acc          | target acc          | domain acc          | seed | method | split      |
| 0  | 0.9879999894183128  | 0.9251666567579376  | 0.4914166614034912  | 2020 | CDAN   | Validation |
| 1  | 0.9886868991015944  | 0.9240404324664268  | 0.4774747621631832  | 2020 | CDAN   | Test       |
| 2  | 0.9888333227427212  | 0.915999990189448   | 0.4176666621933691  | 2021 | CDAN   | Validation |
| 3  | 0.988585888997477   | 0.9242424526746618  | 0.4765151661740674  | 2021 | CDAN   | Test       |
| 4  | 0.9898333227320107  | 0.9173333235085008  | 0.4382499953062505  | 2022 | CDAN   | Validation |
| 5  | 0.9884848788933596  | 0.9236363920499572  | 0.475858600497304   | 2022 | CDAN   | Test       |
| 6  | 0.9903333227266558  | 0.9256666567525826  | 0.4074166623031488  | 2023 | CDAN   | Validation |
| 7  | 0.988585888997477   | 0.9230303314252524  | 0.4785858733084751  | 2023 | CDAN   | Test       |
| 8  | 0.9899999893968924  | 0.9214999901305418  | 0.3995833290537121  | 2024 | CDAN   | Validation |
| 9  | 0.988585888997477   | 0.9237374021540744  | 0.4785858733084751  | 2024 | CDAN   | Test       |
| 10 | 0.9903333227266558  | 0.9161666568543296  | 0.3934999957855325  | 2025 | CDAN   | Validation |
| 11 | 0.988585888997477   | 0.9231313415293698  | 0.4771212267987721  | 2025 | CDAN   | Test       |
| 12 | 0.9903333227266558  | 0.9168333235138562  | 0.3583333294955082  | 2026 | CDAN   | Validation |
| 13 | 0.988585888997477   | 0.9228283112170176  | 0.4766161762781849  | 2026 | CDAN   | Test       |
| 14 | 0.9891666560724844  | 0.9136666568811052  | 0.37308332933753263 | 2027 | CDAN   | Validation |
| 15 | 0.9887879092057121  | 0.9235353819458396  | 0.4776262773193594  | 2027 | CDAN   | Test       |
| 16 | 0.9894999894022476  | 0.9154999901948032  | 0.4342499953490915  | 2028 | CDAN   | Validation |
| 17 | 0.9887879092057121  | 0.9234343718417222  | 0.4766666813302436  | 2028 | CDAN   | Test       |
| 18 | 0.9903333227266558  | 0.9254999900877008  | 0.3904166624852223  | 2029 | CDAN   | Validation |
| 19 | 0.9884848788933596  | 0.9228283112170176  | 0.4779798126837704  | 2029 | CDAN   | Test       |
| 20 | 0.11333333607763052 | 0.12000000290572645 | 0.5000000121071935  | 2020 | CDAN   | Validation |
</file>

<file path="examples/digits_dann/tutorial.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# PyKale Tutorial: Domain Adaptation on Digits with Lightning\n",
        "\n",
        "| [Open in Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/digits_dann/tutorial.ipynb) (click `Runtime`\u2006\u2192\u2006`Run all (Ctrl+F9)` | [Launch Binder](https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Fdigits_dann%2Ftutorial.ipynb) (click `Run`\u2006\u2192\u2006`Run All Cells`) |\n",
        "\n",
        "If using [Google Colab](https://colab.research.google.com), a free GPU can be enabled to save time via setting `Runtime`\u2006\u2192\u2006`Change runtime type` \u2192 `Hardware accelerator: GPU`\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[Domain Adaptation](https://en.wikipedia.org/wiki/Domain_adaptation) takes a model trained and evaluated on one set of data (the source) and adapts it to another (the target). In this tutorial, a model is trained on one digits dataset (source) and adapted to another (target). This tutorial is constructed based on the `digits_dann` example `main.py`, which is in turn refactored from the [ADA: (Yet) Another Domain Adaptation library](https://github.com/criteo-research/pytorch-ada). It has been put together to run interactively on online hosting platforms including [Google Colab](https://colab.research.google.com) or [myBinder](https://mybinder.org), but can also be downloaded and run locally. Follow the [PyKale installation instructions](https://pykale.readthedocs.io/en/latest/installation.html) for this."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "The first few blocks of code are necessary to set up the notebook execution environment and import the required modules, including PyKale.\n",
        "\n",
        "This checks if the notebook is running on Google Colab and installs required packages."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes imgaug && pip uninstall --yes albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "    !git clone https://github.com/pykale/pykale.git\n",
        "    %cd pykale\n",
        "    !pip install .[image,example] \n",
        "    %cd examples/digits_dann\n",
        "    !pip install tensorboard\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This imports required modules."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "from config import get_cfg_defaults\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SequentialSampler\n",
        "import torchvision\n",
        "\n",
        "from model import get_model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from kale.loaddata.image_access import DigitDataset\n",
        "from kale.loaddata.multi_domain import MultiDomainDatasets\n",
        "from kale.utils.seed import set_seed"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "In this tutorial we modify the [default configuration for domain adaptation problems](https://github.com/pykale/pykale/blob/main/examples/digits_dann/config.py) with a customized [`.yaml` file for the specific application in this tutorial](https://github.com/pykale/pykale/blob/main/examples/digits_dann/configs/TUTORIAL.yaml). The configuration is summarized below the following cell."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "cfg_path = \"./configs/tutorial.yaml\" # Path to `.yaml` config file\n",
        "\n",
        "cfg = get_cfg_defaults()\n",
        "cfg.merge_from_file(cfg_path)\n",
        "cfg.freeze()\n",
        "print(cfg)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Check if a GPU is available\n",
        "\n",
        "If a CUDA GPU is available, this should be used to accelerate the training process. The code below checks and reports on this."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using: \" + device)\n",
        "devices = 1 if device == \"cuda\" else 0"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Select Datasets\n",
        "\n",
        "Source and target datasets are specified using `DigitDataset.get_source_target` from values in the configuration (`cfg`) above. In this tutorial, we specify a subset of classes (1, 3 and 8) to make training and testing quicker."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "source, target, num_channels = DigitDataset.get_source_target(\n",
        "    DigitDataset(cfg.DATASET.SOURCE.upper()), DigitDataset(cfg.DATASET.TARGET.upper()), cfg.DATASET.ROOT\n",
        ")\n",
        "\n",
        "class_subset = [1, 3, 8]\n",
        "\n",
        "dataset = MultiDomainDatasets(\n",
        "    source,\n",
        "    target,\n",
        "    config_weight_type=cfg.DATASET.WEIGHT_TYPE,\n",
        "    config_size_type=cfg.DATASET.SIZE_TYPE,\n",
        "    valid_split_ratio=cfg.DATASET.VALID_SPLIT_RATIO,\n",
        "    class_ids=class_subset,\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Set Seed\n",
        "\n",
        "Some algorithms used in model training require generation of pseudo-random numbers. Setting the seed from which these are generated ensures reproducibility."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "seed = cfg.SOLVER.SEED\n",
        "# seed_everything in pytorch_lightning did not set torch.backends.cudnn\n",
        "set_seed(seed)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup Model\n",
        "\n",
        "Here, we use the previously defined configuration and dataset to set up the model we will subsequently train."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "%time model, train_params = get_model(cfg, dataset, num_channels)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Output reports on data file use."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Setup Logger\n",
        "\n",
        "A Tensorboard logger is used to store output generated during model training. This information can be used to assess the effectiveness of the training and to identify problems. The output model is stored at `cfg.OUTPUT.OUT_DIR`."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "tb_logger = TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name=\"seed{}\".format(seed))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup Checkpoint\n",
        "\n",
        "A `ModelCheckpoint` is used to save the model and some quantitative measure(s) periodically."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "checkpoint_callback = ModelCheckpoint(filename=\"{epoch}-{step}-{valid_loss:.4f}\", monitor=\"valid_loss\", mode=\"min\",)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "A `TQDMProgressBar` is used to set the progress bar. `PB_FRESH` determines at which rate (in number of batches) the progress bars get updated. Set it to ``0`` to disable the display."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Setup Trainer\n",
        "\n",
        "A trainer object is used to determine and store model parameters. Here, one is configured with information on how a model should be trained, and what hardware will be used."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "trainer = pl.Trainer(\n",
        "    min_epochs=cfg.SOLVER.MIN_EPOCHS,\n",
        "    max_epochs=cfg.SOLVER.MAX_EPOCHS,\n",
        "    callbacks=[checkpoint_callback, progress_bar],\n",
        "    logger=tb_logger,\n",
        "    accelerator=\"gpu\" if devices != 0 else \"cpu\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Output reports on available GPU and TPU resources."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Train Model\n",
        "\n",
        "Optimize model parameters using the trainer."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "%time trainer.fit(model)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Test Optimized Model\n",
        "\n",
        "Check performance of model optimized with training data against test data which was not used in training."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# test scores\n",
        "%time trainer.test()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Outputs are defined as:\n",
        "\n",
        "* 'test_domain_acc': Accuracy on classifying the domain (source or target) from which data came.\n",
        "* 'test_source_acc': Accuracy on test data drawn from the source dataset.\n",
        "* 'test_target_acc': Accuracy on test data drawn from the target dataset.\n",
        "* 'test_loss': Loss function value on the test data."
      ],
      "cell_type": "markdown"
    }
  ]
}
</file>

<file path="examples/fewshot_protonet/configs/demo.yaml">
# This is a demo config file for protonet.
# Users can modify the dataset and output paths to train their own model.
# For more adjustable hyperparameters, please refer to template.yaml.
DATASET:
  ROOT: "data/omniglot"
OUTPUT:
  OUT_DIR: "outputs"
  LOG_DIR: "logs"
  WEIGHT_DIR: "weights"
</file>

<file path="examples/fewshot_protonet/configs/template.yaml">
# This is a comprehensive config file for protonet.
# Users can modify all changeable hyperparameters to train their own model.
DATASET:
  ROOT: "data/omniglot"
  IMG_SIZE: 84
MODEL:
  BACKBONE: "resnet18"
  PRETRAIN_WEIGHTS: None
TRAIN:
  EPOCHS: 100
  OPTIMIZER: "Adam"
  LEARNING_RATE: 0.001
  NUM_CLASSES: 30
  NUM_SUPPORT_SAMPLES: 5
  NUM_QUERY_SAMPLES: 15
VAL:
  NUM_CLASSES: 5
  NUM_SUPPORT_SAMPLES: 5
  NUM_QUERY_SAMPLES: 15
OUTPUT:
  OUT_DIR: "outputs"
  LOG_DIR: "logs"
  WEIGHT_DIR: "weights"
  SAVE_FREQ: 1
  SAVE_TOP_K: 2
  SAVE_LAST: True
SEED: 1397
GPUS: 1
</file>

<file path="examples/fewshot_protonet/config.py">
"""
Default configurations for prototypical networks
"""
from yacs.config import CfgNode

# ---------------------------------------------------------------------------- #
# Environment settings
# ---------------------------------------------------------------------------- #
_C = CfgNode()
_C.SEED = 1397
_C.GPUS = 1
# ---------------------------------------------------------------------------- #
# Dataset
# ---------------------------------------------------------------------------- #
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "data/omniglot/"
_C.DATASET.IMG_SIZE = 84
# ---------------------------------------------------------------------------- #
# Model
# ---------------------------------------------------------------------------- #
_C.MODEL = CfgNode()
_C.MODEL.BACKBONE = "ResNet18Feature"
_C.MODEL.PRETRAIN_WEIGHTS = None
# ---------------------------------------------------------------------------- #
# Train
# ---------------------------------------------------------------------------- #
_C.TRAIN = CfgNode()
_C.TRAIN.EPOCHS = 100
_C.TRAIN.OPTIMIZER = "SGD"
_C.TRAIN.LEARNING_RATE = 1e-3
_C.TRAIN.NUM_CLASSES = 30
_C.TRAIN.NUM_SUPPORT_SAMPLES = 5
_C.TRAIN.NUM_QUERY_SAMPLES = 15
# ---------------------------------------------------------------------------- #
# Validation and Test
# ---------------------------------------------------------------------------- #
_C.VAL = CfgNode()
_C.VAL.NUM_CLASSES = 5
_C.VAL.NUM_SUPPORT_SAMPLES = 5
_C.VAL.NUM_QUERY_SAMPLES = 15
# ---------------------------------------------------------------------------- #
# Logger
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "outputs"
_C.OUTPUT.LOG_DIR = "logs"
_C.OUTPUT.WEIGHT_DIR = "weights"
_C.OUTPUT.SAVE_FREQ = 1
_C.OUTPUT.SAVE_TOP_K = 1
_C.OUTPUT.SAVE_LAST = True


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/fewshot_protonet/eval_unseen_classes.py">
"""
This demo tests the performance of a pretrained prototypical network on unseen classes.
Users can apply this script to evaluate their pretrained models on unseen classes without re-training.

Reference:
    Snell, J., Swersky, K. and Zemel, R., 2017. Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems, 30.
"""
import argparse
from datetime import datetime

import pytorch_lightning as pl
from config import get_cfg_defaults
from torch.utils.data import DataLoader
from torchvision.models import *

from kale.embed.image_cnn import *
from kale.loaddata.few_shot import NWayKShotDataset
from kale.pipeline.fewshot_trainer import ProtoNetTrainer
from kale.prepdata.image_transform import get_transform


def arg_parse():
    parser = argparse.ArgumentParser(description="Args of ProtoNet")
    parser.add_argument("--cfg", default="configs/demo.yaml", type=str, help="Path to the configuration file")
    parser.add_argument("--ckpt", default=None, help="Path to the checkpoint file")
    args = parser.parse_args()
    return args


def main():
    # ---- get args ----
    args = arg_parse()

    # ---- get configurations ----
    cfg_path = args.cfg
    cfg = get_cfg_defaults()
    cfg.merge_from_file(cfg_path)
    cfg.freeze()

    # ---- set model ----
    print(f"Using backbone: {cfg.MODEL.BACKBONE}")
    net = eval(f"{cfg.MODEL.BACKBONE}(weights={cfg.MODEL.PRETRAIN_WEIGHTS})")
    if cfg.MODEL.BACKBONE.startswith("resnet"):
        net.fc = Flatten()
    model = ProtoNetTrainer(net=net, devices="cuda" if cfg.GPUS > 0 else "cpu")

    # ---- set data loader ----
    transform = get_transform(kind="few-shot", augment=False)
    test_set = NWayKShotDataset(
        path=cfg.DATASET.ROOT,
        mode="test",
        num_support_samples=cfg.VAL.NUM_SUPPORT_SAMPLES,
        num_query_samples=cfg.VAL.NUM_QUERY_SAMPLES,
        transform=transform,
    )
    test_dataloader = DataLoader(test_set, batch_size=cfg.VAL.NUM_CLASSES, drop_last=True)

    # ---- set logger ----
    experiment_time = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    logger = pl.loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name=experiment_time)
    logger.log_hyperparams(cfg)

    # ---- set trainer ----
    trainer = pl.Trainer(
        devices=cfg.GPUS,
        max_epochs=cfg.TRAIN.EPOCHS,
        logger=logger,
        accelerator="gpu" if cfg.GPUS > 0 else "cpu",
        log_every_n_steps=cfg.OUTPUT.SAVE_FREQ,
    )

    # ---- testing on unseen classes ----
    trainer.test(model=model, dataloaders=test_dataloader, ckpt_path=args.ckpt)


if __name__ == "__main__":
    main()
</file>

<file path="examples/fewshot_protonet/main.py">
"""
This demo trains a prototypical network model for few-shot learning problems under N-way-K-shot settings.

- N-way: The number of classes under a particular setting. The model is presented with samples from these N classes and has to classify them. For example, 3-way means the model has to classify 3 different classes.

- K-shot: The number of samples for each class in the support set. For example, in a 2-shot setting, two support samples are provided per class.

By default, this demo uses the Omniglot dataset, which can be downloaded from https://github.com/brendenlake/omniglot.

Reference:
    Snell, J., Swersky, K. and Zemel, R., 2017. Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems, 30.
"""
import argparse
import os
from datetime import datetime
from typing import Optional

import pytorch_lightning as pl
from config import get_cfg_defaults
from torch.utils.data import DataLoader
from torchvision.models import *

from kale.embed.image_cnn import *
from kale.loaddata.few_shot import NWayKShotDataset
from kale.pipeline.fewshot_trainer import ProtoNetTrainer
from kale.prepdata.image_transform import get_transform


def arg_parse():
    parser = argparse.ArgumentParser(description="Args of ProtoNet")
    parser.add_argument("--cfg", default="configs/demo.yaml", type=str, help="Path to the configuration file")
    parser.add_argument("--ckpt", default=None, type=Optional[str], help="Path to the checkpoint file")
    args = parser.parse_args()
    return args


def main():
    # ---- get args ----
    args = arg_parse()

    # ---- get configurations ----
    cfg_path = args.cfg
    cfg = get_cfg_defaults()
    cfg.merge_from_file(cfg_path)
    cfg.freeze()

    # ---- set model ----
    net = eval(f"{cfg.MODEL.BACKBONE}(weights={cfg.MODEL.PRETRAIN_WEIGHTS})")
    if cfg.MODEL.BACKBONE.startswith("resnet"):
        net.fc = Flatten()

    model = ProtoNetTrainer(
        net=net,
        train_num_classes=cfg.TRAIN.NUM_CLASSES,
        train_num_support_samples=cfg.TRAIN.NUM_SUPPORT_SAMPLES,
        train_num_query_samples=cfg.TRAIN.NUM_QUERY_SAMPLES,
        val_num_classes=cfg.VAL.NUM_CLASSES,
        val_num_support_samples=cfg.VAL.NUM_SUPPORT_SAMPLES,
        val_num_query_samples=cfg.VAL.NUM_QUERY_SAMPLES,
        devices="cuda" if cfg.GPUS > 0 else "cpu",
        optimizer=cfg.TRAIN.OPTIMIZER,
        lr=cfg.TRAIN.LEARNING_RATE,
    )

    # ---- set data loader ----
    transform = get_transform(kind="few-shot", augment=False)

    train_set = NWayKShotDataset(
        path=cfg.DATASET.ROOT,
        mode="train",
        num_support_samples=cfg.TRAIN.NUM_SUPPORT_SAMPLES,
        num_query_samples=cfg.TRAIN.NUM_QUERY_SAMPLES,
        transform=transform,
    )
    train_dataloader = DataLoader(train_set, batch_size=cfg.TRAIN.NUM_CLASSES, shuffle=True, drop_last=True)

    val_set = NWayKShotDataset(
        path=cfg.DATASET.ROOT,
        mode="val",
        num_support_samples=cfg.VAL.NUM_SUPPORT_SAMPLES,
        num_query_samples=cfg.VAL.NUM_QUERY_SAMPLES,
        transform=transform,
    )
    val_dataloader = DataLoader(val_set, batch_size=cfg.VAL.NUM_CLASSES, drop_last=True)

    test_set = NWayKShotDataset(
        path=cfg.DATASET.ROOT,
        mode="test",
        num_support_samples=cfg.VAL.NUM_SUPPORT_SAMPLES,
        num_query_samples=cfg.VAL.NUM_QUERY_SAMPLES,
        transform=transform,
    )
    test_dataloader = DataLoader(test_set, batch_size=cfg.VAL.NUM_CLASSES, drop_last=True)

    # ---- set logger ----
    experiment_time = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    logger = pl.loggers.TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name=experiment_time)
    logger.log_hyperparams(cfg)

    # ---- set callbacks ----
    dirpath = os.path.join(cfg.OUTPUT.OUT_DIR, experiment_time, cfg.OUTPUT.WEIGHT_DIR)
    model_checkpoint = pl.callbacks.ModelCheckpoint(
        dirpath=dirpath,
        filename="{epoch}-{val_acc:.2f}",
        monitor="val_acc",
        mode="max",
        save_top_k=cfg.OUTPUT.SAVE_TOP_K,
        save_last=cfg.OUTPUT.SAVE_LAST,
        verbose=True,
    )

    # ---- set trainer ----
    trainer = pl.Trainer(
        devices=cfg.GPUS,
        max_epochs=cfg.TRAIN.EPOCHS,
        logger=logger,
        callbacks=[model_checkpoint],
        accelerator="gpu" if cfg.GPUS > 0 else "cpu",
        log_every_n_steps=cfg.OUTPUT.SAVE_FREQ,
    )

    # ---- training ----
    trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, ckpt_path=args.ckpt)

    # ---- testing ----
    trainer.test(model=model, dataloaders=test_dataloader, ckpt_path="best")


if __name__ == "__main__":
    main()
</file>

<file path="examples/fewshot_protonet/README.md">
# Prototypical Networks for Few-shot Learning

### 1. Description

This demo implements [Prototypical Networks for Few-shot Learning](https://github.com/jakesnell/prototypical-networks) within the `PyKale` framework.

ProtoNet is for few-shot learning problems under $N$-Way $K$-Shot settings:

**$N$-way**: The number of classes under a particular setting. The model is presented with samples from these $N$ classes and needs to classify them. For example, 3-way means the model has to classify 3 different classes.

**$K$-shot**: The number of samples for each class in the support set. For example, in a 2-shot setting, two support samples are provided per class.

**Support set**: It is a small, labeled dataset used to train the model with a few samples of each class. The support set consists of $N$ classes ($N$-way), with $K$ samples ($K$-shot) for each class. For example, under a 3-way-2-shot setting, the support set has 3 classes with 2 samples per class, totaling 6 samples.

**Query set**: It evaluates the model's ability to generalize what it has learned from the support set. It contains samples from the same $N$ classes but not included in the support set. Continuing with the 3-way-2-shot example, the query set would include additional samples from the 3 classes, which the model must classify after learning from the support set.

ProtoNet is a few-shot learning method that can be considered a clustering method. It learns a feature space where samples from the same class are close to each other and samples from different classes are far apart. The prototypes can be seen as the cluster centers, and the feature space is learned to make the samples cluster around these prototypes. But note that ProtoNet operates in a supervised learning context, where the goal is to classify data points based on labeled training examples. Clustering is typically an unsupervised learning task, where the objective is to group data points into clusters without prior knowledge of labels.


## 2. Usage

### Datasets

This model can be used on several few-shot learning datasets, such as mini-ImageNet ([official image list](https://drive.google.com/file/d/1iBu_Iqt49opXHSUNcTRU2WQas1WICLwQ/view)/[ready-to-download data on Kaggle](https://www.kaggle.com/datasets/arjunashok33/miniimagenet)), tiered-ImageNet ([official dataset-generating tool](https://github.com/yaoyao-liu/tiered-imagenet-tools)/[ready-to-download data on Kaggle](https://www.kaggle.com/datasets/arjun2000ashok/tieredimagenet)), and Omniglot ([official downloading code](https://github.com/brendenlake/omniglot)/[ready-to-download data on Kaggle](https://www.kaggle.com/datasets/watesoyan/omniglot)), etc. All datasets should be organized as follows.

<pre>
‚îî‚îÄ‚îÄ root
    ‚îú‚îÄ‚îÄ train
    |   ‚îú‚îÄ‚îÄ class 1
    |   |   ‚îú‚îÄ‚îÄ image 1
    |   |   ‚îú‚îÄ‚îÄ image 2
    |   |   ‚îî‚îÄ‚îÄ ...
    |   ‚îú‚îÄ‚îÄ class 2
    |   |   ‚îú‚îÄ‚îÄ image 1
    |   |   ‚îú‚îÄ‚îÄ image 2
    |   |   ‚îî‚îÄ‚îÄ ...
    |   ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ val
    |   ‚îú‚îÄ‚îÄ class m
    |   |   ‚îú‚îÄ‚îÄ image 1
    |   |   ‚îú‚îÄ‚îÄ image 2
    |   |   ‚îî‚îÄ‚îÄ...
    |   ‚îú‚îÄ‚îÄ class m+1
    |   |   ‚îú‚îÄ‚îÄ image 1
    |   |   ‚îú‚îÄ‚îÄ image 2
    |   |   ‚îî‚îÄ‚îÄ...
    |   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ test
        ‚îú‚îÄ‚îÄ class n
        |   ‚îú‚îÄ‚îÄ image 1
        |   ‚îú‚îÄ‚îÄ image 2
        |   ‚îî‚îÄ‚îÄ ...
        ‚îú‚îÄ‚îÄ class n+1
        |   ‚îú‚îÄ‚îÄ image 1
        |   ‚îú‚îÄ‚îÄ image 2
        |   ‚îî‚îÄ‚îÄ ...
        ‚îî‚îÄ‚îÄ ...
</pre>

### Examples

#### Single GPU training:

Example - Training a ResNet18-based ProtoNet on Omniglot under a 5-way-5-shot setting:

`python main.py --cfg configs/demo.yaml --gpus 1`

Customized running:

`python main.py --cfg configs/template.yaml --gpus 1`

- `demo.yaml` is a demo configuration file. Change `DATASET.ROOT` to fit your dataset directory for a quick demo running.
- `template.yaml` contains all changeable hyperparameters. It allows users to customize the model accordingly.
- Available backbones: any `resnet` structures from `torchvision.models` or `kale.embed.image_cnn`.

#### Test

Example - Testing the pretrained ResNet18-based ProtoNet on unseen classes in Omniglot under a 5-way-5-shot setting:

`python eval_unseen_classes.py --cfg configs/demo.yaml --gpus 1 --ckpt {path to ckpt file}`

Customized running:

`python eval_unseen_classes.py --cfg configs/template.yaml --gpus 1 --ckpt {path to ckpt file}`

The test hyperparameters are the same as the `VAL` section of the config file.

##### Note
If no `test` folder in the dataset, choose one of the following options:
- Use the `val` set as the test set. Copy and paste the `val` folder and rename it as `test`.
- Change the `mode` in `test_set = NWayKShotDataset(..., mode="test", ...)` in `eval_unseen_classes.py` or `main.py` to `val`.

## 3. Related `kale` API

- `kale.loaddata.few_shot`: Dataset class for few-shot learning problems under $N$-way $K$-shot settings.
- `kale.embed.image_cnn`: ResNet feature extractors.
- `kale.pipeline.fewshot_trainer`: ProtoNet trainer in `pl.LightningModule` style.
- `kale.evaluate.metrics.protonet_loss`: Computing the loss and accuracy for ProtoNet.

## Reference
[Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)
```
@inproceedings{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
 }
```
</file>

<file path="examples/landmark_uncertainty/configs/4CH_data.yaml">
# Configuration option for the 4CH data.
# See the documentation for full information on the configuration file options:
# pykale/examples/landmark_uncertainty/README.md

DATASET:
  DATA: "4CH"

PIPELINE:
  # NUM_QUANTILE_BINS: [5]
  NUM_QUANTILE_BINS: [2,3,5,10,20]
  COMBINE_MIDDLE_BINS: False
  PIXEL_TO_MM_SCALE: 1.0
  IND_LANDMARKS_TO_SHOW: [0,1,2]
  COMPARE_Q_VALUES: True
  COMPARE_INDIVIDUAL_Q: True
  COMPARE_Q_MODELS: ["PHD-NET", "U-NET"]
  COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]
  INDIVIDUAL_Q_MODELS: ["U-NET", "PHD-NET"]
  SHOW_IND_LANDMARKS: True
  INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS: [
    ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
    ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
    ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"],
  ]

BOXPLOT:
  ERROR_LIM: 256
  SAMPLES_AS_DOTS: False
  SHOW_SAMPLE_INFO_MODE: "Average"

OUTPUT:
  SAVE_FIGURES: False # True to save, False to visualize in matplotlib
  SAVE_PREPEND: "8std_10_08_22"
</file>

<file path="examples/landmark_uncertainty/configs/isbi_config.yaml">
# Configuration option for the ISBI Cephalometric data.
# See the documentation for full information on the configuration file options:
# pykale/examples/landmark_uncertainty/README.md


DATASET:
    DATA: "ISBI_2std"
    LANDMARKS: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]
    NUM_FOLDS: 4
    UE_PAIRS_VAL: "checkpointuncertainty_pairs_valid"
    UE_PAIRS_TEST: "checkpointuncertainty_pairs_test"

PIPELINE:
    NUM_QUANTILE_BINS: [3,5,10]
    # NUM_QUANTILE_BINS: [5]
    COMBINE_MIDDLE_BINS: False
    PIXEL_TO_MM_SCALE: 0.1
    IND_LANDMARKS_TO_SHOW: [8,9,2,11,14]
    # IND_LANDMARKS_TO_SHOW: [8]
    COMPARE_Q_VALUES: True
    COMPARE_INDIVIDUAL_Q: True
    COMPARE_Q_MODELS: ["U-NET"]
    COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]
    # COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"]]
    INDIVIDUAL_Q_MODELS: ["U-NET"]
    INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS: [
        ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
        ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
        ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"],
    ]
    SHOW_IND_LANDMARKS: True

OUTPUT:
    SAVE_FIGURES: True # True to save, False to visualize in matplotlib
    SAVE_PREPEND: "2std_10_08_22"

BOXPLOT:
    ERROR_LIM: 32
    SAMPLES_AS_DOTS: False
    SHOW_SAMPLE_INFO_MODE: "Average" #["All", "Average"]
</file>

<file path="examples/landmark_uncertainty/configs/no_gt_test_example.yaml">
# Configuration option for the 4CH data where the Ground Truth (GT) labels for the test data
# are not avaliable. Useful example for inference time where you want to bin by uncertainty.
# See the documentation for full information on the configuration file options:
# pykale/examples/landmark_uncertainty/README.md

DATASET:
  DATA: "4CH"
  NUM_FOLDS: 1
  GROUND_TRUTH_TEST_ERRORS_AVAILABLE: False
  SOURCE: https://github.com/pykale/data/blob/landmark-data/tabular/cardiac_landmark_uncertainty/Uncertainty_tuples.zip?raw=true"
  BASE_DIR: "Uncertainty_tuples"

PIPELINE:
  NUM_QUANTILE_BINS: [5,10,20]
  COMBINE_MIDDLE_BINS: False
  PIXEL_TO_MM_SCALE: 1.0
  IND_LANDMARKS_TO_SHOW: [0,1,2]
  COMPARE_Q_VALUES: True
  COMPARE_INDIVIDUAL_Q: True
  COMPARE_Q_MODELS: ["PHD-NET-NO-GT"]
  COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]
  INDIVIDUAL_Q_MODELS: ["PHD-NET-NO-GT"]
  SHOW_IND_LANDMARKS: True
  INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS: [
    ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
    ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
    ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"],
  ]

BOXPLOT:
  ERROR_LIM: 256
  SAMPLES_AS_DOTS: False
  SHOW_SAMPLE_INFO_MODE: "Average"

OUTPUT:
  SAVE_FIGURES: True # True to save, False to visualize in matplotlib
  SAVE_PREPEND: "8std_10_08_22"
</file>

<file path="examples/landmark_uncertainty/configs/one_target_example.yaml">
# Configuration option for the 4CH data.
# See the documentation for full information on the configuration file options:
# pykale/examples/landmark_uncertainty/README.md

DATASET:
  DATA: "4CH"
  LANDMARKS: [0]

PIPELINE:
  # NUM_QUANTILE_BINS: [5]
  NUM_QUANTILE_BINS: [2,3,5,10,20]
  COMBINE_MIDDLE_BINS: False
  PIXEL_TO_MM_SCALE: 1.0
  IND_LANDMARKS_TO_SHOW: [0]
  COMPARE_Q_VALUES: True
  COMPARE_INDIVIDUAL_Q: True
  COMPARE_Q_MODELS: ["PHD-NET", "U-NET"]
  COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]
  INDIVIDUAL_Q_MODELS: ["U-NET", "PHD-NET"]
  SHOW_IND_LANDMARKS: True
  INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS: [
    ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
    ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
    ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"],
  ]

BOXPLOT:
  ERROR_LIM: 256
  SAMPLES_AS_DOTS: False
  SHOW_SAMPLE_INFO_MODE: "Average"

OUTPUT:
  SAVE_FIGURES: True # True to save, False to visualize in matplotlib
  SAVE_PREPEND: "1_target_example"
</file>

<file path="examples/landmark_uncertainty/configs/SA_data.yaml">
# Configuration option for the SA data.
# See the documentation for full information on the configuration file options:
# pykale/examples/landmark_uncertainty/README.md

DATASET:
  DATA: "SA"

PIPELINE:
  NUM_QUANTILE_BINS: [2,3,5,10,20]
  COMBINE_MIDDLE_BINS: False
  PIXEL_TO_MM_SCALE: 1.0
  IND_LANDMARKS_TO_SHOW: [0,1,2]
  COMPARE_Q_VALUES: True
  COMPARE_INDIVIDUAL_Q: True
  COMPARE_Q_MODELS: ["PHD-NET", "U-NET"]
  COMPARE_Q_UNCERTAINTY_ERROR_PAIRS: [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]
  INDIVIDUAL_Q_MODELS: ["PHD-NET", "U-NET"]
  SHOW_IND_LANDMARKS: True

BOXPLOT:
  ERROR_LIM: 128
  SAMPLES_AS_DOTS: False
  SHOW_SAMPLE_INFO_MODE: "Average"

OUTPUT:
  SAVE_FIGURES: True # True to save, False to visualize in matplotlib
  SAVE_PREPEND: "8std_10_08_22"
</file>

<file path="examples/landmark_uncertainty/config.py">
"""
Default configurations for uncertainty estimation using Quantile Binning.
See the documentation for full information on the configuration file options:
pykale/examples/landmark_uncertainty/README.md
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.SOURCE = (
    "https://github.com/pykale/data/raw/main/tabular/cardiac_landmark_uncertainty/Uncertainty_tuples.zip"
)
_C.DATASET.ROOT = "../../../data/landmarks/"
_C.DATASET.BASE_DIR = "Uncertainty_tuples"


_C.DATASET.FILE_FORMAT = "zip"

_C.DATASET.CONFIDENCE_INVERT = [["S-MHA", True], ["E-MHA", True], ["E-CPV", False]]


_C.DATASET.DATA = "4CH"
_C.DATASET.LANDMARKS = [0, 1, 2]
_C.DATASET.NUM_FOLDS = 8
_C.DATASET.GROUND_TRUTH_TEST_ERRORS_AVAILABLE = True


_C.DATASET.UE_PAIRS_VAL = "uncertainty_pairs_valid"
_C.DATASET.UE_PAIRS_TEST = "uncertainty_pairs_test"


# ---------------------------------------------------------------------------- #
# Uncertainty Estimation Pipeline Parameters
# ---------------------------------------------------------------------------- #
_C.PIPELINE = CfgNode()

# Can choose to evaluate over a single value or multiple values for Q (# bins). You can:
# 1) Evaluate over each value of Q (set COMPARE_INDIVIDUAL_Q = True). For each Q it will compare DATASET.MODELS and DATASET.UNCERTAINTY_ERROR_PAIRS against each other.
# 2) Compare results of a single model and a single uncertainty error pair (set COMPARE_Q_VALUES = True).

_C.PIPELINE.NUM_QUANTILE_BINS = [5, 10, 25]

# ~# 1)
# Compare uncertainty measures AND models over each single value of Q?
_C.PIPELINE.COMPARE_INDIVIDUAL_Q = True
# [NAME, KEY (error in csv), KEY (uncertainty in csv)]
_C.PIPELINE.INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS = [
    ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
    ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
    ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"],
]
# Key for model name found in path.
_C.PIPELINE.INDIVIDUAL_Q_MODELS = ["U-NET", "PHD-NET"]
# ~#


# 2)
# Compare a single uncertainty measure on single model through various values of Q bins e.g. 5, 10, 20
_C.PIPELINE.COMPARE_Q_VALUES = True
_C.PIPELINE.COMPARE_Q_MODELS = ["PHD-NET"]  # Which model to compare over values of Q.
_C.PIPELINE.COMPARE_Q_UNCERTAINTY_ERROR_PAIRS = [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"]]
# ~#


_C.PIPELINE.COMBINE_MIDDLE_BINS = False
_C.PIPELINE.PIXEL_TO_MM_SCALE = 1.0
_C.PIPELINE.IND_LANDMARKS_TO_SHOW = [-1]  # -1 means show all landmarks individually, [] means show none
_C.PIPELINE.SHOW_IND_LANDMARKS = True

# ---------------------------------------------------------------------------- #
# Visualization
# ---------------------------------------------------------------------------- #
_C.IM_KWARGS = CfgNode()
_C.IM_KWARGS.cmap = "gray"

_C.MARKER_KWARGS = CfgNode()
_C.MARKER_KWARGS.marker = "o"
_C.MARKER_KWARGS.markerfacecolor = (1, 1, 1, 0.1)
_C.MARKER_KWARGS.markeredgewidth = 1.5
_C.MARKER_KWARGS.markeredgecolor = "r"

_C.WEIGHT_KWARGS = CfgNode()
_C.WEIGHT_KWARGS.markersize = 6
_C.WEIGHT_KWARGS.alpha = 0.7

_C.BOXPLOT = CfgNode()
_C.BOXPLOT.SAMPLES_AS_DOTS = True
_C.BOXPLOT.ERROR_LIM = 64
_C.BOXPLOT.SHOW_SAMPLE_INFO_MODE = "Average"  # "None", "All", "Average"


# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()


_C.OUTPUT.OUT_DIR = "./outputs/"
_C.OUTPUT.SAVE_PREPEND = "example"
_C.OUTPUT.SAVE_FIGURES = True  # True to save, False to visualize in matplotlib


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/landmark_uncertainty/main.py">
"""
Uncertainty Estimation for Landmark Localization


Reference:
L. A. Schobs, A. J. Swift and H. Lu,
"Uncertainty Estimation for Heatmap-Based Landmark Localization,"
in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034,
April 2023, doi: 10.1109/TMI.2022.3222730.

Paper link: https://arxiv.org/abs/2203.02351
"""

import argparse
import os
import warnings

import numpy as np
import pandas as pd
import seaborn as sns
from config import get_cfg_defaults
from pandas import *

import kale.utils.logger as logging
from kale.embed.uncertainty_fitting import fit_and_predict
from kale.interpret.uncertainty_quantiles import generate_fig_comparing_bins, generate_fig_individual_bin_comparison
from kale.utils.download import download_file_by_url

warnings.filterwarnings("error")


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Quantile Binning for landmark uncertainty estimation.")
    parser.add_argument("--cfg", required=False, help="path to config file", type=str)

    args = parser.parse_args()

    """Example:
    python main.py

    To use a custom config, or a config file provided in the configs folder:
    python main.py --cfg ../configs/isbi_config.yaml

    """
    return args


def main():
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    if args.cfg:
        cfg.merge_from_file(args.cfg)
    cfg.freeze()

    # ---- setup output ----
    os.makedirs(cfg.OUTPUT.OUT_DIR, exist_ok=True)
    logger = logging.construct_logger("q_bin", cfg.OUTPUT.OUT_DIR, log_to_terminal=True)
    logger.info(cfg)

    # ---- setup dataset ----
    base_dir = cfg.DATASET.BASE_DIR

    # download data if neccesary
    if cfg.DATASET.SOURCE != None:
        logger.info("Downloading data...")
        data_file_name = "%s.%s" % (base_dir, cfg.DATASET.FILE_FORMAT)
        download_file_by_url(
            cfg.DATASET.SOURCE,
            cfg.DATASET.ROOT,
            data_file_name,
            file_format=cfg.DATASET.FILE_FORMAT,
        )
        logger.info("Data downloaded to %s!", cfg.DATASET.ROOT + base_dir)

    uncertainty_pairs_val = cfg.DATASET.UE_PAIRS_VAL
    uncertainty_pairs_test = cfg.DATASET.UE_PAIRS_TEST
    gt_test_error_available = cfg.DATASET.GROUND_TRUTH_TEST_ERRORS_AVAILABLE

    ind_q_uncertainty_error_pairs = cfg.PIPELINE.INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS
    ind_q_models_to_compare = cfg.PIPELINE.INDIVIDUAL_Q_MODELS

    compare_q_uncertainty_error_pairs = cfg.PIPELINE.COMPARE_Q_UNCERTAINTY_ERROR_PAIRS
    compare_q_models_to_compare = cfg.PIPELINE.COMPARE_Q_MODELS

    dataset = cfg.DATASET.DATA
    landmarks = cfg.DATASET.LANDMARKS
    num_folds = cfg.DATASET.NUM_FOLDS

    ind_landmarks_to_show = cfg.PIPELINE.IND_LANDMARKS_TO_SHOW

    pixel_to_mm_scale = cfg.PIPELINE.PIXEL_TO_MM_SCALE

    # Define parameters for visualization
    cmaps = sns.color_palette("deep", 10).as_hex()

    if gt_test_error_available:
        fit = True
        evaluate = True
        interpret = True
    else:
        fit = True
        evaluate = False
        interpret = False

    show_individual_landmark_plots = cfg.PIPELINE.SHOW_IND_LANDMARKS

    for num_bins in cfg.PIPELINE.NUM_QUANTILE_BINS:
        # create the folder to save to
        save_folder = os.path.join(cfg.OUTPUT.OUT_DIR, dataset, str(num_bins) + "Bins")

        # ---- This is the Fitting Phase ----
        if fit:
            # Fit all the options for the individual Q selection and comparison Q selection

            all_models_to_compare = np.unique(ind_q_models_to_compare + compare_q_models_to_compare)
            all_uncert_error_pairs_to_compare = np.unique(
                ind_q_uncertainty_error_pairs + compare_q_uncertainty_error_pairs, axis=0
            )

            for model in all_models_to_compare:
                for landmark in landmarks:
                    # Define Paths for this loop
                    landmark_results_path_val = os.path.join(
                        cfg.DATASET.ROOT, base_dir, model, dataset, uncertainty_pairs_val + "_l" + str(landmark)
                    )
                    landmark_results_path_test = os.path.join(
                        cfg.DATASET.ROOT, base_dir, model, dataset, uncertainty_pairs_test + "_l" + str(landmark)
                    )

                    fitted_save_at = os.path.join(save_folder, "fitted_quantile_binning", model, dataset)
                    os.makedirs(save_folder, exist_ok=True)

                    uncert_boundaries, estimated_errors, predicted_bins = fit_and_predict(
                        landmark,
                        all_uncert_error_pairs_to_compare,
                        landmark_results_path_val,
                        landmark_results_path_test,
                        num_bins,
                        cfg,
                        groundtruth_test_errors=gt_test_error_available,
                        save_folder=fitted_save_at,
                    )

        ############ Evaluation Phase ##########################

        if evaluate:
            # Get results for each individual bin.
            if cfg.PIPELINE.COMPARE_INDIVIDUAL_Q:
                comparisons_models = "_".join(ind_q_models_to_compare)

                comparisons_um = [str(x[0]) for x in ind_q_uncertainty_error_pairs]
                comparisons_um = "_".join(comparisons_um)

                save_file_preamble = "_".join(
                    [
                        cfg.OUTPUT.SAVE_PREPEND,
                        "ind",
                        dataset,
                        comparisons_models,
                        comparisons_um,
                        "combined" + str(cfg.PIPELINE.COMBINE_MIDDLE_BINS),
                    ]
                )

                generate_fig_individual_bin_comparison(
                    data=[
                        ind_q_uncertainty_error_pairs,
                        ind_q_models_to_compare,
                        dataset,
                        landmarks,
                        num_bins,
                        cmaps,
                        os.path.join(save_folder, "fitted_quantile_binning"),
                        save_file_preamble,
                        cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"],
                        cfg["OUTPUT"]["SAVE_FIGURES"],
                        cfg["DATASET"]["CONFIDENCE_INVERT"],
                        cfg["BOXPLOT"]["SAMPLES_AS_DOTS"],
                        cfg["BOXPLOT"]["SHOW_SAMPLE_INFO_MODE"],
                        cfg["BOXPLOT"]["ERROR_LIM"],
                        show_individual_landmark_plots,
                        interpret,
                        num_folds,
                        ind_landmarks_to_show,
                        pixel_to_mm_scale,
                    ],
                    display_settings={
                        "cumulative_error": True,
                        "errors": True,
                        "jaccard": True,
                        "error_bounds": True,
                        "correlation": True,
                    },
                )

            # If we are comparing bins against each other, we need to wait until all the bins have been fitted.
            if cfg.PIPELINE.COMPARE_Q_VALUES and num_bins == cfg.PIPELINE.NUM_QUANTILE_BINS[-1]:
                for c_model in compare_q_models_to_compare:
                    for c_er_pair in compare_q_uncertainty_error_pairs:
                        save_file_preamble = "_".join(
                            [
                                cfg.OUTPUT.SAVE_PREPEND,
                                "compQ",
                                c_model,
                                c_er_pair[0],
                                dataset,
                                "combined" + str(cfg.PIPELINE.COMBINE_MIDDLE_BINS),
                            ]
                        )

                        all_fitted_save_paths = [
                            os.path.join(cfg.OUTPUT.OUT_DIR, dataset, str(x_bins) + "Bins", "fitted_quantile_binning")
                            for x_bins in cfg.PIPELINE.NUM_QUANTILE_BINS
                        ]

                        hatch_type = "o" if "PHD-NET" == c_model else ""
                        color = (
                            cmaps[0] if c_er_pair[0] == "S-MHA" else cmaps[1] if c_er_pair[0] == "E-MHA" else cmaps[2]
                        )
                        save_folder_comparison = os.path.join(cfg.OUTPUT.OUT_DIR, dataset, "ComparisonBins")
                        os.makedirs(save_folder_comparison, exist_ok=True)

                        logger.info("Comparison Q figures for: %s and %s ", c_model, c_er_pair)
                        generate_fig_comparing_bins(
                            data=[
                                c_er_pair,
                                c_model,
                                dataset,
                                landmarks,
                                cfg.PIPELINE.NUM_QUANTILE_BINS,
                                cmaps,
                                all_fitted_save_paths,
                                save_folder_comparison,
                                save_file_preamble,
                                cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"],
                                cfg["OUTPUT"]["SAVE_FIGURES"],
                                cfg["BOXPLOT"]["SAMPLES_AS_DOTS"],
                                cfg["BOXPLOT"]["SHOW_SAMPLE_INFO_MODE"],
                                cfg["BOXPLOT"]["ERROR_LIM"],
                                show_individual_landmark_plots,
                                interpret,
                                num_folds,
                                ind_landmarks_to_show,
                                pixel_to_mm_scale,
                            ],
                            display_settings={
                                "cumulative_error": True,
                                "errors": True,
                                "jaccard": True,
                                "error_bounds": True,
                                "hatch": hatch_type,
                                "color": color,
                            },
                        )


if __name__ == "__main__":
    main()
</file>

<file path="examples/landmark_uncertainty/README.md">
# Uncertainty Estimation in Landmark Localization


## 1. Description

In this example we implement the methods from [Uncertainty Estimation for Heatmap-based Landmark Localization](https://arxiv.org/abs/2203.02351) [1]. The method is Quantile Binning, which bins landmark predictions by any continuous uncertainty estimation measure. We assign each bin estimated localization error bounds. We can use these bins to filter out the worst predictions, or identify the likely best predictions.

We evaluate how well an uncertainty measure predicts localization error by measuring the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) (a similarity measure) between the predicted bins and the ground truth error quantiles. We also evaluate the accuracy of the estimated error bounds. This framework is applicable to any dataset consisting of (*Continuous Uncertainty Measure*, *Continuous Evaluation Metric*) tuples.

Fig. 1 depicts the features exemplified in this example. Note that **a)** and **b)** are precomputed and provided in tabular form for this example.

|![Quantile Binning Framework](figures/quantile_binning.png)|
|:--:|
| Fig. 1: Overview of our general Quantile Binning framework. **a)** We make a prediction using a heatmap-based landmark localization model, and **b)**  extract a continuous uncertainty measure. **c)**  We learn thresholds to categorize predictions into bins of increasing uncertainty, estimating error bounds for each bin. **e)**  We filter out predictions from high uncertainty bins to improve the proportion of acceptable predictions. **d)**  Finally, we evaluate each uncertainty measure's ability to capture the true error quantiles and the accuracy of the estimated error bounds.|

## 2. Datasets

We provide three tabular datasets containing landmark localization error and uncertainty estimation values: 1) 303 Short Axis View Cardiac Magnetic Resonance (CMR) images with 3 landmarks (SA), 422 Four Chamber View CMR images with 3 landmarks (4ch), and 400 Cephalometric Radiology images with 19 landmarks (Cephalometric). The CMR dataset is from the [ASPIRE Registry](https://erj.ersjournals.com/content/39/4/945) [2], and the Cephalometric dataset is from an [ISBI grand challenge](https://www.researchgate.net/publication/296621456_A_benchmark_for_comparison_of_dental_radiography_analysis_algorithms) [3].

For each uncertainty measure we provide tuples of (*Continuous Uncertainty Measure*, *Continuous Localization Error*) for each sample in the validation and test set in tabular form. We have split the data into 8 folds and used cross validation to gather validation and test set uncertainty tuples for every sample in the datasets.

In this example, we compare the uncertainty measures:

- Single Maximum Heatmap Activation (S-MHA).
- Ensemble Maximum Heatmap Activation (E-MHA).
- Ensemble Coordinate Prediction Variance (E-CPV).

We compare these measures on landmark predictions from:

- A [U-Net model](https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf) [4].
- A [PHD-Net model](https://ieeexplore.ieee.org/document/9433895/) [5].

We also provide an example where the ground truth test error is not available under PHD-NET-NO-GT/.

The full README for the datasets can be found [here](https://github.com/pykale/data/tree/main/tabular/cardiac_landmark_uncertainty).

## 3. Usage

Run Quantile Binning for the Four Chamber data, Short Axis data, Cephalometric data, and Four Chamber No Ground Truth Test labels data respectively:

```bash
python main.py --cfg configs/4CH_data.yaml
python main.py --cfg configs/SA_data.yaml
python main.py --cfg configs/isbi_config.yaml
python main.py --cfg configs/no_gt_test_example.yaml
```


Edit the above yaml files for additional configuration options.

 ## 4. Additional Experimental Results
 Find additional experimental results from our paper in this repository [additional results](https://github.com/Schobs/Qbin).

## 5. Quantile Binning Beyond Landmarks
Quantile Binning can be used for purposes beyond landmarks. Simply follow the same format for your data as detailed [here](https://github.com/pykale/data/tree/main/tabular/cardiac_landmark_uncertainty). In a nutshell, each sample in the .csv should contain a column for the continuous uncertainty measure and a column for the continuous evaluation metric. Multiple uncertainty measures or evaluation metrics can be present for one sample (e.g. results from Monte Carlo Dropout and Deep Ensembles).

In your config.yaml file, specify the \<uncertainty_measure, evaluation_metric\> tuples you want to use for Quantile Binning.

If you want to compare multiple metrics against eachother, you can specify multiple tuples in a list.

Specify them in `PIPELINE.INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS` and/or `PIPELINE.COMPARE_Q_UNCERTAINTY_ERROR_PAIRS`. See [below](#6-advanced-usage---config-options) for more details on these config options.

The pipeline supports evaluating the uncertainty from multiple targets (e.g. landmarks) at once. This works by formatting your data in lists: one list for each target. See the code in main.py for how this works. In our examples, we select DATASET.LANDMARKS to represent the indicies of the targets.

To change this in your example, observe the following in main.py

```python

    for model in all_models_to_compare:
        for landmark in landmarks:
            # Define Paths for this loop
            landmark_results_path_val = os.path.join(
                cfg.DATASET.ROOT, base_dir, model, dataset, uncertainty_pairs_val + "_l" + str(landmark)
            )
            landmark_results_path_test = os.path.join(
                cfg.DATASET.ROOT, base_dir, model, dataset, uncertainty_pairs_test + "_l" + str(landmark)
            )

            fitted_save_at = os.path.join(save_folder, "fitted_quantile_binning", model, dataset)
            os.makedirs(save_folder, exist_ok=True)
            .
            .
            .
            (rest of code)

```

When you loop through the target indicies (landmarks in the example above), you can specify the paths for each target.

If you only have one target (e.g. regression of a house price), follow the config file [one_target_example.yaml](/pykale/examples/landmark_uncertainty/configs/one_target_example.yaml) for how to do this. Essentially, you just use a list of length 1.


## 6. Advanced Usage - Config Options
 You can add your own config or change the config options in the .yaml files.
 To use your own data, you can simply change the paths in DATASET, as shown in the examples.

### Quick Tips:
- To display rather than save figures set `OUTPUT.SAVE_FIGURES`: `False` in the .yaml file.
- To compare results for different numbers of Quantile Bins, check out the documentation below for `PIPELINE.NUM_QUANTILE_BINS`
- If test error is not available, set `DATASET.TEST_ERROR_AVAILABLE`: False in the .yaml file.
- Check out the `BOXPLOT` options to turn on/off visualizations of all landmarks on the box plots and adjust error limits.

The configuration options are broken into a few sections:
- A) [Dataset](#a-dataset) - options related to the dataset.
- B) [Pipeline](#b-pipeline) - options related to the pipeline.
- C) [Plotting](#c-visualisation-im_kwargs-marker_kwargs-weight_kwargs) - options related to the plots.
- D) [Boxplot](#d-boxplot) - options related to the boxplot detai.
- E) [Output](#e-output) - options related to output paths.


### A) Dataset

The following are the configuration options related to the dataset.

#### `DATASET.SOURCE`

The URL source of the dataset. This option points to the location where the dataset is hosted. If you have local data, set this to None and put your data under DATASET.ROOT.

Default:
`_C.DATASET.SOURCE` = "https://github.com/pykale/data/raw/main/tabular/cardiac_landmark_uncertainty/Uncertainty_tuples.zip"

#### `DATASET.ROOT`

The root directory where the dataset will be downloaded and extracted to. If you have local data, set this to the directory where your data is stored.

Default:
`_C.DATASET.ROOT` = "../../../data/landmarks/"

#### `DATASET.BASE_DIR`

The base directory within the `DATASET.ROOT` directory where the dataset will be downloaded and extracted to. If you have local data, set this to one level down of ROOT, where the data is stored.

e.g. if you are using `4CH` data (`DATASET.DATA`="4CH") and the full path to the data is "../../../data/landmarks/Uncertainty_tuples/4CH" then:

1) Set `DATASET.BASE_DIR` = "Uncertainty_tuples"
2)  Set `_C.DATASET.ROOT` = "../../../data/landmarks/".


Default:
`_C.DATASET.BASE_DIR` = "Uncertainty_tuples"

#### `DATASET.FILE_FORMAT`

The format of the dataset file. This option is used to specify the format of the dataset file when the file is downloaded and extracted.

Default:
`_C.DATASET.FILE_FORMAT` = "zip"

 #### `DATASET.CONFIDENCE_INVERT`

A list of tuples specifying the uncertainty measures and whether or not to invert their confidence values. The measures are specified by name and the inversion is specified using a boolean value.

Default:
_C.DATASET.CONFIDENCE_INVERT` = [["S-MHA", True], ["E-MHA", True], ["E-CPV", False]]

#### `DATASET.DATA`

The type of dataset to use. This option specifies which subset of the dataset to use for the experiment. For the examples, you can use 4CH, SA or ISBI.

Default:
`_C.DATASET.DATA` = "4CH"

#### `DATASET.LANDMARKS`

A list of landmark indices to use in the experiment.

Default:
`_C.DATASET.LANDMARKS` = [0, 1, 2]


#### `DATASET.NUM_FOLDS`

The number of cross-validation folds to analyze. If no cross-validaiton, set to 1.

Default:
`_C.DATASET.NUM_FOLDS = 8`


#### `DATASET.GROUND_TRUTH_TEST_ERRORS_AVAILABLE`

A boolean indicating whether ground truth test errors are available in the dataset. If false, it will only fit the quantile binning model to the validation set, it won't attempt to evalute the performance on the test set since there is no ground truth test error.

Default:
`_C.DATASET.GROUND_TRUTH_TEST_ERRORS_AVAILABLE = True`


#### `DATASET.UE_PAIRS_VAL`

The name of the file containing the uncertainty pairs for validation. This option specifies the name of the file containing the uncertainty pairs for validation. This should be the preamble name of the .csv files before _lX.csv where X will be the landmark index.

Default:
`_C.DATASET.UE_PAIRS_VAL = "uncertainty_pairs_valid"`

(The program will infer uncertainty_pairs_valid_l0.csv, uncertainty_pairs_valid_l1.csv, uncertainty_pairs_valid_l2.csv, etc.)

#### `DATASET.UE_PAIRS_TEST`

The name of the file containing the uncertainty pairs for testing. This option specifies the name of the file containing the uncertainty pairs for testing. This should be the preamble name of the .csv files before _lX.csv where X will be the landmark index.

Default:
`_C.DATASET.UE_PAIRS_TEST` = "uncertainty_pairs_test"

(The program will infer uncertainty_pairs_test_l0.csv, uncertainty_pairs_test_l1.csv, uncertainty_pairs_test_l2.csv, etc.)

<br/><br/>


### B) Pipeline

The following are the configuration options related to the pipeline.

#### `PIPELINE.NUM_QUANTILE_BINS`

A list of integers specifying the number of quantile bins to use for the uncertainty histogram. Use multiple if you want to compare the performance of different numbers of quantile bins, or just set it to a single integer if you want to use a single number of quantile bins e.g. [5].

Default:
`_C.PIPELINE.NUM_QUANTILE_BINS` = [5, 10, 25]

#### `PIPELINE.COMPARE_INDIVIDUAL_Q`

A boolean indicating whether to compare uncertainty measures and models over each single value of Q (Q= NUM_QUANTILE_BINS).

Default:
`_C.PIPELINE.COMPARE_INDIVIDUAL_Q` = True

#### `PIPELINE.INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS`

A list of lists specifying the uncertainty error pairs to compare for each value of Q. Each sublist should contain three elements: the name of the uncertainty measure, the key for the error in the CSV file, and the key for the uncertainty in the CSV file.

Default:
`_C.PIPELINE.INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS` = [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"], ["E-CPV", "E-CPV Error", "E-CPV Uncertainty"]]

#### `PIPELINE.INDIVIDUAL_Q_MODELS`

A list of model names found in the path to compare for each value of Q.

Default:
_C.PIPELINE.INDIVIDUAL_Q_MODELS = ["U-NET", "PHD-NET"]

#### `PIPELINE.COMPARE_Q_VALUES`

A boolean indicating whether to compare a single uncertainty measure on a single model through various values of Q bins.

Default:
`_C.PIPELINE.COMPARE_Q_VALUES` = True

#### `PIPELINE.COMPARE_Q_MODELS`

A list of model names to compare over values of Q.

Default:
`_C.PIPELINE.COMPARE_Q_MODELS` = ["PHD-NET"]

#### `PIPELINE.COMPARE_Q_UNCERTAINTY_ERROR_PAIRS`

A list of lists specifying the uncertainty error pairs to compare over values of Q. Each sublist should contain three elements: the name of the uncertainty measure, the key for the error in the CSV file, and the key for the uncertainty in the CSV file.

Default:
`_C.PIPELINE.COMPARE_Q_UNCERTAINTY_ERROR_PAIRS` = [["E-MHA", "E-MHA Error", "E-MHA Uncertainty"]]

#### `PIPELINE.COMBINE_MIDDLE_BINS`

A boolean indicating whether to combine the middle quantile bins into a single bin.

Default:
`_C.PIPELINE.COMBINE_MIDDLE_BINS` = False

#### `PIPELINE.PIXEL_TO_MM_SCALE`

A float specifying the scale factor to convert pixel units to millimeter units.

Default:
`_C.PIPELINE.PIXEL_TO_MM_SCALE` = 1.0

#### `PIPELINE.IND_LANDMARKS_TO_SHOW`

A list of landmark indices to show individually. A value of -1 means show all landmarks individually, and an empty list means show none.

Default:
`_C.PIPELINE.IND_LANDMARKS_TO_SHOW` = [-1]

#### `PIPELINE.SHOW_IND_LANDMARKS`

A boolean indicating whether to show results from individual landmarks.

Default:
_C.`PIPELINE.SHOW_IND_LANDMARKS` = True

<br/><br/>

### C) Visualisation: IM_KWARGS, MARKER_KWARGS, WEIGHT_KWARGS
The following are the configuration options related to visualization and plotting.

#### `IM_KWARGS.CMAP`

The color map to use for the image.

Default:
`_C.IM_KWARGS.cmap` = "gray"

#### `MARKER_KWARGS.MARKER`

The marker style to use for the landmark points.

Default:
`_C.MARKER_KWARGS.marker` = "o"

#### `MARKER_KWARGS.MARKERFACECOLOR`

The face color to use for the landmark points.

Default:
_C.MARKER_KWARGS.markerfacecolor = (1, 1, 1, 0.1)

#### `MARKER_KWARGS.MARKEREDGEWIDTH`

The edge width to use for the landmark points.

Default:
`_C.MARKER_KWARGS.markeredgewidth` = 1.5

#### `MARKER_KWARGS.MARKEREDGECOLOR`

The edge color to use for the landmark points.

Default:
`_C.MARKER_KWARGS.markeredgecolor` = "r"

#### `WEIGHT_KWARGS.MARKERSIZE`

The size to use for the weights of the landmark points.

Default:
`_C.WEIGHT_KWARGS.markersize` = 6

#### `WEIGHT_KWARGS.ALPHA`

The transparency to use for the plots.

Default:
`_C.WEIGHT_KWARGS.alpha` = 0.7

<br/><br/>


### D) BOXPLOT
The following are the configuration options related to the box plot.


#### `BOXPLOT.SAMPLES_AS_DOTS`

A boolean indicating whether to show the samples as dots on the box plot (can be expensive if many landmarks.).

Default:
`_C.BOXPLOT.SAMPLES_AS_DOTS` = True

#### `BOXPLOT.ERROR_LIM`

The error limit to use for the box plot.

Default:
`_C.BOXPLOT.ERROR_LIM` = 64

#### `BOXPLOT.SHOW_SAMPLE_INFO_MODE`

The mode for showing sample information on the box plot. The available modes are "None", "All", and "Average".

Default:
_C.BOXPLOT.SHOW_SAMPLE_INFO_MODE = "Average"

<br/><br/>


### E) OUTPUT

The following are miscellaneous configuration options.

#### `OUTPUT.OUT_DIR`

The folder where the experiment results will be saved.

Default:
`_C.OUTPUT.OUT_DIR` = "./outputs/""

#### `OUTPUT.SAVE_PREPEND`

The string to prepend to the output file names.

Default:
`_C.OUTPUT.SAVE_PREPEND` = "8std_27_07_22"

#### `OUTPUT.SAVE_FIGURES`

A boolean indicating whether to save the figures generated during the experiment.
If False, the figures will be shown instead.

Default:
`_C.OUTPUT.SAVE_FIGURES` = True


## 6. References
[1] L. A. Schobs, A. J. Swift and H. Lu, "Uncertainty Estimation for Heatmap-Based Landmark Localization," in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034, April 2023, doi: 10.1109/TMI.2022.3222730.

[2] J. Hurdman, R. Condliffe, C.A. Elliot, C. Davies, C. Hill, J.M. Wild, D. Capener, P. Sephton, N. Hamilton, I.J. Armstrong, C. Billings, A. Lawrie, I. Sabroe, M. Akil, L. O‚ÄôToole, D.G. Kiely
European Respiratory Journal 2012 39: 945-955; DOI: 10.1183/09031936.00078411

[3] Wang, Ching-Wei & Huang, Cheng-Ta & Lee, Jia-Hong & Li, Chung-Hsing & Chang, Sheng-Wei & Siao, Ming-Jhih & Lai, Tat-Ming & Ibragimov, Bulat & Vrtovec, Toma≈æ & Ronneberger, Olaf & Fischer, Philipp & Cootes, Tim & Lindner, Claudia. (2016). A benchmark for comparison of dental radiography analysis algorithms. Medical Image Analysis. 31. 10.1016/j.media.2016.02.004.

[4] Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention ‚Äì MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science(), vol 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_28

[5] L. Schobs, S. Zhou, M. Cogliano, A. J. Swift and H. Lu, "Confidence-Quantifying Landmark Localisation For Cardiac MRI," 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), Nice, France, 2021, pp. 985-988, doi: 10.1109/ISBI48211.2021.9433895.
</file>

<file path="examples/multiomics_mogonet/configs/MOGONET_BRCA_quick_test.yaml">
DATASET:
  NAME: "TCGA_BRCA"
  URL: "https://github.com/pykale/data/raw/main/multiomics/TCGA_BRCA.zip"
  NUM_MODALITIES: 3
  NUM_CLASSES: 5

SOLVER:
  MAX_EPOCHS_PRETRAIN: 5 # For quick testing
  MAX_EPOCHS: 10  # For quick testing

MODEL:
  EDGE_PER_NODE: 10
  GCN_HIDDEN_DIM: [400, 400, 200]
</file>

<file path="examples/multiomics_mogonet/configs/MOGONET_ROSMAP_quick_test.yaml">
DATASET:
  NAME: "ROSMAP"
  URL: "https://github.com/pykale/data/raw/main/multiomics/ROSMAP.zip"
  NUM_MODALITIES: 3
  NUM_CLASSES: 2

SOLVER:
  MAX_EPOCHS_PRETRAIN: 5 # For quick testing
  MAX_EPOCHS: 10  # For quick testing

MODEL:
  EDGE_PER_NODE: 2
  GCN_HIDDEN_DIM: [200, 200, 100]
</file>

<file path="examples/multiomics_mogonet/config.py">
"""
Default configurations according to the MOGONET method described in 'MOGONET integrates multi-omics data using
graph convolutional networks allowing patient classification and biomarker identification'
- Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021).

https://github.com/txWang/MOGONET/blob/main/main_mogonet.py
"""
from yacs.config import CfgNode

# ---------------------------------------------------------
# Config definition
# ---------------------------------------------------------

_C = CfgNode()

# ---------------------------------------------------------
# Dataset
# ---------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "dataset/"
_C.DATASET.NAME = "TCGA_BRCA"
_C.DATASET.URL = "https://github.com/pykale/data/raw/main/multiomics/TCGA_BRCA.zip"
_C.DATASET.RANDOM_SPLIT = False
_C.DATASET.NUM_MODALITIES = 3  # Number of omics modalities in the dataset
_C.DATASET.NUM_CLASSES = 5

# ---------------------------------------------------------
# Solver
# ---------------------------------------------------------
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2023
_C.SOLVER.MAX_EPOCHS_PRETRAIN = 500
_C.SOLVER.MAX_EPOCHS = 2500

# -----------------------------------------------------------------------------
# Model (MOGONET) configs
# -----------------------------------------------------------------------------
_C.MODEL = CfgNode()
_C.MODEL.EDGE_PER_NODE = 10  # Predefined number of edges per nodes in computing adjacency matrix
_C.MODEL.EQUAL_WEIGHT = False
_C.MODEL.GCN_LR_PRETRAIN = 1e-3
_C.MODEL.GCN_LR = 5e-4
_C.MODEL.GCN_DROPOUT_RATE = 0.5
_C.MODEL.GCN_HIDDEN_DIM = [400, 400, 200]

# The View Correlation Discovery Network (VCDN) to learn the higher-level intra-view and cross-view correlations
# in the label space. See the MOGONET paper for more information.
_C.MODEL.VCDN_LR = 1e-3

# ---------------------------------------------------------
# Misc options
# ---------------------------------------------------------
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/multiomics_mogonet/main.py">
"""
Multiomics Integration via Graph Convolutional Networks for Cancer Classification Tasks.

Reference:
Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021). MOGONET integrates multi-omics data
using graph convolutional networks allowing patient classification and biomarker identification. Nature communications.
https://www.nature.com/articles/s41467-021-23774-w
"""

import argparse
import warnings

import pytorch_lightning as pl
import torch
from config import get_cfg_defaults
from model import MogonetModel

import kale.utils.seed as seed
from kale.loaddata.multiomics_datasets import SparseMultiomicsDataset
from kale.prepdata.tabular_transform import ToOneHotEncoding, ToTensor

warnings.filterwarnings(action="ignore")


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="MOGONET Training for Multiomics Data Integration")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    args = parser.parse_args()

    return args


def main():
    args = arg_parse()

    # ---- setup device ----
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("\n==> Using device " + device)

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    seed.set_seed(cfg.SOLVER.SEED)

    # ---- setup dataset ----
    print("\n==> Preparing dataset...")
    file_names = []
    for modality in range(1, cfg.DATASET.NUM_MODALITIES + 1):
        file_names.append(f"{modality}_tr.csv")
        file_names.append(f"{modality}_lbl_tr.csv")
        file_names.append(f"{modality}_te.csv")
        file_names.append(f"{modality}_lbl_te.csv")

    multiomics_data = SparseMultiomicsDataset(
        root=cfg.DATASET.ROOT,
        raw_file_names=file_names,
        num_modalities=cfg.DATASET.NUM_MODALITIES,
        num_classes=cfg.DATASET.NUM_CLASSES,
        edge_per_node=cfg.MODEL.EDGE_PER_NODE,
        url=cfg.DATASET.URL,
        random_split=cfg.DATASET.RANDOM_SPLIT,
        equal_weight=cfg.MODEL.EQUAL_WEIGHT,
        pre_transform=ToTensor(dtype=torch.float),
        target_pre_transform=ToOneHotEncoding(dtype=torch.float),
    )

    print(multiomics_data)

    # ---- setup model ----
    print("\n==> Building model...")
    mogonet_model = MogonetModel(cfg, dataset=multiomics_data)
    print(mogonet_model)

    # ---- setup pretrain model and trainer ----
    print("\n==> Pretrain GCNs...")
    model = mogonet_model.get_model(pretrain=True)
    trainer_pretrain = pl.Trainer(
        max_epochs=cfg.SOLVER.MAX_EPOCHS_PRETRAIN,
        default_root_dir=cfg.OUTPUT.OUT_DIR,
        accelerator="auto",
        devices="auto",
        enable_model_summary=False,
    )
    trainer_pretrain.fit(model)

    # ---- set train model and trainer ----
    print("\n==> Training model...")
    model = mogonet_model.get_model(pretrain=False)
    trainer = pl.Trainer(
        max_epochs=cfg.SOLVER.MAX_EPOCHS,
        default_root_dir=cfg.OUTPUT.OUT_DIR,
        accelerator="auto",
        devices="auto",
        enable_model_summary=False,
        log_every_n_steps=1,
    )
    trainer.fit(model)

    # ---- testing model ----
    print("\n==> Testing model...")
    _ = trainer.test(model)


if __name__ == "__main__":
    main()
</file>

<file path="examples/multiomics_mogonet/model.py">
from typing import List, Optional

from torch.nn import CrossEntropyLoss
from yacs.config import CfgNode

from kale.embed.mogonet import MogonetGCN
from kale.loaddata.multiomics_datasets import SparseMultiomicsDataset
from kale.pipeline.multiomics_trainer import MultiomicsTrainer
from kale.predict.decode import LinearClassifier, VCDN


class MogonetModel:
    r"""Setup the MOGONET model via the config file.

    Args:
        cfg (CfgNode): A YACS config object.
        dataset (SparseMultiomicsDataset): The input dataset created in form of :class:`~torch_geometric.data.Dataset`.
    """

    def __init__(self, cfg: CfgNode, dataset: SparseMultiomicsDataset) -> None:
        self.cfg = cfg
        self.dataset = dataset
        self.unimodal_encoder: List[MogonetGCN] = []
        self.unimodal_decoder: List[LinearClassifier] = []
        self.multimodal_decoder: Optional[VCDN] = None
        self.loss_function = CrossEntropyLoss(reduction="none")
        self._create_model()

    def _create_model(self) -> None:
        """Create the MOGONET model via the config file."""
        num_modalities = self.cfg.DATASET.NUM_MODALITIES
        num_classes = self.cfg.DATASET.NUM_CLASSES
        gcn_dropout_rate = self.cfg.MODEL.GCN_DROPOUT_RATE
        gcn_hidden_dim = self.cfg.MODEL.GCN_HIDDEN_DIM
        vcdn_hidden_dim = pow(num_classes, num_modalities)

        for modality in range(num_modalities):
            self.unimodal_encoder.append(
                MogonetGCN(
                    in_channels=self.dataset.get(modality).num_features,
                    hidden_channels=gcn_hidden_dim,
                    dropout=gcn_dropout_rate,
                )
            )

            self.unimodal_decoder.append(LinearClassifier(in_dim=gcn_hidden_dim[-1], out_dim=num_classes))

        if num_modalities >= 2:
            self.multimodal_decoder = VCDN(
                num_modalities=num_modalities, num_classes=num_classes, hidden_dim=vcdn_hidden_dim
            )

    def get_model(self, pretrain: bool = False) -> MultiomicsTrainer:
        """Return the prepared MOGONET model based on user preference.

        Args:
            pretrain (bool, optional): Whether to return the pretrain model. (default: ``False``)

        Returns:
            MultiomicsTrainer: The prepared MOGONET model.
        """
        num_modalities = self.cfg.DATASET.NUM_MODALITIES
        num_classes = self.cfg.DATASET.NUM_CLASSES
        gcn_lr_pretrain = self.cfg.MODEL.GCN_LR_PRETRAIN
        gcn_lr = self.cfg.MODEL.GCN_LR
        vcdn_lr = self.cfg.MODEL.VCDN_LR

        if pretrain:
            multimodal_model = None
            train_multimodal_decoder = False
            gcn_lr = gcn_lr_pretrain
        else:
            multimodal_model = self.multimodal_decoder
            train_multimodal_decoder = True
            gcn_lr = gcn_lr

        model = MultiomicsTrainer(
            dataset=self.dataset,
            num_modalities=num_modalities,
            num_classes=num_classes,
            unimodal_encoder=self.unimodal_encoder,
            unimodal_decoder=self.unimodal_decoder,
            loss_fn=self.loss_function,
            multimodal_decoder=multimodal_model,
            train_multimodal_decoder=train_multimodal_decoder,
            gcn_lr=gcn_lr,
            vcdn_lr=vcdn_lr,
        )

        return model

    def __str__(self) -> str:
        r"""Returns a string representation of the model object.

        Returns:
            str: The string representation of the model object.
        """
        return self.get_model().__str__()
</file>

<file path="examples/multiomics_mogonet/README.md">
# Multiomics Data Integration using Graph Convolutional Networks (MOGONET)

## 1. Description

Multimodal learning for multiomics data analysis is a promising research area in biomedical studies. Integrating multiple modalities (e.g., genomics, epigenomics, transcriptomics, proteomics, metabolomics, etc.) captures their complementary information and provides a deeper understanding of most complex human diseases. This example is constructed by refactoring the code of [MOGONET](https://doi.org/10.1038/s41467-021-23774-w) [1], a multiomics integrative method for cancer classification tasks, using the PyTorch Geometric and PyTorch Lightning frameworks.


## 2. MOGONET

**M**ulti-**O**mics **G**raph c**O**nvolutional **NET**works ([MOGONET](https://doi.org/10.1038/s41467-021-23774-w)) is
a multiomics fusion framework for cancer classification and biomarker identification that utilizes supervised graph
convolutional networks for omics datasets. The overall framework of MOGONET is illustrated below.

![MOGONET Architecture](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-021-23774-w/MediaObjects/41467_2021_23774_Fig1_HTML.png)

The original implementation of MOGONET is available [here](https://github.com/txWang/MOGONET).

## 3. Dataset

We have tested the MOGONET method using two preprocessed multiomics benchmarks, ROSMAP and BRCA, which have been provided
by the authors of the MOGONET paper in their repository. A brief description of these datasets is shown in the following
tables.

**Table 1**: Characteristics of the preprocessed ROSMAP multiomics dataset.

|      Omics       | #Training samples | #Test samples | #Features  |
|:----------------:|:-----------------:|:-------------:|:----------:|
| mRNA expression  |        245        |      106      |    200     |
| DNA methylation  |        245        |      106      |    200     |
| miRNA expression |        245        |      106      |    200     |



**Table 2**: Characteristics of the preprocessed BRCA multiomics dataset.

|      Omics       | #Training samples | #Test samples | #Features |
|:----------------:|:-----------------:|:-------------:|:---------:|
| mRNA expression  |        612        |      263      |   1000    |
| DNA methylation  |        612        |      263      |   1000    |
| miRNA expression |        612        |      263      |    503    |

Note: These datasets have been processed following the **Preprocessing** section of the original paper.

## 4. Usage

* Datasets: [BRCA, ROSMAP](https://github.com/pykale/data/tree/main/multiomics)
* Algorithm: MOGONET

Run the MOGONET model for the BRCA and ROSMAP datasets using `yaml` configuration files provided in the `configs` folder
for quick testing. To use them, run:

`python main.py --cfg configs/MOGONET_BRCA_quick_test.yaml`

`python main.py --cfg configs/MOGONET_ROSMAP_quick_test.yaml`


## Reference

[1] Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021). [MOGONET integrates multi-omics data
using graph convolutional networks allowing patient classification and biomarker identification](https://doi.org/10.1038/s41467-021-23774-w). Nature Communications.
</file>

<file path="examples/multisite_neuroimg_adapt/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/multisite_neuroimg_adapt/configs/tutorial.yaml">
MODEL:
  KERNEL: "linear"
  ALPHA: 1.0
  LAMBDA_: 1.0
DATASET:
  ATLAS: "rois_cc200"
  SITE_IDS: ['NYU', "UM_1", "UCLA_1", "USM"]
</file>

<file path="examples/multisite_neuroimg_adapt/config.py">
"""
Default configurations for classification on resting-state fMRI of ABIDE
"""

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "../data"
_C.DATASET.PIPELINE = "cpac"  # options: {‚Äòcpac‚Äô, ‚Äòcss‚Äô, ‚Äòdparsf‚Äô, ‚Äòniak‚Äô}
_C.DATASET.ATLAS = "rois_cc200"
# options: {rois_aal, rois_cc200, rois_cc400, rois_dosenbach160, rois_ez, rois_ho, rois_tt}
_C.DATASET.SITE_IDS = None  # list of site ids to use, if None, use all sites
_C.DATASET.TARGET = "NYU"  # target site ids, e.g. "UM_1", "UCLA_1", "USM"
# ---------------------------------------------------------
# Solver
# ---------------------------------------------------------
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2023
# ---------------------------------------------------------------------------- #
# Machine learning pipeline
# ---------------------------------------------------------------------------- #
_C.MODEL = CfgNode()
_C.MODEL.KERNEL = "rbf"
_C.MODEL.ALPHA = 0.01
_C.MODEL.LAMBDA_ = 1.0
# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"  # output_dir


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/multisite_neuroimg_adapt/main.py">
"""
Autism Detection: Domain Adaptation for Multi-Site Neuroimaging Data Analysis

Reference:
[1] Craddock C., Benhajali Y., Chu C., Chouinard F., Evans A., Jakab A., Khundrakpam BS., Lewis JD., Li Q., Milham M., Yan C. and Bellec P. (2013). The Neuro Bureau Preprocessing Initiative: Open Sharing of Preprocessed Neuroimaging Data and Derivatives. Frontiers in Neuroinformatics, 7. https://doi.org/10.3389/conf.fninf.2013.09.00041

[2] Abraham A., Pedregosa F., Eickenberg M., Gervais P., Mueller A., Kossaifi J., Gramfort A., Thirion B. and Varoquaux G. (2014). Machine Learning for Neuroimaging with scikit-learn. Frontiers in Neuroinformatics, 8. https://doi.org/10.3389/fninf.2014.00014

[3] Zhou S., Li W., Cox C. and Lu H. (2020). Side Information Dependence as a Regularizer for Analyzing Human Brain Conditions across Cognitive Experiments. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04), 6957-6964. https://doi.org/10.1609/aaai.v34i04.6179

[4] Zhou S. (2022). Interpretable Domain-Aware Learning for Neuroimage Classification (Doctoral Dissertation, University of Sheffield). https://etheses.whiterose.ac.uk/31044/1/PhD_thesis_ShuoZhou_170272834.pdf
"""
import argparse
import os

import numpy as np
import pandas as pd
from config import get_cfg_defaults
from nilearn.connectome import ConnectivityMeasure
from nilearn.datasets import fetch_abide_pcp
from sklearn.linear_model import RidgeClassifier

import kale.utils.seed as seed
from kale.evaluate import cross_validation
from kale.pipeline.multi_domain_adapter import CoIRLS


def arg_parse():
    parser = argparse.ArgumentParser(
        description="Autism Detection: Domain Adaptation for Multi-Site Neuroimaging Data Analysis"
    )
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    args = parser.parse_args()
    return args


def main():
    args = arg_parse()

    # ---- Set up configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    seed.set_seed(cfg.SOLVER.SEED)

    # ---- Fetch ABIDE fMRI timeseries ----
    fetch_abide_pcp(
        data_dir=cfg.DATASET.ROOT,
        pipeline=cfg.DATASET.PIPELINE,
        band_pass_filtering=True,
        global_signal_regression=False,
        derivatives=cfg.DATASET.ATLAS,
        quality_checked=False,
        SITE_ID=cfg.DATASET.SITE_IDS,
        verbose=1,
    )

    # ---- Read Phenotypic data ----
    pheno_file = os.path.join(cfg.DATASET.ROOT, "ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv")
    pheno_info = pd.read_csv(pheno_file, index_col=0)

    # ---- Read timeseries from files ----
    data_dir = os.path.join(cfg.DATASET.ROOT, "ABIDE_pcp/%s/filt_noglobal" % cfg.DATASET.PIPELINE)
    use_idx = []
    time_series = []
    for i in pheno_info.index:
        data_file_name = "%s_%s.1D" % (pheno_info.loc[i, "FILE_ID"], cfg.DATASET.ATLAS)
        data_path = os.path.join(data_dir, data_file_name)
        if os.path.exists(data_path):
            time_series.append(np.loadtxt(data_path, skiprows=0))
            use_idx.append(i)

    # ---- Use "DX_GROUP" (autism vs control) as labels, and "SITE_ID" as covariates ----
    pheno = pheno_info.loc[use_idx, ["SITE_ID", "DX_GROUP"]].reset_index(drop=True)

    # ---- Extracting Brain Networks Features ----
    correlation_measure = ConnectivityMeasure(kind="correlation", vectorize=True)
    brain_networks = correlation_measure.fit_transform(time_series)

    # ---- Machine Learning for Multi-site Data ----
    print("Baseline")
    estimator = RidgeClassifier()
    results = cross_validation.leave_one_group_out(
        brain_networks, pheno["DX_GROUP"].values, pheno["SITE_ID"].values, estimator
    )
    print(pd.DataFrame.from_dict(results))

    print("Domain Adaptation")
    estimator = CoIRLS(kernel=cfg.MODEL.KERNEL, lambda_=cfg.MODEL.LAMBDA_, alpha=cfg.MODEL.ALPHA)
    results = cross_validation.leave_one_group_out(
        brain_networks, pheno["DX_GROUP"].values, pheno["SITE_ID"].values, estimator, use_domain_adaptation=True
    )
    print(pd.DataFrame.from_dict(results))


if __name__ == "__main__":
    main()
</file>

<file path="examples/multisite_neuroimg_adapt/README.md">
# Autism Detection: Domain Adaptation for Multi-Site Neuroimaging Data Analysis

### 1. Description

This example demonstrates multi-source domain adaptation method with application in neuroimaging data analysis for
autism detection.

### 2. Materials and Methods

- Data: Four largest subsets of ABIDE I

| Site   | Number of Samples |
|--------|-------------------|
| NYU    | 175               |
| UM_1   | 106               |
| UCLA_1 | 72                |
| USM    | 71                |
- Atlas: CC200
- Pre-processing pipeline: cpac
- Classification problem: Autism vs Control
- Pipeline:
  1. Constructing brain networks from resting-state fMRI data
  2. Classification with Ridge Classifier or Covariate Independence Regularized Least Squares (CoIRLS) classifier


### 3. Related `kale` API

`kale.interpret.visualize`: Visualize the results of a model.

`kale.pipeline.multi_domain_adapter.CoIRLS`: Covariate Independence Regularized Least Squares (CoIRLS) classifier.

`kale.utils.download.download_file_by_url`: Download a file from a URL.

### References

[1] Craddock C., Benhajali Y., Chu C., Chouinard F., Evans A., Jakab A., Khundrakpam BS., Lewis JD., Li Q., Milham M., Yan C. and Bellec P. (2013). [The Neuro Bureau Preprocessing Initiative: Open Sharing of Preprocessed Neuroimaging Data and Derivatives](https://doi.org/10.3389/conf.fninf.2013.09.00041). Frontiers in Neuroinformatics, 7.

[2] Abraham A., Pedregosa F., Eickenberg M., Gervais P., Mueller A., Kossaifi J., Gramfort A., Thirion B. and Varoquaux G. (2014). [Machine Learning for Neuroimaging with scikit-learn](https://doi.org/10.3389/fninf.2014.00014). Frontiers in Neuroinformatics, 8.

[3] Zhou S., Li W., Cox C. and Lu H. (2020). [Side Information Dependence as a Regularizer for Analyzing Human Brain Conditions across Cognitive Experiments](https://doi.org/10.1609/aaai.v34i04.6179). Proceedings of the AAAI Conference on Artificial Intelligence, 34(04), 6957-6964.

[4] Zhou S. (2022). [Interpretable Domain-Aware Learning for Neuroimage Classification](https://etheses.whiterose.ac.uk/31044/1/PhD_thesis_ShuoZhou_170272834.pdf) (Doctoral Dissertation, University of Sheffield).
</file>

<file path="examples/multisite_neuroimg_adapt/tutorial.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# PyKale Tutorial: Domain Adaptation for Autism Detection with Multi-site Brain Imaging Data\n",
        "| [Open in Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/multisite_neuroimg_adapt/tutorial.ipynb) (click `Runtime`\u2006\u2192\u2006`Run all (Ctrl+F9)` |  [Launch Binder](https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Fmultisite_neuroimg_adapt%2Ftutorial.ipynb) (click `Run`\u2006\u2192\u2006`Run All Cells`) |"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "- Pre-processing:\n",
        "    - [Data loading](#Data-Preparation)\n",
        "    - [Construct brain networks](#Extracting-Brain-Networks-Features)\n",
        "- Machine learning pipeline:\n",
        "    - [Baseline: Ridge classifier](#Baseline-Model)\n",
        "    - [Domain adaptation](#Domain-Adaptation)\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "[1] Craddock C., Benhajali Y., Chu C., Chouinard F., Evans A., Jakab A., Khundrakpam BS., Lewis JD., Li Q., Milham M., Yan C. and Bellec P. (2013). [The Neuro Bureau Preprocessing Initiative: Open Sharing of Preprocessed Neuroimaging Data and Derivatives](https://doi.org/10.3389/conf.fninf.2013.09.00041). Frontiers in Neuroinformatics, 7.\n",
        "\n",
        "[2] Abraham A., Pedregosa F., Eickenberg M., Gervais P., Mueller A., Kossaifi J., Gramfort A., Thirion B. and Varoquaux G. (2014). [Machine Learning for Neuroimaging with scikit-learn](https://doi.org/10.3389/fninf.2014.00014). Frontiers in Neuroinformatics, 8.\n",
        "\n",
        "[3] Zhou S., Li W., Cox C. and Lu H. (2020). [Side Information Dependence as a Regularizer for Analyzing Human Brain Conditions across Cognitive Experiments](https://doi.org/10.1609/aaai.v34i04.6179). Proceedings of the AAAI Conference on Artificial Intelligence, 34(04), 6957-6964.\n",
        "\n",
        "[4] Zhou S. (2022). [Interpretable Domain-Aware Learning for Neuroimage Classification](https://etheses.whiterose.ac.uk/31044/1/PhD_thesis_ShuoZhou_170272834.pdf) (Doctoral Dissertation, University of Sheffield)."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes imgaug && pip uninstall --yes albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "    !git clone https://github.com/pykale/pykale.git\n",
        "    %cd pykale\n",
        "    !pip install .[image,example]\n",
        "    %cd examples/multisite_neuroimg_adapt\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not running on CoLab\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "This imports required modules."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from config import get_cfg_defaults\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "from nilearn.datasets import fetch_abide_pcp\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "import kale.utils.seed as seed\n",
        "from kale.evaluate import cross_validation\n",
        "from kale.pipeline.multi_domain_adapter import CoIRLS"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Path to `.yaml` config file\n",
        "cfg_path = \"configs/tutorial.yaml\" \n",
        "cfg = get_cfg_defaults()\n",
        "cfg.merge_from_file(cfg_path)\n",
        "cfg.freeze()\n",
        "seed.set_seed(cfg.SOLVER.SEED)\n",
        "print(cfg)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "### Fetch ABIDE fMRI timeseries"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "root_dir = cfg.DATASET.ROOT\n",
        "pipeline = cfg.DATASET.PIPELINE  # fmri pre-processing pipeline\n",
        "atlas = cfg.DATASET.ATLAS\n",
        "site_ids = cfg.DATASET.SITE_IDS\n",
        "abide = fetch_abide_pcp(data_dir=root_dir, pipeline=pipeline,\n",
        "                        band_pass_filtering=True, global_signal_regression=False,\n",
        "                        derivatives=atlas, quality_checked=False,\n",
        "                        SITE_ID=site_ids,\n",
        "                        verbose=0)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Read Phenotypic data"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "pheno_file = os.path.join(cfg.DATASET.ROOT, \"ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv\")\n",
        "pheno_info = pd.read_csv(pheno_file, index_col=0)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "View Phenotypic data"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "pheno_info.head()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Read timeseries from files"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "data_dir = os.path.join(root_dir, \"ABIDE_pcp/%s/filt_noglobal\" % pipeline)\n",
        "use_idx = []\n",
        "time_series = []\n",
        "for i in pheno_info.index:\n",
        "    data_file_name = \"%s_%s.1D\" % (pheno_info.loc[i, \"FILE_ID\"], atlas)\n",
        "    data_path = os.path.join(data_dir, data_file_name)\n",
        "    if os.path.exists(data_path):\n",
        "        time_series.append(np.loadtxt(data_path, skiprows=0))\n",
        "        use_idx.append(i)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Use \"DX_GROUP\" (autism vs control) as labels, and \"SITE_ID\" as covariates"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "pheno = pheno_info.loc[use_idx, [\"SITE_ID\", \"DX_GROUP\"]].reset_index(drop=True)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Extracting Brain Networks Features"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "correlation_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
        "brain_networks = correlation_measure.fit_transform(time_series)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Machine Learning for Multi-site Data\n",
        "\n",
        "### Cross validation Pipeline"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Baseline"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "estimator = RidgeClassifier()\n",
        "results = cross_validation.leave_one_group_out(\n",
        "    brain_networks, pheno[\"DX_GROUP\"].values, pheno[\"SITE_ID\"].values, estimator\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "pd.DataFrame.from_dict(results)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Domain Adaptation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "estimator = CoIRLS(kernel=cfg.MODEL.KERNEL, lambda_=cfg.MODEL.LAMBDA_, alpha=cfg.MODEL.ALPHA)\n",
        "results = cross_validation.leave_one_group_out(\n",
        "  brain_networks, pheno[\"DX_GROUP\"].values, pheno[\"SITE_ID\"].values, estimator, use_domain_adaptation=True\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "pd.DataFrame.from_dict(results)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
</file>

<file path="examples/office_multisource_adapt/configs/digits-M3SDA.yaml">
DAN:
  METHOD: "M3SDA"
DATASET:
  NAME: "digits"
  NUM_CLASSES: 10
  ROOT: "../data/digits"
  TARGET: "MNIST_RGB"
  NUM_REPEAT: 1
SOLVER:
  TRAIN_BATCH_SIZE: 100
  TEST_BATCH_SIZE: 100
OUTPUT:
  OUT_DIR: "outputs/digits/M3SDASAN/Tgt_MNIST_RGB"
</file>

<file path="examples/office_multisource_adapt/configs/Office2A-M3SDA.yaml">
DAN:
  METHOD: "M3SDA"
DATASET:
  NAME: "office_caltech"
  NUM_CLASSES: 10
  ROOT: "../data/office"
  TARGET: "amazon"
  NUM_REPEAT: 5
SOLVER:
  TRAIN_BATCH_SIZE: 100
  TEST_BATCH_SIZE: 100
OUTPUT:
  OUT_DIR: "outputs/office_caltech/M3SDA/Tgt_amazon"
</file>

<file path="examples/office_multisource_adapt/configs/Office2A-MFSAN.yaml">
DAN:
  METHOD: "MFSAN"
DATASET:
  NAME: "office_caltech"
  NUM_CLASSES: 10
  ROOT: "../data/office"
  TARGET: "amazon"
  NUM_REPEAT: 5
SOLVER:
  TRAIN_BATCH_SIZE: 100
  TEST_BATCH_SIZE: 100
OUTPUT:
  OUT_DIR: "outputs/office_caltech/MFSAN/Tgt_amazon"
</file>

<file path="examples/office_multisource_adapt/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/office_multisource_adapt/configs/tutorial.yaml">
DAN:
  METHOD: "M3SDA"
DATASET:
  NAME: "office"
  NUM_CLASSES: 10
  ROOT: "../data/Office"
  TARGET: "amazon"
  SOURCE: ["dslr", "webcam"]
  NUM_REPEAT: 1
SOLVER:
  TRAIN_BATCH_SIZE: 100
  TEST_BATCH_SIZE: 100
OUTPUT:
  OUT_DIR: "outputs/office_caltech/M3SDA/Tgt_amazon"
</file>

<file path="examples/office_multisource_adapt/config.py">
"""
Default configurations for multi-source domain adapation
"""
import os

from yacs.config import CfgNode

# -----------------------------------------------------------------------------
# Config definition
# -----------------------------------------------------------------------------

_C = CfgNode()

# -----------------------------------------------------------------------------
# Dataset
# -----------------------------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "../data"
_C.DATASET.NAME = "digits"  # choices=['office', 'digits', 'office_caltech', 'office31']
_C.DATASET.TARGET = "MNIST"
# -----------------------------------------------------------------------------
_C.DATASET.SOURCE = None
# a list of source domain names (e.g. ["SVHN", "USPS_RGB"]) or None. If None, all domains (excluding the target)
# will be used as sources
# -----------------------------------------------------------------------------
_C.DATASET.NUM_CLASSES = 10
_C.DATASET.NUM_REPEAT = 10  # 10
_C.DATASET.NUM_CHANNELS = 3
_C.DATASET.DIMENSION = 784
_C.DATASET.WEIGHT_TYPE = "natural"
_C.DATASET.SIZE_TYPE = "source"

# ---------------------------------------------------------------------------- #
# Solver
# ---------------------------------------------------------------------------- #
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 2021
_C.SOLVER.BASE_LR = 0.001  # Initial learning rate
_C.SOLVER.MOMENTUM = 0.9
_C.SOLVER.WEIGHT_DECAY = 0.0005  # 1e-4
_C.SOLVER.NESTEROV = True

_C.SOLVER.TYPE = "SGD"
_C.SOLVER.MAX_EPOCHS = 120  # "nb_adapt_epochs": 100,
# _C.SOLVER.WARMUP = True
_C.SOLVER.MIN_EPOCHS = 20  # "nb_init_epochs": 20,
_C.SOLVER.TRAIN_BATCH_SIZE = 100
_C.SOLVER.TEST_BATCH_SIZE = 100

# Adaptation-specific solver config
_C.SOLVER.AD_LAMBDA = True
_C.SOLVER.AD_LR = True
_C.SOLVER.INIT_LAMBDA = 1

# ---------------------------------------------------------------------------- #
# Domain Adaptation Net (DAN) configs
# ---------------------------------------------------------------------------- #
_C.DAN = CfgNode()
_C.DAN.METHOD = "M3SDA"  # choices=['M3SDA', 'MFSAN']
_C.DAN.USERANDOM = False
_C.DAN.RANDOM_DIM = 1024

# ---------------------------------------------------------------------------- #
# Misc options
# ---------------------------------------------------------------------------- #
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"  # output_dir
_C.OUTPUT.VERBOSE = False  # To discuss, for HPC jobs
_C.OUTPUT.PB_FRESH = 0  # 0 # 50 # 0 to disable  ; MAYBE make it a command line option
_C.OUTPUT.OUT_DIR = os.path.join("outputs", "Tgt" + _C.DATASET.TARGET)


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/office_multisource_adapt/main.py">
"""This example is about domain adaptation for digit image datasets, using PyTorch Lightning.

Reference: https://github.com/thuml/CDAN/blob/master/pytorch/train_image.py
"""

import argparse
import logging

import pytorch_lightning as pl
from config import get_cfg_defaults
from model import get_model
from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar
from pytorch_lightning.loggers import TensorBoardLogger

from kale.loaddata.image_access import ImageAccess
from kale.loaddata.multi_domain import MultiDomainAdapDataset
from kale.utils.seed import set_seed


def arg_parse():
    """Parsing arguments"""
    parser = argparse.ArgumentParser(description="Multi-source domain adaptation")
    parser.add_argument("--cfg", required=True, help="path to config file", type=str)
    parser.add_argument(
        "--devices",
        default=1,
        help="gpu id(s) to use. int(0) for cpu. list[x,y] for xth, yth GPU."
        "str(x) for the first x GPUs. str(-1)/int(-1) for all available GPUs",
    )
    parser.add_argument("--resume", default="", type=str)
    args = parser.parse_args()
    return args


def main():
    """The main for this multi-source domain adaptation example, showing the workflow"""
    args = arg_parse()

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    print(cfg)

    # ---- setup output ----
    format_str = "@%(asctime)s %(name)s [%(levelname)s] - (%(message)s)"
    logging.basicConfig(format=format_str)
    # ---- setup dataset ----
    if type(cfg.DATASET.SOURCE) == list:
        sub_domain_set = cfg.DATASET.SOURCE + [cfg.DATASET.TARGET]
    else:
        sub_domain_set = None
    num_channels = cfg.DATASET.NUM_CHANNELS
    if cfg.DATASET.NAME.upper() == "DIGITS":
        kwargs = {"return_domain_label": True}
    else:
        kwargs = {"download": True, "return_domain_label": True}

    data_access = ImageAccess.get_multi_domain_images(
        cfg.DATASET.NAME.upper(), cfg.DATASET.ROOT, sub_domain_set=sub_domain_set, **kwargs
    )

    # Repeat multiple times to get std
    for i in range(0, cfg.DATASET.NUM_REPEAT):
        seed = cfg.SOLVER.SEED + i * 10
        dataset = MultiDomainAdapDataset(data_access, random_state=seed)
        set_seed(seed)  # seed_everything in pytorch_lightning did not set torch.backends.cudnn
        print(f"==> Building model for seed {seed} ......")
        # ---- setup model and logger ----
        model, train_params = get_model(cfg, dataset, num_channels)

        tb_logger = TensorBoardLogger(cfg.OUTPUT.OUT_DIR, name="seed{}".format(seed))
        checkpoint_callback = ModelCheckpoint(
            filename="{epoch}-{step}-{valid_loss:.4f}",
            monitor="valid_loss",
            mode="min",
        )
        progress_bar = TQDMProgressBar(cfg.OUTPUT.PB_FRESH)

        trainer = pl.Trainer(
            min_epochs=cfg.SOLVER.MIN_EPOCHS,
            max_epochs=cfg.SOLVER.MAX_EPOCHS,
            callbacks=[checkpoint_callback, progress_bar],
            accelerator="gpu" if args.devices != 0 else "cpu",
            devices=args.devices if args.devices != 0 else "auto",
            logger=tb_logger,  # logger,
            # weights_summary='full',
            fast_dev_run=False,  # True,
        )

        trainer.fit(model)
        trainer.test()


if __name__ == "__main__":
    main()
</file>

<file path="examples/office_multisource_adapt/model.py">
"""
Define the learning model and configure training parameters.
"""
# Author: Shuo Zhou
# Initial Date: 09.09.2021

from copy import deepcopy

import torch

from kale.embed.image_cnn import ResNet18Feature, SmallCNNFeature
from kale.pipeline.multi_domain_adapter import create_ms_adapt_trainer
from kale.predict.class_domain_nets import ClassNetSmallImage


def get_config(cfg):
    """
    Sets the hyper-parameters for the optimizer and experiment using the config file

    Args:
        cfg: A YACS config object.
    """
    config_params = {
        "train_params": {
            "adapt_lambda": cfg.SOLVER.AD_LAMBDA,
            "adapt_lr": cfg.SOLVER.AD_LR,
            "lambda_init": cfg.SOLVER.INIT_LAMBDA,
            "nb_adapt_epochs": cfg.SOLVER.MAX_EPOCHS,
            "nb_init_epochs": cfg.SOLVER.MIN_EPOCHS,
            "init_lr": cfg.SOLVER.BASE_LR,
            "batch_size": cfg.SOLVER.TRAIN_BATCH_SIZE,
            "optimizer": {
                "type": cfg.SOLVER.TYPE,
                "optim_params": {
                    "momentum": cfg.SOLVER.MOMENTUM,
                    "weight_decay": cfg.SOLVER.WEIGHT_DECAY,
                    "nesterov": cfg.SOLVER.NESTEROV,
                },
            },
        },
        "data_params": {
            "dataset_group": cfg.DATASET.NAME,
            "dataset_name": cfg.DATASET.NAME + "_Target_" + cfg.DATASET.TARGET,
            "source": "_".join(cfg.DATASET.SOURCE) if cfg.DATASET.SOURCE is not None else None,
            "target": cfg.DATASET.TARGET,
            "size_type": cfg.DATASET.SIZE_TYPE,
            "weight_type": cfg.DATASET.WEIGHT_TYPE,
        },
    }
    return config_params


# Based on https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/utils/experimentation.py
def get_model(cfg, dataset, num_channels):
    """
    Builds and returns a model and associated hyper-parameters according to the config object passed.

    Args:
        cfg: A YACS config object.
        dataset: A multidomain dataset consisting of source and target datasets.
        num_channels: The number of image channels.
    """
    config_params = get_config(cfg)
    train_params = config_params["train_params"]
    train_params_local = deepcopy(train_params)

    # setup feature extractor
    if cfg.DATASET.NAME.upper() == "DIGITS":
        feature_network = SmallCNNFeature(num_channels)
    else:
        feature_network = ResNet18Feature()

    if cfg.DAN.METHOD == "MFSAN":
        feature_network = torch.nn.Sequential(*(list(feature_network.children())[:-1]))

    method_params = {"n_classes": cfg.DATASET.NUM_CLASSES, "target_domain": cfg.DATASET.TARGET}

    model = create_ms_adapt_trainer(
        method=cfg.DAN.METHOD,
        dataset=dataset,
        feature_extractor=feature_network,
        task_classifier=ClassNetSmallImage,
        **method_params,
        **train_params_local,
    )

    return model, train_params
</file>

<file path="examples/office_multisource_adapt/README.md">
# Image Classification: Multi-source Domain Adaptation on Images (e.g. Office, Digits) with Lightning

### 1. Description

This example demonstrates multi-source domain adaptation methods with application in image recognition
on [office-caltech dataset](https://github.com/pykale/data/tree/main/images/office).

### 2. Usage

* Datasets:
  * Digits (10 classes), 4 domains: MNIST, Modified MNIST (MNISTM), UPSP, SVHN
  * Office-31 (31 classes), 3 domains: Amazon (A), DSLR (D), Webcam (W)
  * Office-Caltech (10 classes), 4 domains: Amazon (A), DSLR (D), Webcam (W), Caltech (C)
* Algorithms:
  * [Moment matching for multi-source domain adaptation (M3SDA)](https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.html) [[1](#4-references)][[Code](https://github.com/pykale/pykale/blob/main/kale/pipeline/multi_domain_adapter.py#L105)]
  * [Multiple Feature Spaces Adaptation Network (MFSAN)](https://ojs.aaai.org/index.php/AAAI/article/view/4551) [[2](#4-references)][[Code](https://github.com/pykale/pykale/blob/main/kale/pipeline/multi_domain_adapter.py#L245)]
* Example: Caltech, DSLR, and Webcam (three sources) to Amazon (target) using M3SDA and MFSAN

`python main.py --cfg configs/Office2A-M3SDA.yaml --devices 1`

`python main.py --cfg configs/Office2A-MFSAN.yaml --devices 1`

### 3. Related `kale` API

`kale.embed.image_cnn`: Extract features from small-size (32x32) images using CNN.

`kale.loaddata.image_access`: General APIs for data loaders of image datasets.

`kale.loaddata.multi_domain`: Construct the dataset for (multiple) source and target domains.

`kale.pipeline.multi_domain_adapter`: Multi-source domain adaptation pipelines for image classification.

`kale.predict.class_domain_nets`: Classifiers for data or domain.

`kale.prepdata.image_transform`: Transforms for image data.

### 4. References

[1] Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., & Wang, B. (2019). [Moment matching for multi-source domain adaptation](https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.html). In *ICCV 2019* (pp. 1406-1415).

[2] Zhu, Y., Zhuang, F. and Wang, D. (2019). [Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources](https://ojs.aaai.org/index.php/AAAI/article/view/4551). In *AAAI 2019* (pp. 5989-5996).
</file>

<file path="examples/polypharmacy_gripnet/configs/PoSE_MINI-GripNet.yaml">
SOLVER:
  BASE_LR: 0.01
  MAX_EPOCHS: 2  # For quick testing
</file>

<file path="examples/polypharmacy_gripnet/configs/README.md">
# Configurations

Configurations for experiments using [YAML](https://en.wikipedia.org/wiki/YAML).
</file>

<file path="examples/polypharmacy_gripnet/config.py">
"""
Default configurations for Polypharmacy Side Effect Prediction using GripNet
"""
from yacs.config import CfgNode

# ---------------------------------------------------------
# Config definition
# ---------------------------------------------------------

_C = CfgNode()

# ---------------------------------------------------------
# Dataset
# ---------------------------------------------------------
_C.DATASET = CfgNode()
_C.DATASET.ROOT = "./data"
_C.DATASET.NAME = "pose"
_C.DATASET.URL = "https://github.com/pykale/data/raw/main/graphs/pose_pyg_2.pt"

# ---------------------------------------------------------
# Solver
# ---------------------------------------------------------
_C.SOLVER = CfgNode()
_C.SOLVER.SEED = 1111
_C.SOLVER.BASE_LR = 0.01
_C.SOLVER.EPSILON = 1e-10
_C.SOLVER.MAX_EPOCHS = 66
_C.SOLVER.LOG_EVERY_N_STEPS = 1

# ---------------------------------------------------------
# GripNet supervertex configs
# ---------------------------------------------------------
_C.GRIPN_SV1 = CfgNode()
_C.GRIPN_SV1.NAME = "protein"
_C.GRIPN_SV1.INTER_FEAT_CHANNELS = 16
_C.GRIPN_SV1.INTER_AGG_CHANNELS_LIST = [16, 16]
_C.GRIPN_SV1.EXTER_AGG_CHANNELS_LIST = []
# Elements in `EXTER_AGG_CHANNELS_LIST` should be a list of combinations of supervertex name (str) and embedding
#   dimension (int). If the supervertex is a root supervertex, it should be an empty list.
_C.GRIPN_SV1.MODE = ""  # `MODE` is either "cat" or "add"

_C.GRIPN_SV2 = CfgNode()
_C.GRIPN_SV2.NAME = "drug"
_C.GRIPN_SV2.INTER_FEAT_CHANNELS = 32
_C.GRIPN_SV2.INTER_AGG_CHANNELS_LIST = [16, 16]
_C.GRIPN_SV2.EXTER_AGG_CHANNELS_LIST = [["protein", 16]]
_C.GRIPN_SV2.MODE = "cat"

# ---------------------------------------------------------
# Misc options
# ---------------------------------------------------------
_C.OUTPUT = CfgNode()
_C.OUTPUT.OUT_DIR = "./outputs"


def get_cfg_defaults():
    return _C.clone()
</file>

<file path="examples/polypharmacy_gripnet/main.py">
import argparse
import warnings

import pytorch_lightning as pl
import torch
from config import get_cfg_defaults
from model import get_model, get_supervertex
from torch.utils.data import DataLoader

import kale.utils.seed as seed
from kale.loaddata.polypharmacy_datasets import PolypharmacyDataset
from kale.prepdata.supergraph_construct import SuperEdge, SuperGraph, SuperVertex

warnings.filterwarnings(action="ignore")


def arg_parse():
    parser = argparse.ArgumentParser(description="GripNet Training for Polypharmacy Side Effect Prediction")
    parser.add_argument("--cfg", type=str, default="config.yaml", help="config file path")
    args = parser.parse_args()

    return args


def main():
    args = arg_parse()

    # ---- setup device ----
    device = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device)

    # ---- setup configs ----
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.cfg)
    cfg.freeze()
    seed.set_seed(cfg.SOLVER.SEED)

    # ---- setup dataset and data loader ----
    train_dataset = PolypharmacyDataset(cfg.DATASET.URL, cfg.DATASET.ROOT, cfg.DATASET.NAME, mode="train")
    dataloader_train = DataLoader(train_dataset, batch_size=1)

    # ---- setup supergraph ----
    # create protein and drug supervertex
    supervertex_protein = SuperVertex("protein", train_dataset.protein_feat, train_dataset.protein_edge_index)
    supervertex_drug = SuperVertex("drug", train_dataset.drug_feat, train_dataset.edge_index, train_dataset.edge_type)

    # create superedge form protein to drug supervertex
    superedge = SuperEdge("protein", "drug", train_dataset.protein_drug_edge_index)

    setting_protein = get_supervertex(cfg.GRIPN_SV1)
    setting_drug = get_supervertex(cfg.GRIPN_SV2)

    # construct supergraph
    supergraph = SuperGraph([supervertex_protein, supervertex_drug], [superedge])
    supergraph.set_supergraph_para_setting([setting_protein, setting_drug])

    # ---- setup model and trainer ----
    model = get_model(supergraph, cfg)
    print(model)

    trainer = pl.Trainer(
        default_root_dir=cfg.OUTPUT.OUT_DIR,
        max_epochs=cfg.SOLVER.MAX_EPOCHS,
        log_every_n_steps=cfg.SOLVER.LOG_EVERY_N_STEPS,
    )

    # ---- train, validate and test ----
    # The training set is reused here as the validation and test sets for usage demonstration. See ReadMe for details.
    trainer.fit(model, dataloader_train, dataloader_train)
    _ = trainer.test(model, dataloader_train)


if __name__ == "__main__":
    main()
</file>

<file path="examples/polypharmacy_gripnet/model.py">
from yacs.config import CfgNode

from kale.predict.decode import GripNetLinkPrediction
from kale.prepdata.supergraph_construct import SuperGraph, SuperVertexParaSetting


def get_supervertex(sv_configs: CfgNode) -> SuperVertexParaSetting:
    """Get supervertex parameter setting from configurations."""

    exter_list = sv_configs.EXTER_AGG_CHANNELS_LIST

    if len(exter_list):
        exter_dict = {k: v for k, v in exter_list}

        return SuperVertexParaSetting(
            sv_configs.NAME,
            sv_configs.INTER_FEAT_CHANNELS,
            sv_configs.INTER_AGG_CHANNELS_LIST,
            exter_agg_channels_dict=exter_dict,
            mode=sv_configs.MODE,
        )

    return SuperVertexParaSetting(
        sv_configs.NAME,
        sv_configs.INTER_FEAT_CHANNELS,
        sv_configs.INTER_AGG_CHANNELS_LIST,
    )


def get_model(supergraph: SuperGraph, cfg: CfgNode) -> GripNetLinkPrediction:
    """Get model from the supergraph and configurations."""

    learning_rate = cfg.SOLVER.BASE_LR
    epsilon = cfg.SOLVER.EPSILON

    return GripNetLinkPrediction(supergraph, learning_rate, epsilon)
</file>

<file path="examples/polypharmacy_gripnet/README.md">
# Polypharmacy Side Effect Prediction using GripNet (Link Prediction)

## 1. Description

Polypharmacy is the concurrent use of multiple medications by patients. Its side effects, polypharmacy side effects, are caused by drug combinations rather than by any single drug. In this example, we train a [GripNet model](https://doi.org/10.1016/j.patcog.2022.108973) [1] for predicting polypharmacy side effects.

## 2. GripNet

**Gr**aph **i**nformation **p**ropagation **Net**work ([GripNet](https://doi.org/10.1016/j.patcog.2022.108973)) is an effective and efficient framework for learning node representations on knowledge graphs (Subfigure A) for link prediction and node classification. It introduces a novel supergraph structure (Subfigure B) and constructs a graph neural network that passes messages via a task-specific propagation path based on the supergraph (Subfigure C).

![GripNet Architecture](https://ars.els-cdn.com/content/image/1-s2.0-S0031320322004538-gr2_lrg.jpg)

The original implementation of GripNet is [here](https://github.com/NYXFLOWER/GripNet.git).

## 3. Dataset

The [GripNet](https://doi.org/10.1016/j.patcog.2022.108973) paper studied polypharmacy side effects on the dataset constructed by [Zitnik et al. [2]](https://academic.oup.com/bioinformatics/article/34/13/i457/5045770?login=false) integrating three datasets. This integrated dataset can be regarded as a [knowledge graph](https://en.wikipedia.org/wiki/Knowledge_graph). It contains the relationships between proteins (P-P), proteins and drugs (P-D), and drugs, labeled with the P-P association, P-D association, and 1,317 side effects, respectively. The data statistics are shown in the table below. A standard usage should include a training set, a validation set, and a test set. The lack of a validation set during model fitting (i.e., training) may cause a warning from `pytorch_lightning`.

| Dataset           | Nodes            | Edges      | #Unique Edge Labels |
| ----------------- | ---------------- | ---------- | ------------------- |
| PP-Decagon        | 19,081(P)        | 715,612    | 1                   |
| ChG-TargetDecagon | 3,648(P), 284(D) | 18,690     | 1                   |
| ChChSe-Decagon    | 645(D)           | 4,649,441  | 1,317               |

Note: In this example, we use a [small subset of the integrated dataset above](https://github.com/pykale/data/blob/main/graphs/pose_pyg_2.pt) to illustrate the usage of GripNet in predicting polypharmacy side effects. Here, we reuse the training set as the validation and test sets for the purpose of demonstration only. We will update this example when an improved example dataset becomes available at a later time.

## 4. Usage

- Dataset download: [[here](https://github.com/pykale/data/tree/main/graphs)]

- Algorithm: GripNet

```python
python main.py
```

We provide a `yaml` config file for a quick testing in the `configs` folder. To use it, run:

```python
python main.py --cfg configs/PoSE_MINI-GripNet.yaml
```

## References

[1] Xu, H., Sang, S., Bai, P., Li, R., Yang, L., & Lu, H. (2022). [GripNet: Graph Information Propagation on Supergraph for Heterogeneous Graphs](https://doi.org/10.1016/j.patcog.2022.108973). Pattern Recognition.

[2] Zitnik, M., Agrawal, M., & Leskovec, J. (2018). [Modeling polypharmacy side effects with graph convolutional networks](https://academic.oup.com/bioinformatics/article/34/13/i457/5045770?login=false). Bioinformatics, 34(13), i457-i466.
</file>

<file path="examples/toy_domain_adaptation/main.py">
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.datasets import make_blobs
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder

from kale.interpret.visualize import distplot_1d
from kale.pipeline.multi_domain_adapter import CoIRLS


def main():
    np.random.seed(29118)
    # Generate toy data
    n_samples = 200

    xs, ys = make_blobs(n_samples, centers=[[0, 0], [0, 2]], cluster_std=[0.3, 0.35])
    xt, yt = make_blobs(n_samples, centers=[[2, -2], [2, 0.2]], cluster_std=[0.35, 0.4])

    # visualize toy data
    colors = ["c", "m"]
    x_all = [xs, xt]
    y_all = [ys, yt]
    labels = ["source", "Target"]
    plt.figure(figsize=(8, 5))
    for i in range(2):
        idx_pos = np.where(y_all[i] == 1)
        idx_neg = np.where(y_all[i] == 0)
        plt.scatter(
            x_all[i][idx_pos, 0],
            x_all[i][idx_pos, 1],
            c=colors[i],
            marker="o",
            alpha=0.4,
            label=labels[i] + " positive",
        )
        plt.scatter(
            x_all[i][idx_neg, 0],
            x_all[i][idx_neg, 1],
            c=colors[i],
            marker="x",
            alpha=0.4,
            label=labels[i] + " negative",
        )
    plt.legend()
    plt.title("Source domain and target domain blobs data", fontsize=14, fontweight="bold")
    plt.show()

    clf = RidgeClassifier(alpha=1.0)
    clf.fit(xs, ys)

    yt_pred = clf.predict(xt)
    print("Accuracy on target domain: {:.2f}".format(accuracy_score(yt, yt_pred)))

    # visualize decision scores of non-adaptation classifier
    ys_score = clf.decision_function(xs)
    yt_score = clf.decision_function(xt)
    title = "Ridge classifier decision score distribution"
    title_kwargs = {"fontsize": 14, "fontweight": "bold"}
    hist_kwargs = {"kde": True, "alpha": 0.7}
    plt_labels = ["Source", "Target"]
    distplot_1d(
        [ys_score, yt_score],
        labels=plt_labels,
        xlabel="Decision Scores",
        title=title,
        title_kwargs=title_kwargs,
        hist_kwargs=hist_kwargs,
    ).show()

    # domain adaptation
    clf_ = CoIRLS(lambda_=1)
    # encoding one-hot domain covariate matrix
    covariates = np.zeros(n_samples * 2)
    covariates[:n_samples] = 1
    enc = OneHotEncoder(handle_unknown="ignore")
    covariates_mat = enc.fit_transform(covariates.reshape(-1, 1)).toarray()

    x = np.concatenate((xs, xt))
    clf_.fit(x, ys, covariates_mat)
    yt_pred_ = clf_.predict(xt)
    print("Accuracy on target domain: {:.2f}".format(accuracy_score(yt, yt_pred_)))

    ys_score_ = clf_.decision_function(xs).detach().numpy().reshape(-1)
    yt_score_ = clf_.decision_function(xt).detach().numpy().reshape(-1)
    title = "Domain adaptation classifier decision score distribution"
    distplot_1d(
        [ys_score_, yt_score_],
        labels=plt_labels,
        xlabel="Decision Scores",
        title=title,
        title_kwargs=title_kwargs,
        hist_kwargs=hist_kwargs,
    ).show()


if __name__ == "__main__":
    main()
</file>

<file path="examples/toy_domain_adaptation/README.md">
# Domain Adaptation on Toy Data

### 1. Description

This example demonstrates domain adaptation methods on the generated toy data.

### 2. Usage

* Datasets: Toy data (2 domains and 2 classes) generated by `sklearn.datasets.make_blobs`.
* Algorithms: Covariate Independence Regularized Least Squares (CoIRLS) classifier.

`python main.py`

### 3. Related `kale` API

`kale.interpret.visualize.distplot_1d`: Visualize 1D distributions.

`kale.pipeline.multi_domain_adapter.CoIRLS`: Covariate Independence Regularized Least Squares (CoIRLS) classifier.


### References

[1] Zhou, S., Li, W., Cox, C.R., & Lu, H. (2020). [Side Information Dependence as a Regularizer for Analyzing Human Brain Conditions across Cognitive Experiments](https://ojs.aaai.org//index.php/AAAI/article/view/6179). in *AAAI 2020*, New York, USA.

[2] Zhou, S. (2022). [Interpretable Domain-Aware Learning for Neuroimage Classification](https://etheses.whiterose.ac.uk/31044/1/PhD_thesis_ShuoZhou_170272834.pdf) (Doctoral dissertation, University of Sheffield).
</file>

<file path="examples/toy_domain_adaptation/tutorial.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# PyKale Tutorial: Domain Adaptation on Toy Data\n",
        "| [Open in Colab](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/toy_domain_adaptation/tutorial.ipynb) (click `Runtime`\u2006\u2192\u2006`Run all (Ctrl+F9)` |  [Launch Binder](https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Ftoy_domain_adaptation%2Ftutorial.ipynb) (click `Run`\u2006\u2192\u2006`Run All Cells`) |"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# import seaborn first to avoid seaborn import error caused by newer scipy version, to be solved later\n",
        "import seaborn as sns"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    !pip uninstall --yes imgaug && pip uninstall --yes albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "    !pip install git+https://github.com/pykale/pykale.git\n",
        "    !git clone https://github.com/pykale/pykale.git\n",
        "    %cd pykale/examples/toy_domain_adaptation\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Generate toy data"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_moons, make_blobs"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "n_samples = 1000\n",
        "\n",
        "xs, ys = make_blobs(n_samples, centers=[[0, 0], [0, 2]], cluster_std=[0.3, 0.35])\n",
        "xt, yt = make_blobs(n_samples, centers=[[2, -2], [2, 0.2]], cluster_std=[0.35, 0.4])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "colors = [\"c\", \"m\"]\n",
        "x_all = [xs, xt]\n",
        "y_all = [ys, yt]\n",
        "labels = [\"source\", \"Target\"]\n",
        "for i in range(2):\n",
        "    idx_pos = np.where(y_all[i] == 1)\n",
        "    idx_neg = np.where(y_all[i] == 0)\n",
        "    plt.scatter(x_all[i][idx_pos, 0], x_all[i][idx_pos, 1], c=colors[i], marker=\"o\", alpha=0.4, \n",
        "                label=labels[i] + \" positive\")\n",
        "    plt.scatter(x_all[i][idx_neg, 0], x_all[i][idx_neg, 1], c=colors[i], marker=\"x\", alpha=0.4, \n",
        "                label=labels[i] + \" negative\")\n",
        "plt.legend()\n",
        "plt.title('Source domain and target domain blobs data',fontsize=14,fontweight='bold')"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Classification"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from kale.interpret.visualize import distplot_1d\n",
        "from kale.pipeline.multi_domain_adapter import CoIRLS"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "#### Training a standard Ridge classifier"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "clf = RidgeClassifier(alpha=1.0)\n",
        "clf.fit(xs, ys)\n",
        "\n",
        "yt_pred = clf.predict(xt)\n",
        "print('Accuracy on target domain: {:.2f}'.format(accuracy_score(yt, yt_pred)))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "ys_score = clf.decision_function(xs)\n",
        "yt_score = clf.decision_function(xt)\n",
        "\n",
        "title = \"Ridge classifier decision score distribution\"\n",
        "title_kwargs = {\"fontsize\": 14, \"fontweight\": \"bold\"}\n",
        "hist_kwargs = {\"kde\": True, \"alpha\": 0.7}\n",
        "plt_labels = [\"Source\", \"Target\"]\n",
        "distplot_1d(\n",
        "    [ys_score, yt_score],\n",
        "    title=title,\n",
        "    xlabel=\"Decision Scores\",\n",
        "    labels=plt_labels,\n",
        "    hist_kwargs=hist_kwargs,\n",
        "    title_kwargs=title_kwargs,\n",
        ").show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "#### Training a domain adaptation classifier"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "clf_ = CoIRLS()\n",
        "# encoding one-hot domain covariate matrix\n",
        "covariates = np.zeros(n_samples * 2)\n",
        "covariates[:n_samples] = 1\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "covariates_mat = enc.fit_transform(covariates.reshape(-1, 1)).toarray()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "x = np.concatenate((xs, xt))\n",
        "clf_.fit(x, ys, covariates_mat)\n",
        "yt_pred_ = clf_.predict(xt)\n",
        "print(\"Accuracy on target domain: {:.2f}\".format(accuracy_score(yt, yt_pred_)))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "ys_score_ = clf_.decision_function(xs).detach().numpy().reshape(-1)\n",
        "yt_score_ = clf_.decision_function(xt).detach().numpy().reshape(-1)\n",
        "plt.figure(figsize=(10, 5))\n",
        "title = \"Domain adaptation classifier decision score distribution\"\n",
        "distplot_1d(\n",
        "    [ys_score_, yt_score_],\n",
        "    title=title,\n",
        "    xlabel=\"Decision Scores\",\n",
        "    labels=plt_labels,\n",
        "    hist_kwargs=hist_kwargs,\n",
        "    title_kwargs=title_kwargs,\n",
        ").show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
</file>

<file path="examples/video_loading/main.py">
import os

import matplotlib.pyplot as plt
import torch
from mpl_toolkits.axes_grid1 import ImageGrid
from torchvision import transforms

from kale.loaddata.videos import VideoFrameDataset
from kale.prepdata.video_transform import ImglistToTensor
from kale.utils.download import download_file_gdrive

"""
Ignore this function and look at "main" below.
"""


def plot_video(rows, cols, frame_list, plot_width, plot_height):
    fig = plt.figure(figsize=(plot_width, plot_height))
    grid = ImageGrid(
        fig,
        111,  # similar to subplot(111)
        nrows_ncols=(rows, cols),  # creates 2x2 grid of axes
        axes_pad=0.3,  # pad between axes in inch.
    )

    for index, (ax, im) in enumerate(zip(grid, frame_list)):
        # Iterating over the grid returns the Axes.
        ax.imshow(im)
        ax.set_title(index)
    plt.show()


if __name__ == "__main__":
    """
    This demo uses the dummy dataset inside of the folder "demo_dataset".
    It is structured just like a real dataset would need to be structured.

    TABLE OF CODE CONTENTS:
    1. Minimal demo without image transforms
    2. Minimal demo without sparse temporal sampling for single continuous frame clips, without image transforms
    3. Demo with image transforms
    4. Demo 3 continued with PyTorch dataloader
    5. Demo of using a dataset where samples have multiple separate class labels

    """
    download_file_gdrive(
        id="1U4D23R8u8MJX9KVKb92bZZX-tbpKWtga",
        output_directory=os.path.join(os.getcwd()),
        output_file_name="demo_datasets.zip",
        file_format=".zip",
    )

    videos_root = os.path.join(os.getcwd(), "demo_dataset")
    annotation_file = os.path.join(videos_root, "annotations.txt")

    """ DEMO 1 WITHOUT IMAGE TRANSFORMS """
    dataset = VideoFrameDataset(
        root_path=videos_root,
        annotationfile_path=annotation_file,
        num_segments=5,
        frames_per_segment=1,
        imagefile_template="img_{:05d}.jpg",
        transform=None,
        random_shift=True,
        test_mode=False,
    )

    sample = dataset[0]
    frames = sample[0]  # list of PIL images
    label = sample[1]  # integer label

    plot_video(rows=1, cols=5, frame_list=frames, plot_width=15.0, plot_height=3.0)

    """ DEMO 2 SINGLE CONTINUOUS FRAME CLIP INSTEAD OF SAMPLED FRAMES, WITHOUT TRANSFORMS """
    # If you do not want to use sparse temporal sampling, and instead
    # want to just load N consecutive frames starting from a random
    # start index, this is easy. Simply set NUM_SEGMENTS=1 and
    # FRAMES_PER_SEGMENT=N. Each time a sample is loaded, N
    # frames will be loaded from a new random start index.
    dataset = VideoFrameDataset(
        root_path=videos_root,
        annotationfile_path=annotation_file,
        num_segments=1,
        frames_per_segment=9,
        imagefile_template="img_{:05d}.jpg",
        transform=None,
        random_shift=True,
        test_mode=False,
    )

    sample = dataset[1]
    frames = sample[0]  # list of PIL images
    label = sample[1]  # integer label

    plot_video(rows=3, cols=3, frame_list=frames, plot_width=10.0, plot_height=5.0)

    """ DEMO 3 WITH TRANSFORMS """
    # As of torchvision 0.8.0, torchvision transforms support batches of images
    # of size (BATCH x CHANNELS x HEIGHT x WIDTH) and apply deterministic or random
    # transformations on the batch identically on all images of the batch. Any torchvision
    # transform for image augmentation can thus also be used  for video augmentation.
    preprocess = transforms.Compose(
        [
            ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor
            transforms.Resize(299),  # image batch, resize smaller edge to 299
            transforms.CenterCrop(299),  # image batch, center crop to square 299x299
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    dataset = VideoFrameDataset(
        root_path=videos_root,
        annotationfile_path=annotation_file,
        num_segments=5,
        frames_per_segment=1,
        imagefile_template="img_{:05d}.jpg",
        transform=preprocess,
        random_shift=True,
        test_mode=False,
    )

    sample = dataset[1]
    frame_tensor = sample[0]  # tensor of shape (NUM_SEGMENTS*FRAMES_PER_SEGMENT) x CHANNELS x HEIGHT x WIDTH
    label = sample[1]  # integer label

    print("Video Tensor Size:", frame_tensor.size())

    """
    Denormalize is just for visualization purposes, to undo the transforms applied
    to the list of frames of a video.
    """

    def denormalize(video_tensor):
        """
        Undoes mean/standard deviation normalization, zero to one scaling,
        and channel rearrangement for a batch of images.
        args:
            video_tensor: a (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor
        """
        inverse_normalize = transforms.Normalize(
            mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]
        )
        return (inverse_normalize(video_tensor) * 255.0).type(torch.uint8).permute(0, 2, 3, 1).numpy()

    frame_tensor = denormalize(frame_tensor)
    plot_video(rows=1, cols=5, frame_list=frames, plot_width=15.0, plot_height=3.0)

    """ DEMO 3 CONTINUED: DATALOADER """
    dataloader = torch.utils.data.DataLoader(
        dataset=dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True
    )

    for epoch in range(10):
        for video_batch, labels in dataloader:
            """
            Insert Training Code Here
            """
            print(labels)
            print("\nVideo Batch Tensor Size:", video_batch.size())
            print("Batch Labels Size:", labels.size())
            break
        break

    """ DEMO 5: SAMPLES WITH MULTIPLE LABELS """
    """
    Apart from supporting just a single label per sample, VideoFrameDataset also supports multi-label samples,
    where a sample can be associated with more than just one label. EPIC-KITCHENS, for example, associates a
    noun, verb, and action with each video clip. To support this, instead of each row in annotations.txt
    being (VIDEO_PATH, START_FRAME, END_FRAME, LABEL_ID), each row can also be
    (VIDEO_PATH, START_FRAME, END_FRAME, LABEL_1_ID, ..., LABEL_N_ID). An example of this can be seen in the
    directory `demo_dataset_multilabel`.

    Each sample returned by VideoFrameDataset is then ((FRAMESxCHANNELSxHEIGHTxWIDTH), (LABEL_1, ..., LABEL_N)).
    When paired with the `torch.utils.data.DataLoader`, instead of yielding each batch as
    ((BATCHxFRAMESxCHANNELSxHEIGHTxWIDTH), (BATCH)) where the second tuple item is the labels of the batch,
    `torch.utils.data.DataLoader` returns a batch as ((BATCHxFRAMESxCHANNELSxHEIGHTxWIDTH), ((BATCH),...,(BATCH))
    where the second tuple item is itself a tuple, with N BATCH-sized tensors of labels, where N is the
    number of labels assigned to each sample.
    """
    videos_root = os.path.join(os.getcwd(), "demo_dataset_multilabel")
    annotation_file = os.path.join(videos_root, "annotations.txt")

    dataset = VideoFrameDataset(
        root_path=videos_root,
        annotationfile_path=annotation_file,
        num_segments=5,
        frames_per_segment=1,
        imagefile_template="img_{:05d}.jpg",
        transform=preprocess,
        random_shift=True,
        test_mode=False,
    )

    dataloader = torch.utils.data.DataLoader(
        dataset=dataset, batch_size=3, shuffle=True, num_workers=2, pin_memory=True
    )

    print("\nMulti-Label Example")
    for epoch in range(10):
        for batch in dataloader:
            """
            Insert Training Code Here
            """
            video_batch, (labels1, labels2, labels3) = batch

            print("Video Batch Tensor Size:", video_batch.size())
            print("Labels1 Size:", labels1.size())  # == batch_size
            print("Labels2 Size:", labels2.size())  # == batch_size
            print("Labels3 Size:", labels3.size())  # == batch_size

            break
        break
</file>

<file path="examples/video_loading/README.md">
# Efficient Video Dataset Loading, Preprocessing, and Augmentation
Author: [Raivo Koot](https://github.com/RaivoKoot)

If you are completely unfamiliar with loading datasets in PyTorch using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`, I recommend
getting familiar with these first through [PyTorch data loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) or
[PyTorch dataloading examples](https://github.com/utkuozbulak/pytorch-custom-dataset-examples).
### Overview: This example demonstrates the use of `kale.loaddata.videos.VideoFrameDataset`
The VideoFrameDataset class serves to `easily`, `efficiently` and `effectively` load video samples from video datasets in PyTorch.
1) Easily because this dataset class can be used with custom datasets with minimum effort and no modification. The class merely expects the
video dataset to have a certain structure on disk and expects a .txt annotation file that enumerates each video sample. Details on this
can be found at `https://pykale.readthedocs.io/en/latest/kale.loaddata.html#kale-loaddata-videos-module`.
2) Efficiently because the video loading pipeline that this class implements is very fast. This minimizes GPU waiting time during training by eliminating input bottlenecks
that can slow down training time by several folds.
3) Effectively because the implemented sampling strategy for video frames is very strong. Video training using the entire sequence of
video frames (often several hundred) is too memory and compute intense. Therefore, this implementation samples frames evenly from the video (sparse temporal sampling)
so that the loaded frames represent every part of the video, with support for arbitrary and differing video lengths within the same dataset.
This approach has shown to be very effective and is taken from
["Temporal Segment Networks (ECCV2016)"](https://arxiv.org/abs/1608.00859) with modifications.

In conjunction with PyTorch's DataLoader, the VideoFrameDataset class returns video batch tensors of size `BATCH x FRAMES x CHANNELS x HEIGHT x WIDTH`.

For a demo, visit `main.py`.
### QuickDemo (main.py)
```python
root = os.path.join(os.getcwd(), 'demo_dataset')  # Folder in which all videos lie in a specific structure
annotation_file = os.path.join(root, 'annotations.txt')  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_INDEX)

""" DEMO 1 WITHOUT IMAGE TRANSFORMS """
dataset = VideoFrameDataset(
    root_path=root,
    annotationfile_path=annotation_file,
    num_segments=5,
    frames_per_segment=1,
    image_template='img_{:05d}.jpg',
    transform=None,
    random_shift=True,
    test_mode=False
)

sample = dataset[0]  # take first sample of dataset
frames = sample[0]   # list of PIL images
label = sample[1]    # integer label

for image in frames:
    plt.imshow(image)
    plt.title(label)
    plt.show()
    plt.pause(1)
```
![alt text](https://github.com/RaivoKoot/images/blob/main/Action_Video.jpg "Action Video")
# Table of Contents
- [Efficient Video Dataset Loading, Preprocessing, and Augmentation](#efficient-video-dataset-loading-preprocessing-and-augmentation)
    - [Overview: This example demonstrates the use of `kale.loaddata.videos.VideoFrameDataset`](#overview-this-example-demonstrates-the-use-of-kaleloaddatavideosvideoframedataset)
    - [QuickDemo (main.py)](#quickdemo-mainpy)
- [Table of Contents](#table-of-contents)
    - [1. Requirements](#1-requirements)
    - [2. Custom Dataset](#2-custom-dataset)
    - [3. Video Frame Sampling Method](#3-video-frame-sampling-method)
    - [4. Alternate Video Frame Sampling Methods](#4-alternate-video-frame-sampling-methods)
    - [5. Using VideoFrameDataset for training](#5-using-videoframedataset-for-training)
    - [6. Allowing Multiple Labels per Sample](#6-allowing-multiple-labels-per-sample)
    - [7. Conclusion](#7-conclusion)
    - [8. Acknowledgements](#8-acknowledgements)

### 1. Requirements
```
# Without these three, VideoFrameDataset will not work.
torchvision >= 0.10.0
torch >= 1.9.0
python >= 3.8,<3.11
```

### 2. Custom Dataset
(This description explains using custom datasets where each sample has a single class label. If you want to know how to
use a dataset where a sample can have more than a single class label, read this anyways and then read `6.` below)

To use any dataset, two conditions must be met.
1) The video data must be supplied as RGB frames, each frame saved as an image file. Each video must have its own folder, in which the frames of
that video lie. The frames of a video inside its folder must be named uniformly with consecutive indices such as `img_00001.jpg` ... `img_00120.jpg`, if there are 120 frames.
   Indices can start at zero or any other number and the exact file name template can be chosen freely. The filename template
   for frames in this example is "img_{:05d}.jpg" (python string formatting, specifying 5 digits after the underscore), and must be supplied to the
   constructor of VideoFrameDataset as a parameter. Each video folder must lie inside some `root` folder.
2) To enumerate all video samples in the dataset and their required metadata, a `.txt` annotation file must be manually created that contains a row for each
video clip sample in the dataset. The training, validation, and testing datasets must have separate annotation files. Each row must be a space-separated list that contains
`VIDEO_PATH START_FRAME END_FRAME CLASS_INDEX`. The `VIDEO_PATH` of a video sample should be provided without the `root` prefix of this dataset.

This example project demonstrates this using a dummy dataset inside of `demo_dataset/`, which is the `root` dataset folder of this example. When you run main.py, the dataset
folder will be automatically downloaded to this directory. The folder structure looks as follows:
```
demo_dataset
‚îÇ
‚îú‚îÄ‚îÄ‚îÄannotations.txt
‚îú‚îÄ‚îÄ‚îÄjumping # arbitrary class folder naming
‚îÇ       ‚îú‚îÄ‚îÄ‚îÄ0001  # arbitrary video folder naming
‚îÇ       ‚îÇ     ‚îú‚îÄ‚îÄ‚îÄimg_00001.jpg
‚îÇ       ‚îÇ     .
‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄimg_00017.jpg
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ0002
‚îÇ             ‚îú‚îÄ‚îÄ‚îÄimg_00001.jpg
‚îÇ             .
‚îÇ             ‚îî‚îÄ‚îÄ‚îÄimg_00018.jpg
‚îÇ
‚îî‚îÄ‚îÄ‚îÄrunning # arbitrary folder naming, no videos inside yet


```
The accompanying annotation `.txt` file contains the following rows (PATH, START_FRAME, END_FRAME, LABEL_ID)
```
jumping/0001 1 17 0
jumping/0002 1 18 0
```
Another annotations file that uses multiple clips from each video could be
```
jumping/0001 1 8 0
jumping/0001 5 17 0
jumping/0002 1 18 0
jumping/0002 10 18 0
```
(END_FRAME is inclusive)

Instantiating a VideoFrameDataset with the `root_path` parameter pointing to `demo_dataset`, the `annotationsfile_path` parameter pointing to the annotation file, and
the `imagefile_template` parameter as "img_{:05d}.jpg", is all that it takes to start using the VideoFrameDataset class.
### 3. Video Frame Sampling Method
When loading a video, only a number of its frames are loaded. They are chosen in the following way:
1. The frame index range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS even segments. From each segment, a random start-index is sampled from which FRAMES_PER_SEGMENT consecutive indices are loaded.
This results in NUM_SEGMENTS*FRAMES_PER_SEGMENT chosen indices, whose frames are loaded as PIL images and put into a list and returned when calling
`dataset[i]`.

![alt text](https://github.com/RaivoKoot/images/blob/main/Sparse_Temporal_Sampling.jpg "Sparse-Temporal-Sampling-Strategy")

### 4. Alternate Video Frame Sampling Methods
If you do not want to use sparse temporal sampling and instead want to sample a single N-frame continuous
clip from a video, this is possible. Set `NUM_SEGMENTS=1` and `FRAMES_PER_SEGMENT=N`. Because VideoFrameDataset
will chose a random start index per segment and take `NUM_SEGMENTS` continuous frames from each sampled start
index, this will result in a single N-frame continuous clip per video that starts at a random index.
An example of this is in `demo.py`.

### 5. Using VideoFrameDataset for training
As demonstrated in `main.py`, we can use PyTorch's `torch.utils.data.DataLoader` class with VideoFrameDataset to take care of shuffling, batching, and more.
To turn the lists of PIL images returned by VideoFrameDataset into tensors, the transform `kale.prepdata.video_transform.ImglistToTensor()` can be supplied
as the `transform` parameter to VideoFrameDataset. This turns a list of N PIL images into a batch of images/frames of shape `N x CHANNELS x HEIGHT x WIDTH`.
We can further chain preprocessing and augmentation functions that act on batches of images onto the end of `ImglistToTensor()`, as seen in `main.py`.

As of `torchvision 0.8.0`, all torchvision transforms can now also operate on batches of images, and they apply deterministic or random transformations
on the batch identically on all images of the batch. Because a single video-tensor (FRAMES x CHANNELS x HEIGHT x WIDTH)
has the same shape as an image batch tensor (BATCH x CHANNELS x HEIGHT x WIDTH), any torchvision transform can be used here to apply video-uniform preprocessing and augmentation.

REMEMBER:
Pytorch transforms are applied to individual dataset samples (in this case a list of PIL images of a video, or a video-frame tensor after `ImglistToTensor()`) before
batching. So, any transforms used here must expect its input to be a frame tensor of shape `FRAMES x CHANNELS x HEIGHT x WIDTH` or a list of PIL images if `ImglistToTensor()` is not used.

### 6. Allowing Multiple Labels per Sample
Your dataset labels might be more complicated than just a single label id per sample. For example, in the EPIC-KITCHENS dataset
each video clip has a verb class, noun class, and action class. In this case, each sample is associated with three label ids.
To accommodate for datasets where a sample can have N integer labels, `annotation.txt` files can be used where each row
is space separated `PATH,   FRAME_START,    FRAME_END,    LABEL_1_ID,    ...,    LABEL_N_ID`, instead of
`PATH,   FRAME_START,    FRAME_END,    LABEL_ID`. The VideoFrameDataset class
can handle this type of annotation files too, without changing anything apart from the rows in your `annotations.txt`.

The `annotations.txt` file for a dataset where multiple clip samples can come from the same video and each sample has
three labels, would have rows like `PATH,   START_FRAME,    END_FRAME,    LABEL1,    LABEL2,    LABEL3` as seen below
```
jumping/0001 1 8 0 2 1
jumping/0001 5 17 0 10 3
jumping/0002 1 18 0 5 3
running/0001 10 15 1 3 3
running/0001 5 10 1 1 0
running/0002 1 15 1 12 4
```

When you use `torch.utils.data.DataLoader` with VideoFrameDataset to retrieve your batches during
training, the dataloader then no longer returns batches as a `( (BATCHxFRAMESxHEIGHTxWIDTH) , (BATCH) )` tuple, where the second item is
just a list/tensor of the batch's labels. Instead, the second item is replaced with the tuple
`( (BATCH) ... (BATCH) )` where the first BATCH-sized list gives label_1 for the whole batch, and the last BATCH-sized
list gives label_n for the whole batch.

A demo of this can be found at the end in `main.py`. It uses the dummy dataset in directory `demo_dataset_multilabel`,
that is also automatically downloaded.

### 7. Conclusion
A proper code-based explanation on how to use VideoFrameDataset for training is provided in `main.py`

### 8. Acknowledgements
We thank the authors of TSN for their [codebase](https://github.com/yjxiong/tsn-pytorch), from which we took VideoFrameDataset and adapted it
for general use and compatibility.
```
@InProceedings{wang2016_TemporalSegmentNetworks,
    title={Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
    author={Limin Wang and Yuanjun Xiong and Zhe Wang and Yu Qiao and Dahua Lin and
            Xiaoou Tang and Luc {Val Gool}},
    booktitle={The European Conference on Computer Vision (ECCV)},
    year={2016}
}
```
</file>

<file path="examples/README.md">
# Examples

Get started with these examples that demonstrate key functionalities of PyKale.

Naming convention: `data_method` or `problem_method`.

## Separate data and code

All data in examples are from the public domain so they will be downloaded into local directories, either automatically or following instructions in specific examples. To keep this repository size small, we do not upload data here unless the size is less than 300KB. This is done by setting `.gitignore`. Data can be shared at [PyKale Data Repository](https://github.com/pykale/data) or other external locations, e.g., Google Drive.

## Examples are available on synthetic data and three application areas

- Synthetic data analysis
  - [Toy data classification with domain adaptation](https://github.com/pykale/pykale/tree/main/examples/toy_domain_adaptation)
- Image/video recognition
  - Image classification on [CIFAR via transformers](https://github.com/pykale/pykale/tree/master/examples/cifar_cnntransformer), [CIFAR via ISONet](https://github.com/pykale/pykale/tree/master/examples/cifar_isonet), [digits via domain adaptation](https://github.com/pykale/pykale/tree/master/examples/digits_dann), [digits/office via multi-source domain adaptation](https://github.com/pykale/pykale/tree/main/examples/office_multisource_adapt)
  - [Action recognition via domain adaptation](https://github.com/pykale/pykale/tree/master/examples/action_dann)
  - [Video loading](https://github.com/pykale/pykale/tree/master/examples/video_loading)
- Bioinformatics data analysis
  - [Drug-target interaction prediction](https://github.com/pykale/pykale/tree/master/examples/bindingdb_deepdta)
  - [Polypharmacy side effect prediction](https://github.com/pykale/pykale/tree/master/examples/polypharmacy_gripnet)
- Medical image analysis
  - [Cardiac MRI diagnosis](https://github.com/pykale/pykale/tree/master/examples/cmri_mpca)
  - [Brain fMRI classification](https://github.com/pykale/pykale/tree/main/examples/multisite_neuroimg_adapt)
</file>

<file path="kale/embed/attention_cnn.py">
from typing import Any, Tuple, Union

import torch
import torch.nn as nn

from kale.embed.positional_encoding import PositionalEncoding
from kale.prepdata.tensor_reshape import seq_to_spatial, spatial_to_seq


class ContextCNNGeneric(nn.Module):
    """
    A template to construct a feature extractor consisting of a CNN followed by a
    sequence-to-sequence contextualizer like a Transformer-Encoder. Before inputting the CNN output
    tensor to the contextualizer, the tensor's spatial dimensions are unrolled
    into a sequence.

    Args:
        cnn (nn.Module): any convolutional neural network that takes in batches of images of
             shape (batch_size, channels, height, width) and outputs tensor
             representations of shape (batch_size, out_channels, out_height, out_width).
        cnn_output_shape (tuple): A tuple of shape (batch_size, num_channels, height, width)
                           describing the output shape of the given CNN (required).
        contextualizer (nn.Module, optional): A sequence-to-sequence model that takes inputs of shape
                         (num_timesteps, batch_size, num_features) and uses
                         attention to contextualize the sequence and returns
                         a sequence of the exact same shape. This will mainly be
                         a Transformer-Encoder (required).
        output_type (string): One of 'sequence' or 'spatial'. If Spatial then the final
                      output of the model, which is a sequence, will be reshaped
                      to resemble the image-batch shape of the output of the CNN.
                      If Sequence then the output sequence is returned as is (required).

    Examples:
        >>> cnn = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3),
        >>>                     nn.Conv2d(32, 64, kernel_size=3),
        >>>                     nn.MaxPool2d(2))
        >>> cnn_output_shape = (-1, 64, 8, 8)
        >>> contextualizer = nn.TransformerEncoderLayer(...)
        >>> output_type = 'spatial'
        >>>
        >>> attention_cnn = ContextCNNGeneric(cnn, cnn_output_shape, contextualizer, output_type)
        >>> output = attention_cnn(torch.randn((32,3,16,16)))
        >>>
        >>> output.size() == cnn_output_shape # True
    """

    def __init__(
        self,
        cnn: nn.Module,
        cnn_output_shape: Tuple[int, int, int, int],
        contextualizer: Union[nn.Module, Any],
        output_type: str,
    ):
        super(ContextCNNGeneric, self).__init__()
        assert output_type in ["spatial", "sequence"], (
            "parameter 'output_type' must be one of ('spatial', 'sequence')" + f" but is {output_type}"
        )

        self.cnn = cnn
        self.cnn_output_shape = cnn_output_shape
        self.contextualizer = contextualizer
        self.output_type = output_type

    def forward(self, x: torch.Tensor):
        """
        Pass the input through the cnn and then the contextualizer.

        Args:
            x: input image batch exactly as for CNNs (required).
        """
        cnn_rep = self.cnn(x)
        seq_rep = spatial_to_seq(cnn_rep)
        seq_rep = self.contextualizer(seq_rep)

        output = seq_rep
        if self.output_type == "spatial":
            desired_height = self.cnn_output_shape[2]
            desired_width = self.cnn_output_shape[3]
            output = seq_to_spatial(output, desired_height, desired_width)

        return output


class CNNTransformer(ContextCNNGeneric):
    """
    A feature extractor consisting of a given CNN backbone followed by a standard
    Transformer-Encoder. See documentation of "ContextCNNGeneric" for more
    information.

    Args:
        cnn (nn.Module): any convolutional neural network that takes in batches of images of
             shape (batch_size, channels, height, width) and outputs tensor
             representations of shape (batch_size, out_channels, out_height, out_width) (required).
        cnn_output_shape (tuple): a tuple of shape (batch_size, num_channels, height, width)
                           describing the output shape of the given CNN (required).
        num_layers (int): number of attention layers in the Transformer-Encoder (required).
        num_heads (int): number of attention heads in each transformer block (required).
        dim_feedforward (int): number of neurons in the intermediate dense layer of
                          each transformer feedforward block (required).
        dropout (float): dropout rate of the transformer layers (required).
        output_type (string): one of 'sequence' or 'spatial'. If Spatial then the final
                      output of the model, which is the sequence output of the
                      Transformer-Encoder, will be reshaped to resemble the
                      image-batch shape of the output of the CNN (required).
        positional_encoder (nn.Module): None or a nn.Module that expects inputs of
                            shape (sequence_length, batch_size, embedding_dim)
                            and returns the same input after adding
                            some positional information to the embeddings. If
                            `None`, then the default and fixed sin-cos positional
                            encodings of base transformers are applied (optional).

    Examples:
        See pykale/examples/cifar_cnntransformer/model.py
    """

    def __init__(
        self,
        cnn: nn.Module,
        cnn_output_shape: Tuple[int, int, int, int],
        num_layers: int,
        num_heads: int,
        dim_feedforward: int,
        dropout: float,
        output_type: str,
        positional_encoder: nn.Module = None,
    ):
        num_channels = cnn_output_shape[1]
        height = cnn_output_shape[2]
        width = cnn_output_shape[3]

        encoder_layer = nn.TransformerEncoderLayer(num_channels, num_heads, dim_feedforward, dropout)
        encoder_normalizer = nn.LayerNorm(num_channels)
        encoder = nn.TransformerEncoder(encoder_layer, num_layers, encoder_normalizer)

        if positional_encoder is None:
            positional_encoder = PositionalEncoding(d_model=num_channels, max_len=height * width)
        else:
            # allows for passing the identity block to skip this step
            # or chosing a different encoding
            positional_encoder = positional_encoder

        transformer_input_dropout = nn.Dropout(dropout)
        contextualizer = nn.Sequential(positional_encoder, transformer_input_dropout, encoder)

        super(CNNTransformer, self).__init__(cnn, cnn_output_shape, contextualizer, output_type)

        # Copied from https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer
        for p in encoder.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
</file>

<file path="kale/embed/factorization.py">
# =============================================================================
# Author: Shuo Zhou, shuo.zhou@sheffield.ac.uk
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""Python implementation of a tensor factorization algorithm Multilinear Principal Component Analysis (MPCA)
    and a matrix factorization algorithm Maximum Independence Domain Adaptation (MIDAÔºâ
"""
import logging
import warnings

import numpy as np
from numpy.linalg import multi_dot
from scipy import linalg
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.preprocessing import KernelCenterer, LabelBinarizer
from sklearn.utils.validation import check_is_fitted
from tensorly.base import fold, unfold
from tensorly.tenalg import multi_mode_dot


def _check_n_dim(x, n_dims):
    """Raise error if the number of dimensions of the input data is not consistent with the expected value.

    Args:
        x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)
        n_dims (int): number of dimensions expected, i.e. N+1

    """
    if not x.ndim == n_dims:
        error_msg = "The expected number of dimensions is %s but it is %s for given data" % (n_dims, x.ndim)
        logging.error(error_msg)
        raise ValueError(error_msg)


def _check_shape(x, shape_):
    """Raise error if the shape for each sample (i.e. excluding the first dimension) of the input data is not consistent
        with the given shape.

    Args:
        x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)
        shape_: expected shape for each sample, i.e. (I_1, I_2, ..., I_N)

    """
    if not x.shape[1:] == shape_:
        error_msg = "The expected shape of data is %s, but %s for given data" % (x.shape[1:], shape_)
        logging.error(error_msg)
        raise ValueError(error_msg)


def _check_tensor_dim_shape(x, n_dims, shape_):
    """Check whether the number of dimensions of the input data and the shape for each sample are consistent with
        expected values

    Args:
        x (array-like): input data, shape (n_samples, I_1, I_2, ..., I_N)
        n_dims (int): number of dimensions expected, i.e. N+1
        shape_: expected shape for each sample, i.e. (I_1, I_2, ..., I_N)

    """
    _check_n_dim(x, n_dims)
    _check_shape(x, shape_)


class MPCA(BaseEstimator, TransformerMixin):
    """MPCA implementation compatible with scikit-learn

    Args:
        var_ratio (float, optional): Percentage of variance explained (between 0 and 1). Defaults to 0.97.
        max_iter (int, optional): Maximum number of iteration. Defaults to 1.
        vectorize (bool): Whether return the transformed/projected tensor in vector. Defaults to False.
        n_components (int): Number of components to keep. Applies only when vectorize=True. Defaults to None.

    Attributes:
        proj_mats (list of arrays): A list of transposed projection matrices, shapes (P_1, I_1), ...,
            (P_N, I_N), where P_1, ..., P_N are output tensor shape for each sample.
        idx_order (array-like): The ordering index of projected (and vectorized) features in decreasing variance.
        mean_ (array-like): Per-feature empirical mean, estimated from the training set, shape (I_1, I_2, ..., I_N).
        shape_in (tuple): Input tensor shapes, i.e. (I_1, I_2, ..., I_N).
        shape_out (tuple): Output tensor shapes, i.e. (P_1, P_2, ..., P_N).

    Reference:
        Haiping Lu, K.N. Plataniotis, and A.N. Venetsanopoulos, "MPCA: Multilinear Principal Component Analysis of
        Tensor Objects", IEEE Transactions on Neural Networks, Vol. 19, No. 1, Page: 18-39, January 2008. For initial
        Matlab implementation, please go to https://uk.mathworks.com/matlabcentral/fileexchange/26168.

    Examples:
        >>> import numpy as np
        >>> from kale.embed.factorization import MPCA
        >>> x = np.random.random((40, 20, 25, 20))
        >>> x.shape
        (40, 20, 25, 20)
        >>> mpca = MPCA()
        >>> x_projected = mpca.fit_transform(x)
        >>> x_projected.shape
        (40, 18, 23, 18)
        >>> x_projected = mpca.transform(x)
        >>> x_projected.shape
        (40, 7452)
        >>> x_projected = mpca.transform(x)
        >>> x_projected.shape
        (40, 50)
        >>> x_rec = mpca.inverse_transform(x_projected)
        >>> x_rec.shape
        (40, 20, 25, 20)
    """

    def __init__(self, var_ratio=0.97, max_iter=1, vectorize=False, n_components=None):
        self.var_ratio = var_ratio
        if max_iter > 0 and isinstance(max_iter, int):
            self.max_iter = max_iter
        else:
            msg = "Number of max iterations must be a positive integer but given %s" % max_iter
            logging.error(msg)
            raise ValueError(msg)
        self.proj_mats = []
        self.vectorize = vectorize
        self.n_components = n_components

    def fit(self, x, y=None):
        """Fit the model with input training data x.

        Args
            x (array-like tensor): Input data, shape (n_samples, I_1, I_2, ..., I_N), where n_samples is the number of
                samples, I_1, I_2, ..., I_N are the dimensions of corresponding mode (1, 2, ..., N), respectively.
            y (None): Ignored variable.

        Returns:
            self (object). Returns the instance itself.
        """
        self._fit(x)
        return self

    def _fit(self, x):
        """Solve MPCA"""

        shape_ = x.shape  # shape of input data
        n_dims = x.ndim

        self.shape_in = shape_[1:]
        self.mean_ = np.mean(x, axis=0)
        x = x - self.mean_

        # init
        shape_out = ()
        proj_mats = []

        # get the output tensor shape based on the cumulative distribution of eigen values for each mode
        for i in range(1, n_dims):
            mode_data_mat = unfold(x, mode=i)
            singular_vec_left, singular_val, singular_vec_right = linalg.svd(mode_data_mat, full_matrices=False)
            eig_values = np.square(singular_val)
            idx_sorted = (-1 * eig_values).argsort()
            cum = eig_values[idx_sorted]
            tot_var = np.sum(cum)

            for j in range(1, cum.shape[0] + 1):
                if np.sum(cum[:j]) / tot_var > self.var_ratio:
                    shape_out += (j,)
                    break
            proj_mats.append(singular_vec_left[:, idx_sorted][:, : shape_out[i - 1]].T)

        # set n_components to the maximum n_features if it is None
        if self.n_components is None:
            self.n_components = int(np.prod(shape_out))

        for i_iter in range(self.max_iter):
            for i in range(1, n_dims):  # ith mode
                x_projected = multi_mode_dot(
                    x,
                    [proj_mats[m] for m in range(n_dims - 1) if m != i - 1],
                    modes=[m for m in range(1, n_dims) if m != i],
                )
                mode_data_mat = unfold(x_projected, i)

                singular_vec_left, singular_val, singular_vec_right = linalg.svd(mode_data_mat, full_matrices=False)
                eig_values = np.square(singular_val)
                idx_sorted = (-1 * eig_values).argsort()
                proj_mats[i - 1] = (singular_vec_left[:, idx_sorted][:, : shape_out[i - 1]]).T

        x_projected = multi_mode_dot(x, proj_mats, modes=[m for m in range(1, n_dims)])
        x_proj_unfold = unfold(x_projected, mode=0)  # unfold the tensor projection to shape (n_samples, n_features)
        # x_proj_cov = np.diag(np.dot(x_proj_unfold.T, x_proj_unfold))  # covariance of unfolded features
        x_proj_cov = np.sum(np.multiply(x_proj_unfold.T, x_proj_unfold.T), axis=1)  # memory saving computing covariance
        idx_order = (-1 * x_proj_cov).argsort()

        self.proj_mats = proj_mats
        self.idx_order = idx_order
        self.shape_out = shape_out
        self.n_dims = n_dims

        return self

    def transform(self, x):
        """Perform dimension reduction on x

        Args:
            x (array-like tensor): Data to perform dimension reduction, shape (n_samples, I_1, I_2, ..., I_N).

        Returns:
            array-like tensor:
                Projected data in lower dimension, shape (n_samples, P_1, P_2, ..., P_N) if self.vectorize==False.
                If self.vectorize==True, features will be sorted based on their explained variance ratio, shape
                (n_samples, P_1 * P_2 * ... * P_N) if self.n_components is None, and shape (n_samples, n_components)
                if self.n_component is a valid integer.
        """
        # reshape x to shape (1, I_1, I_2, ..., I_N) if x in shape (I_1, I_2, ..., I_N), i.e. n_samples = 1
        if x.ndim == self.n_dims - 1:
            x = x.reshape((1,) + x.shape)
        _check_tensor_dim_shape(x, self.n_dims, self.shape_in)
        x = x - self.mean_

        # projected tensor in lower dimensions
        x_projected = multi_mode_dot(x, self.proj_mats, modes=[m for m in range(1, self.n_dims)])

        if self.vectorize:
            x_projected = unfold(x_projected, mode=0)
            x_projected = x_projected[:, self.idx_order]
            if isinstance(self.n_components, int):
                n_features = int(np.prod(self.shape_out))
                if self.n_components > n_features:
                    self.n_components = n_features
                    warn_msg = "n_components exceeds the maximum number, all features will be returned."
                    logging.warning(warn_msg)
                    warnings.warn(warn_msg)
                x_projected = x_projected[:, : self.n_components]

        return x_projected

    def inverse_transform(self, x):
        """Reconstruct projected data to the original shape and add the estimated mean back

        Args:
            x (array-like tensor): Data to be reconstructed, shape (n_samples, P_1, P_2, ..., P_N), if
                self.vectorize == False, where P_1, P_2, ..., P_N are the reduced dimensions of corresponding
                mode (1, 2, ..., N), respectively. If self.vectorize == True, shape (n_samples, self.n_components)
                or shape (n_samples, P_1 * P_2 * ... * P_N).

        Returns:
            array-like tensor:
                Reconstructed tensor in original shape, shape (n_samples, I_1, I_2, ..., I_N)
        """
        # reshape x to tensor in shape (n_samples, self.shape_out) if x has been unfolded
        if x.ndim <= 2:
            if x.ndim == 1:
                # reshape x to a 2D matrix (1, n_components) if x in shape (n_components,)
                x = x.reshape((1, -1))
            n_samples = x.shape[0]
            n_features = x.shape[1]
            if n_features <= np.prod(self.shape_out):
                x_ = np.zeros((n_samples, np.prod(self.shape_out)))
                x_[:, self.idx_order[:n_features]] = x[:]
            else:
                msg = "Feature dimension exceeds the shape upper limit."
                logging.error(msg)
                raise ValueError(msg)

            x = fold(x_, mode=0, shape=((n_samples,) + self.shape_out))

        x_rec = multi_mode_dot(x, self.proj_mats, modes=[m for m in range(1, self.n_dims)], transpose=True)

        x_rec = x_rec + self.mean_

        return x_rec


class MIDA(BaseEstimator, TransformerMixin):
    """Maximum independence domain adaptation
    Args:
        n_components (int): Number of components to keep.
        kernel (str): "linear", "rbf", or "poly". Kernel to use for MIDA. Defaults to "linear".
        mu (float): Hyperparameter of the l2 penalty. Defaults to 1.0.
        eta (float): Hyperparameter of the label dependence. Defaults to 1.0.
        augmentation (bool): Whether using covariates as augment features. Defaults to False.
        kernel_params (dict or None): Parameters for the kernel. Defaults to None.

    References:
        [1] Yan, K., Kou, L. and Zhang, D., 2018. Learning domain-invariant subspace using domain features and
            independence maximization. IEEE transactions on cybernetics, 48(1), pp.288-299.
    """

    def __init__(
        self,
        n_components,
        kernel="linear",
        lambda_=1.0,
        mu=1.0,
        eta=1.0,
        augmentation=False,
        kernel_params=None,
    ):
        self.n_components = n_components
        self.kernel = kernel
        self.mu = mu
        self.eta = eta
        self.augmentation = augmentation
        if kernel_params is None:
            self.kernel_params = {}
        else:
            self.kernel_params = kernel_params
        self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)
        self._centerer = KernelCenterer()
        self.x_fit = None

    def _get_kernel(self, x, y=None):
        if self.kernel in ["linear", "rbf", "poly"]:
            params = self.kernel_params or {}
        else:
            raise ValueError("Pre-computed kernel not supported")
        return pairwise_kernels(x, y, metric=self.kernel, filter_params=True, **params)

    def fit(self, x, y=None, covariates=None):
        """
        Args:
            x : array-like. Input data, shape (n_samples, n_features)
            y : array-like. Labels, shape (nl_samples,)
            covariates : array-like. Domain co-variates, shape (n_samples, n_co-variates)

        Note:
            Unsupervised MIDA is performed if y is None.
            Semi-supervised MIDA is performed is y is not None.
        """
        if self.augmentation and isinstance(covariates, np.ndarray):
            x = np.concatenate((x, covariates), axis=1)

        # Kernel matrix
        kernel_x = self._get_kernel(x)
        kernel_x[np.isnan(kernel_x)] = 0

        # Solve the optimization problem
        self._fit(kernel_x, y, covariates)
        self.x_fit = x

        return self

    def _fit(self, kernel_x, y, covariates=None):
        """solve MIDA

        Args:
            kernel_x: array-like, kernel matrix of input data x, shape (n_samples, n_samples)
            y: array-like. Labels, shape (nl_samples,)
            covariates: array-like. Domain co-variates, shape (n_samples, n_covariates)

        Returns:
            self
        """
        n_samples = kernel_x.shape[0]
        # Identity (unit) matrix
        unit_mat = np.eye(n_samples)
        # Centering matrix
        ctr_mat = unit_mat - 1.0 / n_samples * np.ones((n_samples, n_samples))

        kernel_x = self._centerer.fit_transform(kernel_x)
        if isinstance(covariates, np.ndarray):
            kernel_c = np.dot(covariates, covariates.T)
        else:
            kernel_c = np.zeros((n_samples, n_samples))

        if y is not None:
            n_labeled = y.shape[0]
            if n_labeled > n_samples:
                raise ValueError("Number of labels exceeds number of samples")
            y_mat_ = self._label_binarizer.fit_transform(y)
            y_mat = np.zeros((n_samples, y_mat_.shape[1]))
            y_mat[:n_labeled, :] = y_mat_
            ker_y = np.dot(y_mat, y_mat.T)
            obj = multi_dot(
                [
                    kernel_x,
                    self.mu * ctr_mat
                    + self.eta * multi_dot([ctr_mat, ker_y, ctr_mat])
                    - multi_dot([ctr_mat, kernel_c, ctr_mat]),
                    kernel_x.T,
                ]
            )
        else:
            obj = multi_dot([kernel_x, self.mu * ctr_mat - multi_dot([ctr_mat, kernel_c, ctr_mat]), kernel_x.T])

        eig_values, eig_vectors = linalg.eigh(obj, subset_by_index=[n_samples - self.n_components, n_samples - 1])
        idx_sorted = eig_values.argsort()[::-1]

        self.eig_values_ = eig_values[idx_sorted]
        self.U = eig_vectors[:, idx_sorted]
        self.U = np.asarray(self.U, dtype=np.float64)

        return self

    def fit_transform(self, x, y=None, covariates=None):
        """
        Args:
            x : array-like, shape (n_samples, n_features)
            y : array-like, shape (n_samples,)
            covariates : array-like, shape (n_samples, n_covariates)

        Returns:
            x_transformed : array-like, shape (n_samples, n_components)
        """
        self.fit(x, y, covariates)

        return self.transform(x, covariates)

    def transform(self, x, covariates=None):
        """
        Args:
            x : array-like, shape (n_samples, n_features)
            covariates : array-like, augmentation features, shape (n_samples, n_covariates)
        Returns:
            x_transformed : array-like, shape (n_samples, n_components)
        """
        check_is_fitted(self, "x_fit")
        if isinstance(covariates, np.ndarray) and self.augmentation:
            x = np.concatenate((x, covariates), axis=1)
        kernel_x = self._centerer.transform(
            pairwise_kernels(x, self.x_fit, metric=self.kernel, filter_params=True, **self.kernel_params)
        )

        return np.dot(kernel_x, self.U)
</file>

<file path="kale/embed/feature_fusion.py">
"""This module implements three different multimodal fusion methods:
1. Concat
2. BimodalInteractionFusion
3. LowRankTensorFusion
Each of these fusion methods are designed to work with input modalities as PyTorch tensors and perform different
operations to combine and create a joint representation of the input data.
Reference: https://github.com/pliang279/MultiBench/blob/main/fusions/common_fusions.py
"""

import torch
from torch import nn
from torch.autograd import Variable


class Concat(nn.Module):
    """Concat is a simple PyTorch module for fusing multimodal data by concatenating tensors along dimension 1.
    This fusion method is often used in multimodal learning where data from different modalities (e.g., image, audio)
    are processed separately and then fused together for further processing or decision-making. Each modality data is
    first flattened from its second dimension onward and then these flattened tensors are concatenated together.
    This approach to fusion maintains the independence of the modalities before the fusion point, allowing the network
    to learn separate representations for each modality before combining them.
    """

    def __init__(self):
        super(Concat, self).__init__()

    def forward(self, modalities):
        flattened = []
        for modality in modalities:
            flattened.append(torch.flatten(modality, start_dim=1))
        return torch.cat(flattened, dim=1)


class BimodalInteractionFusion(nn.Module):
    """BimodalInteractionFusion is a PyTorch module that performs fusion of two data modalities through a
    hypernetwork-based interaction mechanism. The 'input_dims' argument specifies the input dimensions of the two
    modalities. The 'output_dim' argument specifies the output dimension after the fusion. The 'output' argument
    defines the type of bimodal matrix interactions to be performed, which can be 'matrix', 'vector', or 'scalar'.
        This fusion method  supports three types of bimodal interactions:
            - Matrix: It implements a general hypernetwork mechanism where the interaction is multiplicative. It uses
            separate weight matrices and biases for the two modalities.
            - Vector: It uses diagonal forms and gating mechanisms, applying element-wise multiplication to combine the
            modalities.
            - Scalar: It applies scales and biases to the input modalities before combining them.
        This fusion method uses xavier normal distribution for initializing the weight matrices and normal distribution
        for the biases. It also provides options to clip the parameter values and their gradients within specified
        ranges to prevent them from exploding or vanishing.
        This fusion approach allows for complex interactions between the modalities and is well-suited for tasks that
        require the integration of heterogeneous data.
    Args:
        input_dims (int): list or tuple of 2 integers indicating input dimensions of the 2 modalities
        output_dim (int): output dimension after the fusion
        output (str): type of BimodalMatrix Interactions, options from 'matrix','vector','scalar'
        flatten (bool): whether we need to flatten the input modalities
        clip (tuple, optional): clip parameter values, None if no clip
        grad_clip (tuple, optional): clip grad values, None if no clip
        flip (bool): whether to swap the two input modalities in forward function or not
    """

    def __init__(self, input_dims, output_dim, output, flatten=False, clip=None, grad_clip=None, flip=False):
        super(BimodalInteractionFusion, self).__init__()
        self.input_dims = input_dims
        self.clip = clip
        self.output_dim = output_dim
        self.output = output
        self.flatten = flatten
        if output == "matrix":
            self.W = nn.Parameter(torch.Tensor(input_dims[0], input_dims[1], output_dim))
            nn.init.xavier_normal_(self.W)
            self.U = nn.Parameter(torch.Tensor(input_dims[0], output_dim))
            nn.init.xavier_normal_(self.U)
            self.V = nn.Parameter(torch.Tensor(input_dims[1], output_dim))
            nn.init.xavier_normal_(self.V)
            self.b = nn.Parameter(torch.Tensor(output_dim))
            nn.init.normal_(self.b)
        elif output == "vector":
            self.W = nn.Parameter(torch.Tensor(input_dims[0], input_dims[1]))
            nn.init.xavier_normal_(self.W)
            self.U = nn.Parameter(torch.Tensor(self.input_dims[0], self.input_dims[1]))
            nn.init.xavier_normal_(self.U)
            self.V = nn.Parameter(torch.Tensor(self.input_dims[1]))
            nn.init.normal_(self.V)
            self.b = nn.Parameter(torch.Tensor(self.input_dims[1]))
            nn.init.normal_(self.b)
        elif output == "scalar":
            self.W = nn.Parameter(torch.Tensor(input_dims[0]))
            nn.init.normal_(self.W)
            self.U = nn.Parameter(torch.Tensor(input_dims[0]))
            nn.init.normal_(self.U)
            self.V = nn.Parameter(torch.Tensor(1))
            nn.init.normal_(self.V)
            self.b = nn.Parameter(torch.Tensor(1))
            nn.init.normal_(self.b)
        self.flip = flip
        if grad_clip is not None:
            for p in self.parameters():
                p.register_hook(lambda grad: torch.clamp(grad, grad_clip[0], grad_clip[1]))

    def _repeatHorizontally(self, tensor, dim):
        return tensor.repeat(dim).view(dim, -1).transpose(0, 1)

    def forward(self, modalities):
        if len(modalities) == 1:
            return modalities[0]
        elif len(modalities) > 2:
            assert False
        m1 = modalities[0]
        m2 = modalities[1]
        if self.flip:
            m1 = modalities[1]
            m2 = modalities[0]

        if self.flatten:
            m1 = torch.flatten(m1, start_dim=1)
            m2 = torch.flatten(m2, start_dim=1)
        if self.clip is not None:
            m1 = torch.clip(m1, self.clip[0], self.clip[1])
            m2 = torch.clip(m2, self.clip[0], self.clip[1])

        if self.output == "matrix":
            Wprime = torch.einsum("bn, nmd -> bmd", m1, self.W) + self.V  # bmd
            bprime = torch.matmul(m1, self.U) + self.b  # bmd
            output = torch.einsum("bm, bmd -> bd", m2, Wprime) + bprime  # bmd

        elif self.output == "vector":
            Wprime = torch.matmul(m1, self.W) + self.V  # bm
            bprime = torch.matmul(m1, self.U) + self.b  # b
            output = Wprime * m2 + bprime  # bm

        elif self.output == "scalar":
            Wprime = torch.matmul(m1, self.W.unsqueeze(1)).squeeze(1) + self.V
            bprime = torch.matmul(m1, self.U.unsqueeze(1)).squeeze(1) + self.b
            output = self._repeatHorizontally(Wprime, self.input_dims[1]) * m2 + self._repeatHorizontally(
                bprime, self.input_dims[1]
            )
        return output


class LowRankTensorFusion(nn.Module):
    """LowRankTensorFusion is a PyTorch module that performs multimodal fusion using a low-rank tensor-based approach.
       The 'input_dims' argument specifies the input dimensions of each modality. The 'output_dim' argument defines the
       output dimension after the fusion. The 'rank' argument is a hyperparameter specifying the rank for the low-rank
       tensor decomposition.
       This fusion method performs fusion by assuming a low-rank structure for the interaction tensor, effectively
       compressing the interaction space. It leverages a set of low-rank factors, one for each modality, that are
       learned during training.
       These factors are initialized with xavier normal distribution and are applied to their corresponding modalities
       during the forward pass. A tensor product is computed across all modalities and their respective factors,
       resulting in a fused tensor.
       Next, a weighted summation of this fused tensor is computed using fusion weights, followed by the addition of a
       fusion bias. Both fusion weights and bias are learnable parameters initialized with xavier normal distribution
       and zero respectively.
       The final output is reshaped to the specified 'output_dim' and returned. If 'flatten' is set to True, each
       modality is first flattened before concatenation with a ones tensor and the subsequent multiplication with its
       factor.
       This approach provides an efficient and compact representation for capturing interactions among multiple
       modalities, making it suitable for tasks involving high-dimensional multimodal data.
    Args:
        input_dims (int): A list or tuple of integers indicating input dimensions of the modalities.
        output_dim (int): output dimension after the fusion.
        rank (int): A hyperparameter specifying the rank for the low-rank tensor decomposition.
        flatten (bool): Boolean to dictate if output should be flattened or not. Default: True

    Note:
        Adapted from https://github.com/Justin1904/Low-rank-Multimodal-Fusion.
    """

    def __init__(self, input_dims, output_dim, rank, flatten=True):
        super(LowRankTensorFusion, self).__init__()

        self.input_dims = input_dims
        self.output_dim = output_dim
        self.rank = rank
        self.flatten = flatten

        self.factors = []
        for input_dim in input_dims:
            factor = nn.Parameter(torch.Tensor(self.rank, input_dim + 1, self.output_dim)).to(
                torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            )
            nn.init.xavier_normal_(factor)
            self.factors.append(factor)

        self.fusion_weights = nn.Parameter(torch.Tensor(1, self.rank)).to(
            torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        )
        self.fusion_bias = nn.Parameter(torch.Tensor(1, self.output_dim)).to(
            torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        )
        nn.init.xavier_normal_(self.fusion_weights)
        self.fusion_bias.data.fill_(0)

    def forward(self, modalities):
        batch_size = modalities[0].shape[0]
        fused_tensor = 1
        for modality, factor in zip(modalities, self.factors):
            ones = Variable(torch.ones(batch_size, 1).type(modality.dtype), requires_grad=False).to(
                torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            )
            modality = modality.to(torch.device("cuda:0" if torch.cuda.is_available() else "cpu"))

            factor = factor.to(torch.device("cuda:0" if torch.cuda.is_available() else "cpu"))

            if self.flatten:
                modality_withones = torch.cat((ones, torch.flatten(modality, start_dim=1)), dim=1)
            else:
                modality_withones = torch.cat((ones, modality), dim=1)
            modality_factor = torch.matmul(modality_withones, factor)
            fused_tensor = fused_tensor * modality_factor

        output = torch.matmul(self.fusion_weights, fused_tensor.permute(1, 0, 2)).squeeze() + self.fusion_bias
        output = output.view(-1, self.output_dim)
        return output
</file>

<file path="kale/embed/gcn.py">
import numpy as np
import torch
from torch.nn import Parameter
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import add_remaining_self_loops
from torch_scatter import scatter_add


# Copy-paste with slight modification from torch_geometric.nn.GCNConv
class GCNEncoderLayer(MessagePassing):
    r"""
    Modification of PyTorch Geometirc's nn.GCNConv, which reduces the computational cost of GCN layer for
    `GripNet <https://github.com/NYXFLOWER/GripNet>`_ model.
    The graph convolutional operator from the `"Semi-supervised Classification with Graph Convolutional Networks"
    <https://arxiv.org/abs/1609.02907>`_ (ICLR 2017) paper.

    .. math::
        \mathbf{X}^{\prime} = \mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}
        \mathbf{\hat{D}}^{-1/2} \mathbf{X} \mathbf{\Theta},

    where :math:`\mathbf{\hat{A}} = \mathbf{A} + \mathbf{I}` denotes the
    adjacency matrix with inserted self-loops and
    :math:`\hat{D}_{ii} = \sum_{j=0} \hat{A}_{ij}` its diagonal degree matrix.

    Note: For more information please see Pytorch Geomertic's `nn.GCNConv
    <https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#module-torch_geometric.nn.conv.message_passing>`_ docs.

    Args:
        in_channels (int): Size of each input sample.
        out_channels (int): Size of each output sample.
        improved (bool, optional): If set to :obj:`True`, the layer computes
            :math:`\mathbf{\hat{A}}` as :math:`\mathbf{A} + 2\mathbf{I}`.
            (default: :obj:`False`)
        cached (bool, optional): If set to :obj:`True`, the layer will cache
            the computation of :math:`\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}
            \mathbf{\hat{D}}^{-1/2}` on first execution, and will use the
            cached version for further executions.
            This parameter should only be set to :obj:`True` in transductive
            learning scenarios. (default: :obj:`False`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """

    def __init__(self, in_channels, out_channels, improved=False, cached=False, bias=True, **kwargs):
        super(GCNEncoderLayer, self).__init__(aggr="add", **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.improved = improved
        self.cached = cached
        self.cached_result = None

        self.weight = Parameter(torch.Tensor(in_channels, out_channels))

        if bias:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        stdv = np.sqrt(6.0 / (self.weight.size(-2) + self.weight.size(-1)))
        self.weight.data.uniform_(-stdv, stdv)

        if self.bias is not None:
            self.bias.data.fill_(0)

        self.cached_result = None
        self.cached_num_edges = None

    @staticmethod
    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):
        """
        Add self-loops and apply symmetric normalization
        """
        if edge_weight is None:
            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)

        fill_value = 1 if not improved else 2
        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, fill_value, num_nodes)

        row, col = edge_index
        deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float("inf")] = 0

        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]

    def forward(self, x, edge_index, edge_weight=None):
        """
        Args:
            x (torch.Tensor): The input node feature embedding.
            edge_index (torch.Tensor): Graph edge index in COO format with shape [2, num_edges].
            edge_weight (torch.Tensor, optional): The one-dimensional relation weight for each edge in
                :obj:`edge_index` (default: None).
        """
        x = torch.matmul(x, self.weight)

        if self.cached and self.cached_result is not None:
            if edge_index.size(1) != self.cached_num_edges:
                raise RuntimeError(
                    "Cached {} number of edges, but found {}".format(self.cached_num_edges, edge_index.size(1))
                )

        if not self.cached or self.cached_result is None:
            self.cached_num_edges = edge_index.size(1)
            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, self.improved, x.dtype)
            self.cached_result = edge_index, norm

        edge_index, norm = self.cached_result

        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j, norm):
        return norm.view(-1, 1) * x_j

    def update(self, aggr_out):
        if self.bias is not None:
            aggr_out = aggr_out + self.bias
        return aggr_out

    def __repr__(self):
        return "{}({}, {})".format(self.__class__.__name__, self.in_channels, self.out_channels)


# Copy-paste with slight modification from torch_geometric.nn.RGCNConv
class RGCNEncoderLayer(MessagePassing):
    r"""
    Modification of PyTorch Geometirc's nn.RGCNConv, which reduces the computational and memory
    cost of RGCN encoder layer for `GripNet <https://github.com/NYXFLOWER/GripNet>`_ model.
    The relational graph convolutional operator from the `"Modeling
    Relational Data with Graph Convolutional Networks" <https://arxiv.org/abs/1703.06103>`_ paper.

    .. math::
        \mathbf{x}^{\prime}_i = \mathbf{\Theta}_{\textrm{root}} \cdot
        \mathbf{x}_i + \sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_r(i)}
        \frac{1}{|\mathcal{N}_r(i)|} \mathbf{\Theta}_r \cdot \mathbf{x}_j,

    where :math:`\mathcal{R}` denotes the set of relations, *i.e.* edge types.
    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which
    stores a relation identifier
    :math:`\in \{ 0, \ldots, |\mathcal{R}| - 1\}` for each edge.

    Note: For more information please see Pytorch Geomertic‚Äôs `nn.RGCNConv
    <https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#module-torch_geometric.nn.conv.message_passing>`_ docs.

    Args:
        in_channels (int): Size of each input sample.
        out_channels (int): Size of each output sample.
        num_relations (int): Number of edge relations.
        num_bases (int): Use bases-decoposition regulatization scheme and num_bases denotes the number of bases.
        after_relu (bool): Whether input embedding is activated by relu function or not.
        bias (bool): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`False`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """

    def __init__(self, in_channels, out_channels, num_relations, num_bases, after_relu, bias=False, **kwargs):
        super(RGCNEncoderLayer, self).__init__(aggr="mean", **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_relations = num_relations
        self.num_bases = num_bases
        self.after_relu = after_relu

        self.basis = Parameter(torch.Tensor(num_bases, in_channels, out_channels))
        self.att = Parameter(torch.Tensor(num_relations, num_bases))
        self.root = Parameter(torch.Tensor(in_channels, out_channels))

        if bias:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        self.att.data.normal_(std=1 / np.sqrt(self.num_bases))

        if self.after_relu:
            self.root.data.normal_(std=2 / self.in_channels)
            self.basis.data.normal_(std=2 / self.in_channels)

        else:
            self.root.data.normal_(std=1 / np.sqrt(self.in_channels))
            self.basis.data.normal_(std=1 / np.sqrt(self.in_channels))

        if self.bias is not None:
            self.bias.data.zero_()

    def forward(self, x, edge_index, edge_type, range_list):
        """
        Args:
            x (torch.Tensor): The input node feature embedding.
            edge_index (torch.Tensor): Graph edge index in COO format with shape [2, num_edges].
            edge_type (torch.Tensor): The one-dimensional relation type/index for each edge in
                :obj:`edge_index`.
            range_list (torch.Tensor): The index range list of each edge type with shape [num_types, 2].
        """
        return self.propagate(edge_index, x=x, edge_type=edge_type, range_list=range_list)

    def message(self, x_j, edge_index, edge_type, range_list):
        w = torch.matmul(self.att, self.basis.view(self.num_bases, -1))
        w = w.view(self.num_relations, self.in_channels, self.out_channels)
        # w = w[edge_type, :, :]
        # out = torch.bmm(x_j.unsqueeze(1), w).squeeze(-2)

        out_list = []
        for et in range(range_list.shape[0]):
            start, end = range_list[et]

            tmp = torch.matmul(x_j[start:end, :], w[et])

            # xxx = x_j[start: end, :]
            # tmp = checkpoint(torch.matmul, xxx, w[et])

            out_list.append(tmp)

        # TODO: test this
        return torch.cat(out_list)

    def update(self, aggr_out, x):
        out = aggr_out + torch.matmul(x, self.root)

        if self.bias is not None:
            out = out + self.bias
        return out

    def __repr__(self):
        return "{}({}, {}, num_relations={})".format(
            self.__class__.__name__, self.in_channels, self.out_channels, self.num_relations
        )
</file>

<file path="kale/embed/gripnet.py">
"""
The GripNet proposed in the `"GripNet: Graph Information Propagation on Supergraph for Heterogeneous Graphs"
    <https://doi.org/10.1016/j.patcog.2022.108973>`_ (PatternRecognit 2022) paper, which is an efficient
    framework to learn node representations on heterogeneous graphs for the downstream link prediction,
    node classification, and visualization. The code is based on the https://github.com/NYXFLOWER/GripNet.
"""

import logging
from typing import Dict

import torch
import torch.nn.functional as F
from torch.nn import Module

from kale.embed.gcn import GCNEncoderLayer, RGCNEncoderLayer
from kale.prepdata.supergraph_construct import SuperGraph, SuperVertex, SuperVertexParaSetting


class GripNetInternalModule(Module):
    r"""
    The internal module of a supervertex, which is composed of an internal feature layer and multiple internal
    aggregation layers.

    Args:
        in_channels (int): the dimension of node features on this supervertex.
        num_edge_type (int): the number of edge types on this supervertex.
        start_supervertex (bool): whether this supervertex is a start supervertex on the supergraph.
        setting (SuperVertexParaSetting): supervertex parameter settings.
    """

    def __init__(
        self, in_channels: int, num_edge_type: int, start_supervertex: bool, setting: SuperVertexParaSetting
    ) -> None:
        super(GripNetInternalModule, self).__init__()
        # in and out dimension
        self.in_channels = in_channels
        self.out_channels = setting.inter_agg_channels_list[-1]

        self.num_edge_type = num_edge_type
        self.multirelational = 1 if num_edge_type > 1 else 0
        self.start_supervertex = start_supervertex
        self.setting = setting

        self.__init_inter_feat_layer__()
        self.__init_inter_agg_layer__()

    def __init_inter_feat_layer__(self):
        """internal feature layer"""

        self.embedding = torch.nn.Parameter(torch.Tensor(self.in_channels, self.setting.inter_feat_channels))
        self.embedding.requires_grad = True

        # reset parameters to be normally distributed
        self.embedding.data.normal_()

    def __init_inter_agg_layer__(self):
        """internal aggregation layers"""

        # compute the dim of input of the first internal aggregation layer
        self.in_agg_channels = self.setting.inter_feat_channels
        if not self.start_supervertex:
            if self.setting.mode == "cat":
                if not self.setting.exter_agg_channels_dict:
                    error_msg = "`exter_agg_channels_dict` is not set."
                    logging.error(error_msg)
                    raise ValueError(error_msg)
                self.in_agg_channels += sum(self.setting.exter_agg_channels_dict.values())
            elif self.setting.mode == "add":
                tmp = set([self.in_agg_channels] + list(self.setting.exter_agg_channels_dict.values()))
                if len(tmp) != 1:
                    error_msg = "`in_agg_channels` should be the same as any element in `exter_agg_channels_dict`."
                    logging.error(error_msg)
                    raise ValueError(error_msg)
            else:
                error_msg = "`mode` value is invalid. Use 'cat' or 'add'."
                logging.error(error_msg)
                raise ValueError(error_msg)

        # create and initialize the internal aggregation layers
        self.num_inter_agg_layer = len(self.setting.inter_agg_channels_list)
        tmp_dim = [self.in_agg_channels] + self.setting.inter_agg_channels_list

        if self.setting.concat_output:
            self.out_channels = sum(tmp_dim)

        if self.multirelational:
            # using RGCN if there are multiple edge types
            after_relu = [False if i == 0 else True for i in range(self.num_inter_agg_layer)]
            self.inter_agg_layers = torch.nn.ModuleList(
                [
                    RGCNEncoderLayer(
                        tmp_dim[i], tmp_dim[i + 1], self.num_edge_type, self.setting.num_bases, after_relu[i]
                    )
                    for i in range(self.num_inter_agg_layer)
                ]
            )
        else:
            # using GCN if there is only one edge type
            self.inter_agg_layers = torch.nn.ModuleList(
                [GCNEncoderLayer(tmp_dim[i], tmp_dim[i + 1], cached=True) for i in range(self.num_inter_agg_layer)]
            )

    def forward(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        edge_type: torch.Tensor = None,
        range_list: torch.Tensor = None,
        edge_weight: torch.Tensor = None,
    ) -> torch.Tensor:
        r"""
        Args:
            x (torch.Tensor): the input node feature embedding.
            edge_index (torch.Tensor): edge index in COO format with shape [2, #edges].
            edge_type (torch.Tensor, optional): one-dimensional relation type for each edge, indexed from 0.
                Defaults to None.
            range_list (torch.Tensor, optional): The index range list of each edge type with shape [num_types, 2].
                Defaults to None.
            edge_weight (torch.Tensor, optional): one-dimensional weight for each edge. Defaults to None.

        Note: The internal feature layer is computed in the `forward` function of GripNet class. If the supervertex
        is not a start supervertex, `x` should be the sum or concat of the outputs of the internal feature
        layer and all external aggregation layers.
        """

        if self.setting.concat_output:
            tmp = []
            tmp.append(x)

        if self.multirelational:
            if edge_type is None or range_list is None:
                error_msg = "`edge_type` and `range_list` are not set."
                logging.error(error_msg)
                raise ValueError(error_msg)

        # internal feature aggregation layers
        for net in self.inter_agg_layers[:-1]:
            x = net(x, edge_index, edge_type, range_list) if self.multirelational else net(x, edge_index, edge_weight)
            x = F.relu(x, inplace=True)
            if self.setting.concat_output:
                tmp.append(x)

        x = (
            self.inter_agg_layers[-1](x, edge_index, edge_type, range_list)
            if self.multirelational
            else self.inter_agg_layers[-1](x, edge_index, edge_weight)
        )

        x = F.relu(x, inplace=True)
        if self.setting.concat_output:
            tmp.append(x)
            x = torch.cat(tmp, dim=1)

        return x

    def __repr__(self):
        tmp = [f"\n    ({i}): {l}" for i, l in enumerate(self.inter_agg_layers)]

        return (
            "{}: ModuleList(\n  (0): InternalFeatureLayer: Embedding({}, {})\n  "
            "(1): InternalFeatureAggregationModule: ModuleList({}\n  )\n)"
        ).format(self.__class__.__name__, self.in_channels, self.setting.inter_feat_channels, "".join(tmp))


class GripNetExternalModule(Module):
    """The internal module of a supervertex, which is an external feature layer.

    Args:
        in_channels (int): Size of each input sample. In GripNet, it should be the dimension of the output embedding of
        the corresponding parent supervertex.
        out_channels (int): Size of each output sample. In GripNet, it is the dimension of the output embedding of
            the supervertex.
        num_out_node (int): the number of output nodes.
    """

    def __init__(self, in_channels: int, out_channels: int, num_out_node: int) -> None:
        super(GripNetExternalModule, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_out_node = num_out_node

        self.exter_agg_layer = GCNEncoderLayer(in_channels, out_channels, cached=True)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_weight: torch.Tensor = None, use_relu=True):
        """
        Args:
            x (torch.Tensor): the input node feature embedding.
            edge_index (torch.Tensor): edge index in COO format with shape [2, #edges].
            edge_weight (torch.Tensor, optional): one-dimensional weight for each edge. Defaults to None.
            use_relu (bool, optional): whether to use ReLU before returning node feature embeddings. Defaults to True.
        """

        n_source, n_feat = x.shape
        bigraph_edge_index = edge_index + 0
        bigraph_edge_index[1, :] += n_source

        x = torch.cat([x, torch.zeros((self.num_out_node, n_feat)).to(x.device)], dim=0)
        x = self.exter_agg_layer(x, bigraph_edge_index, edge_weight)[n_source:, :]

        if use_relu:
            x = F.relu(x, inplace=True)

        return x

    def __repr__(self):
        return "{}: {}".format(self.__class__.__name__, self.exter_agg_layer.__repr__())


class GripNet(Module):
    r"""The GripNet model.

    Args:
        supergraph (SuperGraph): the supergraph.

    Reference:
        Xu, H., Sang, S., Bai, P., Li, R., Yang, L. and Lu, H., 2022. GripNet: Graph Information
        Propagation on Supergraph for Heterogeneous Graphs. Pattern Recognition, p.108973.
    """

    def __init__(self, supergraph: SuperGraph) -> None:
        super(GripNet, self).__init__()

        self.supergraph = supergraph
        self.task_supervertex_name = supergraph.topological_order[-1]

        self.out_embed_dict: Dict[str, torch.Tensor] = {}

        self.__check_supergraph__()
        self.__init_supervertex_module_dict__()

        self.out_channels = self.__get_out_channels__()

    def __check_supergraph__(self) -> None:
        """check whether the input supergraph has parameter settings"""

        if self.supergraph.supervertex_setting_dict is None:
            error_msg = "`supervertex_setting_dict` is not set."
            logging.error(error_msg)
            raise ValueError(error_msg)

    def __init_supervertex_module_dict__(self):
        self.supervertex_module_dict: Dict[str, torch.nn.ModuleList] = {}
        for supervertex_name in self.supergraph.topological_order:
            supervertex = self.supergraph.supervertex_dict[supervertex_name]
            setting = self.supergraph.supervertex_setting_dict[supervertex_name]
            self.__init_module_supervertex__(supervertex, setting)

    def __init_module_supervertex__(self, supervertex: SuperVertex, setting: SuperVertexParaSetting):
        module_list = torch.nn.ModuleList()

        if not supervertex.start_supervertex:
            # add the external modules from all parent supervertices
            for in_name in supervertex.in_supervertex_list:
                in_channels = self.supervertex_module_dict[in_name][-1].out_channels

                if setting.exter_agg_channels_dict is None:
                    error_msg = "`exter_agg_channels_dict` is not set."
                    logging.error(error_msg)
                    raise ValueError(error_msg)

                out_channels = setting.exter_agg_channels_dict[in_name]
                module_list.append(GripNetExternalModule(in_channels, out_channels, supervertex.num_node))

        # add the internal module
        module_list.append(
            GripNetInternalModule(
                supervertex.num_node_feat, supervertex.num_edge_type, supervertex.start_supervertex, setting
            )
        )

        self.supervertex_module_dict[supervertex.name] = module_list

    def __get_out_channels__(self):
        task_supervertex = self.supergraph.topological_order[-1]
        return self.supervertex_module_dict[task_supervertex][-1].out_channels

    def forward(self):
        if self.supergraph.supervertex_setting_dict is None:
            error_msg = "`supervertex_setting_dict` is not set."
            logging.error(error_msg)
            raise ValueError(error_msg)

        for supervertex_name in self.supergraph.topological_order:
            mode = self.supergraph.supervertex_setting_dict[supervertex_name].mode
            self.__forward_supervertex__(supervertex_name, mode)

        return self.out_embed_dict[self.task_supervertex_name]

    def __forward_supervertex__(self, supervertex_name: str, mode: str):
        supervertex = self.supergraph.supervertex_dict[supervertex_name]
        model = self.supervertex_module_dict[supervertex_name]

        # internal feature layer
        x = torch.matmul(supervertex.node_feat, model[-1].embedding)

        # external feature aggregation layers
        if mode == "add":
            for idx in range(len(supervertex.in_supervertex_list)):
                parent_name = supervertex.in_supervertex_list[idx]
                parent_x = self.out_embed_dict[parent_name]
                superedge = self.supergraph.superedge_dict[(parent_name, supervertex_name)]

                x += model[idx](parent_x, superedge.edge_index, superedge.edge_weight)
        else:
            tmp = [x]
            for idx in range(len(supervertex.in_supervertex_list)):
                parent_name = supervertex.in_supervertex_list[idx]
                parent_x = self.out_embed_dict[parent_name]
                superedge = self.supergraph.superedge_dict[(parent_name, supervertex_name)]

                tmp.append(model[idx](parent_x, superedge.edge_index, superedge.edge_weight))
            x = torch.cat(tmp, dim=1)

        # internal feature aggregation layers
        if supervertex.num_edge_type > 1:
            x = model[-1](
                x, supervertex.edge_index, supervertex.edge_type, supervertex.range_list, supervertex.edge_weight
            )
        else:
            x = model[-1](x, supervertex.edge_index, supervertex.edge_weight)

        self.out_embed_dict[supervertex_name] = x

    def __repr__(self):
        return "{}: ModuleDict(\n{})".format(self.__class__.__name__, self.supervertex_module_dict)
</file>

<file path="kale/embed/image_cnn.py">
"""CNNs for extracting features from small images of size 32x32 (e.g. MNIST) and regular images of size 224x224 (e.g.
ImageNet). The code is based on https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/modules.py,
 which is for domain adaptation.
"""

import torch.nn as nn
from torch.nn import functional as F
from torchvision import models


class Flatten(nn.Module):
    """Flatten layer
    This module is to replace the last fc layer of the pre-trained model with a flatten layer. It flattens the input
    tensor to a 2D vector, which is (B, N). B is the batch size and N is the product of all dimensions except
    the batch size.

    Examples:
        >>> x = torch.randn(8, 3, 224, 224)
        >>> x = Flatten()(x)
        >>> print(x.shape)
        >>> (8, 150528)
    """

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        return x.view(x.size(0), -1)


class Identity(nn.Module):
    """Identity layer
    This module is to replace any unwanted layers in a pre-defined model with an identity layer.
    It returns the input tensor as the output.

    Examples:
        >>> x = torch.randn(8, 3, 224, 224)
        >>> x = Identity()(x)
        >>> print(x.shape)
        >>> (8, 3, 224, 224)
    """

    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x


# From FeatureExtractorDigits in adalib
class SmallCNNFeature(nn.Module):
    """
    A feature extractor for small 32x32 images (e.g. CIFAR, MNIST) that outputs a feature vector of length 128.

    Args:
        num_channels (int): the number of input channels (default=3).
        kernel_size (int): the size of the convolution kernel (default=5).

    Examples::
        >>> feature_network = SmallCNNFeature(num_channels)
    """

    def __init__(self, num_channels=3, kernel_size=5):
        super(SmallCNNFeature, self).__init__()
        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=kernel_size)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(64, 64, kernel_size=kernel_size)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(2)
        self.relu2 = nn.ReLU()
        self.conv3 = nn.Conv2d(64, 64 * 2, kernel_size=kernel_size)
        self.bn3 = nn.BatchNorm2d(64 * 2)
        self.sigmoid = nn.Sigmoid()
        self._out_features = 128

    def forward(self, input_):
        x = self.bn1(self.conv1(input_))
        x = self.relu1(self.pool1(x))
        x = self.bn2(self.conv2(x))
        x = self.relu2(self.pool2(x))
        x = self.sigmoid(self.bn3(self.conv3(x)))
        x = x.view(x.size(0), -1)
        return x

    def output_size(self):
        return self._out_features


class SimpleCNNBuilder(nn.Module):
    """A builder for simple CNNs to experiment with different basic architectures.

    Args:
        num_channels (int, optional): the number of input channels. Defaults to 3.
        conv_layers_spec (list): a list for each convolutional layer given as [num_channels, kernel_size].
            For example, [[16, 3], [16, 1]] represents 2 layers with 16 filters and kernel sizes of 3 and 1 respectively.
        activation_fun (str): a string specifying the activation function to use. one of ('relu', 'elu', 'leaky_relu').
            Defaults to "relu".
        use_batchnorm (boolean): a boolean flag indicating whether to use batch normalization. Defaults to True.
        pool_locations (tuple): the index after which pooling layers should be placed in the convolutional layer list.
            Defaults to (0,3). (0,3) means placing 2 pooling layers after the first and fourth convolutional layer.
        num_channels (int): the number of input channels. Defaults to 3.
    """

    activations = {"relu": nn.ReLU(), "elu": nn.ELU(), "leaky_relu": nn.LeakyReLU()}

    def __init__(
        self, conv_layers_spec, activation_fun="relu", use_batchnorm=True, pool_locations=(0, 3), num_channels=3
    ):
        super(SimpleCNNBuilder, self).__init__()
        self.layers = nn.ModuleList()
        in_channels = num_channels
        activation_fun = self.activations[activation_fun]

        # Repetitively adds a convolution, batch-norm, activation Function, and max-pooling layer.
        for layer_num, (num_kernels, kernel_size) in enumerate(conv_layers_spec):
            conv = nn.Conv2d(in_channels, num_kernels, kernel_size, stride=1, padding=(kernel_size - 1) // 2)
            self.layers.append(conv)

            if use_batchnorm:
                self.layers.append(nn.BatchNorm2d(num_kernels))

            self.layers.append(activation_fun)

            if layer_num in pool_locations:
                self.layers.append(nn.MaxPool2d(kernel_size=2))

            in_channels = num_kernels

    def forward(self, x):
        for block in self.layers:
            x = block(x)

        return x


class _Bottleneck(nn.Module):
    """Simple bottleneck as domain specific feature extractor, used in multi-source domain adaptation method MFSAN only.
    Compared to the torchvision implementation, it accepts both 1D and 2D input, and the value of expansion is
    flexible and an average pooling layer is added.

    The code is based on:
        https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L86,
        https://github.com/easezyc/deep-transfer-learning/blob/master/MUDA/MFSAN/MFSAN_2src/resnet.py#L94, and
        https://github.com/easezyc/deep-transfer-learning/blob/master/MUDA/MFSAN/MFSAN_3src/resnet.py#L93
    """

    def __init__(self, inplanes: int, planes: int, stride: int = 1, expansion: int = 1, input_dimension=2):
        super(_Bottleneck, self).__init__()
        self.input_dimension = input_dimension
        if input_dimension == 2:
            conv = nn.Conv2d
            bn = nn.BatchNorm2d
            avgpool = nn.AdaptiveAvgPool2d(1)
        else:
            conv = nn.Conv1d
            bn = nn.BatchNorm1d
            avgpool = nn.AdaptiveAvgPool1d(1)
        self.expansion = expansion
        self.conv1 = conv(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = bn(planes)
        self.conv2 = conv(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = bn(planes)
        self.conv3 = conv(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = bn(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.stride = stride
        self.avgpool = avgpool

    def forward(self, x):
        if self.input_dimension == 1 and len(x.shape) == 2:
            x = x.view(x.shape + (1,))
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)
        out = self.relu(out)

        out = self.avgpool(out)
        out = out.view(out.size(0), -1)

        return out


class ResNet18Feature(nn.Module):
    """
    Modified ResNet18 (without the last layer) feature extractor for regular 224x224 images.

    Args:
        weights (models.ResNet18_Weights or string): The pretrained weights to use. See
         https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights
        for more details. By default, ResNet18_Weights.DEFAULT will be used.
    """

    def __init__(self, weights=models.ResNet18_Weights.DEFAULT):
        super(ResNet18Feature, self).__init__()
        self.model = models.resnet18(weights=weights)
        self._out_features = self.model.fc.in_features
        self.model.fc = Flatten()

    def forward(self, x):
        return self.model(x)

    def output_size(self):
        return self._out_features


class ResNet34Feature(nn.Module):
    """
    Modified ResNet34 (without the last layer) feature extractor for regular 224x224 images.

    Args:
        weights (models.ResNet34_Weights or string): The pretrained weights to use. See
         https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet34.html#torchvision.models.ResNet34_Weights
        for more details. By default, ResNet34_Weights.DEFAULT will be used.
    """

    def __init__(self, weights=models.ResNet34_Weights.DEFAULT):
        super(ResNet34Feature, self).__init__()
        self.model = models.resnet34(weights=weights)
        self._out_features = self.model.fc.in_features
        self.model.fc = Flatten()

    def forward(self, x):
        return self.model(x)

    def output_size(self):
        return self._out_features


class ResNet50Feature(nn.Module):
    """
    Modified ResNet50 (without the last layer) feature extractor for regular 224x224 images.

    Args:
        weights (models.ResNet50_Weights or string): The pretrained weights to use. See
         https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#torchvision.models.ResNet50_Weights
        for more details. By default, ResNet50_Weights.DEFAULT will be used.
    """

    def __init__(self, weights=models.ResNet50_Weights.DEFAULT):
        super(ResNet50Feature, self).__init__()
        self.model = models.resnet50(weights=weights)
        self._out_features = self.model.fc.in_features
        self.model.fc = Flatten()

    def forward(self, x):
        return self.model(x)

    def output_size(self):
        return self._out_features


class ResNet101Feature(nn.Module):
    """
    Modified ResNet101 (without the last layer) feature extractor for regular 224x224 images.

    Args:
        weights (models.ResNet101_Weights or string): The pretrained weights to use. See
         https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet101.html#torchvision.models.ResNet101_Weights
        for more details. By default, ResNet101_Weights.DEFAULT will be used.
    """

    def __init__(self, weights=models.ResNet101_Weights.DEFAULT):
        super(ResNet101Feature, self).__init__()
        self.model = models.resnet101(weights=weights)
        self._out_features = self.model.fc.in_features
        self.model.fc = Flatten()

    def forward(self, x):
        return self.model(x)

    def output_size(self):
        return self._out_features


class ResNet152Feature(nn.Module):
    """
    Modified ResNet152 (without the last layer) feature extractor for regular 224x224 images.

    Args:
        weights (models.ResNet152_Weights or string): The pretrained weights to use. See
         https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights
        for more details. By default, ResNet152_Weights.DEFAULT will be used.
    """

    def __init__(self, weights=models.ResNet152_Weights.DEFAULT):
        super(ResNet152Feature, self).__init__()
        self.model = models.resnet152(weights=weights)
        self._out_features = self.model.fc.in_features
        self.model.fc = Flatten()

    def forward(self, x):
        return self.model(x)

    def output_size(self):
        return self._out_features


class LeNet(nn.Module):
    """LeNet is a customizable Convolutional Neural Network (CNN) model based on the LeNet architecture, designed for
    feature extraction from image and audio modalities.
       LeNet supports several layers of 2D convolution, followed by batch normalization, max pooling, and adaptive
       average pooling, with a configurable number of channels.
       The depth of the network (number of convolutional blocks) is adjustable with the 'additional_layers' parameter.
       An optional linear layer can be added at the end for further transformation of the output, which could be useful
       for various tasks such as classification or regression. The 'output_each_layer' option allows for returning the
       output of each layer instead of just the final output, which can be beneficial for certain tasks or for analyzing
       the intermediate representations learned by the network.
       By default, the output tensor is squeezed before being returned, removing dimensions of size one, but this can be
       configured with the 'squeeze_output' parameter.

    Args:
        input_channels (int): Input channel number.
        output_channels (int): Output channel number for block.
        additional_layers (int): Number of additional blocks for LeNet.
        output_each_layer (bool, optional): Whether to return the output of all layers. Defaults to False.
        linear (tuple, optional): Tuple of (input_dim, output_dim) for optional linear layer post-processing. Defaults to None.
        squeeze_output (bool, optional): Whether to squeeze output before returning. Defaults to True.

    Note:
        Adapted code from https://github.com/slyviacassell/_MFAS/blob/master/models/central/avmnist.py.
    """

    def __init__(
        self,
        input_channels,
        output_channels,
        additional_layers,
        output_each_layer=False,
        linear=None,
        squeeze_output=True,
    ):
        super(LeNet, self).__init__()
        self.output_each_layer = output_each_layer
        self.conv_layers = [nn.Conv2d(input_channels, output_channels, kernel_size=5, padding=2, bias=False)]
        self.batch_norms = [nn.BatchNorm2d(output_channels)]
        self.global_pools = [nn.AdaptiveAvgPool2d(1)]

        for i in range(additional_layers):
            self.conv_layers.append(
                nn.Conv2d(
                    (2**i) * output_channels, (2 ** (i + 1)) * output_channels, kernel_size=3, padding=1, bias=False
                )
            )
            self.batch_norms.append(nn.BatchNorm2d(output_channels * (2 ** (i + 1))))
            self.global_pools.append(nn.AdaptiveAvgPool2d(1))

        self.conv_layers = nn.ModuleList(self.conv_layers)
        self.batch_norms = nn.ModuleList(self.batch_norms)
        self.global_pools = nn.ModuleList(self.global_pools)
        self.squeeze_output = squeeze_output
        self.linear = None

        if linear is not None:
            self.linear = nn.Linear(linear[0], linear[1])

        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)):
                nn.init.kaiming_uniform_(m.weight)

    def forward(self, x):
        intermediate_outputs = []
        output = x
        for i in range(len(self.conv_layers)):
            output = F.relu(self.batch_norms[i](self.conv_layers[i](output)))
            output = F.max_pool2d(output, 2)
            global_pool = self.global_pools[i](output).view(output.size(0), -1)
            intermediate_outputs.append(global_pool)

        if self.linear is not None:
            output = self.linear(output)
        intermediate_outputs.append(output)

        if self.output_each_layer:
            if self.squeeze_output:
                return [t.squeeze() for t in intermediate_outputs]
            return intermediate_outputs

        if self.squeeze_output:
            return output.squeeze()
        return output
</file>

<file path="kale/embed/mogonet.py">
# =============================================================================
# Author: Sina Tabakhi, sina.tabakhi@gmail.com
# =============================================================================

"""
Construct a message passing network using PyTorch Geometric for the MOGONET method. MOGONET is a multiomics fusion
framework for cancer classification and biomarker identification that utilizes supervised graph convolutional networks
for omics datasets.

This code is written by refactoring the MOGONET code (https://github.com/txWang/MOGONET/blob/main/models.py) within
the 'MessagePassing' base class provided in the PyTorch Geometric.

Reference:
Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021). MOGONET integrates multi-omics data
using graph convolutional networks allowing patient classification and biomarker identification. Nature communications.
https://www.nature.com/articles/s41467-021-23774-w
"""

from typing import List, Optional, Union

import torch
import torch.nn.functional as F
import torch_sparse
from torch import Tensor
from torch.nn import Module, Parameter
from torch.nn.init import xavier_normal_
from torch_geometric.nn.aggr import Aggregation
from torch_geometric.nn.conv import MessagePassing
from torch_sparse import SparseTensor


class MogonetGCNConv(MessagePassing):
    r"""Create message passing layers for the MOGONET method. Each layer is defined as:

    .. math::
        H^{(l+1)}=f(H^{(l)}, A) = \sigma(AH^{(l)}W^{(l)})

    where :math:`\mathbf{H^{(l)}}` is the input of the :math:`l`-th layer and :math:`\mathbf{W^{(l)}}` is the weight
    matrix of the :math:`l`-th layer. :math:`\sigma(.)` denotes a non-linear activation function.

    For more information please refer to the MOGONET paper.

    Args:
        in_channels (int): Size of each input sample.
        out_channels (int): Size of each output sample.
        bias (bool, optional): If set to ``False``, the layer will not learn an additive bias. (default: ``True``)
        aggr (string or list or Aggregation, optional): The aggregation scheme to use,
            *e.g.*, ``"add"``, ``"sum"``, ``"mean"``, ``"min"``, ``"max"`` or ``"mul"``.
        **kwargs (optional): Additional arguments of :class:`torch_geometric.nn.conv.MessagePassing`.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        bias: bool = True,
        aggr: Optional[Union[str, List[str], Aggregation]] = "add",
        **kwargs,
    ) -> None:
        super().__init__(aggr=aggr, **kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.weight = Parameter(torch.Tensor(self.in_channels, self.out_channels))

        if bias:
            self.bias = Parameter(torch.Tensor(self.out_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self) -> None:
        """Reset all parameters of the model."""
        xavier_normal_(self.weight.data)

        if self.bias is not None:
            self.bias.data.fill_(0.0)

    def forward(self, x: Tensor, edge_index: SparseTensor) -> Tensor:
        # The format of edge_index is SparseTensor which performs fast sparse-matrix multiplication
        x = torch.mm(x, self.weight)
        out = self.propagate(edge_index, x=x)
        return out

    def message(self, x_j: Tensor) -> Tensor:
        r"""Construct messages from node :math:`j` to node :math:`i` for each edge in ``edge_index``."""
        return x_j

    def message_and_aggregate(self, adj_t: Union[SparseTensor, Tensor], x: Tensor) -> Tensor:
        r"""Fuse computations of :func:`message` and :func:`aggregate` into a single function."""
        return torch_sparse.matmul(adj_t, x, reduce=self.aggr)

    def update(self, aggr_out: Tensor) -> Tensor:
        r"""Update node embeddings for each node :math:`i \in \mathcal{V}`."""
        if self.bias is not None:
            aggr_out = aggr_out + self.bias
        return aggr_out


class MogonetGCN(Module):
    r"""Create the structure of the graph convolutional network in the MOGONET method.
    For more information please refer to the MOGONET paper.

    Args:
        in_channels (int): Size of each input sample.
        hidden_channels (List[int]): A list of sizes of hidden layers.
        dropout (float): Probability of an element to be zeroed.
    """

    def __init__(self, in_channels: int, hidden_channels: List[int], dropout: float) -> None:
        super().__init__()
        self.conv1 = MogonetGCNConv(in_channels, hidden_channels[0])
        self.conv2 = MogonetGCNConv(hidden_channels[0], hidden_channels[1])
        self.conv3 = MogonetGCNConv(hidden_channels[1], hidden_channels[2])
        self.dropout = dropout

    def forward(self, x: Tensor, edge_index: SparseTensor) -> Tensor:
        x = self.conv1(x, edge_index)
        x = F.leaky_relu(x, 0.25)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.leaky_relu(x, 0.25)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv3(x, edge_index)
        x = F.leaky_relu(x, 0.25)

        return x
</file>

<file path="kale/embed/positional_encoding.py">
# Created by Raivo Koot from modifying https://pytorch.org/tutorials/beginner/transformer_tutorial.html

import math

import torch
import torch.nn as nn


class PositionalEncoding(nn.Module):
    """
    Implements the positional encoding as described in the NIPS2017 paper
    'Attention Is All You Need' about Transformers
    (https://arxiv.org/abs/1706.03762).
    Essentially, for all timesteps in a given sequence,
    adds information about the relative temporal location of a timestep
    directly into the features of that timestep, and then returns this
    slightly-modified, same-shape sequence.

    args:
        d_model: the number of features that each timestep has (required).
        max_len: the maximum sequence length that the positional
                  encodings should support (required).
    """

    def __init__(self, d_model: int, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pe", pe)

        self.scaling_term = math.sqrt(d_model)
        self.max_len = max_len
        self.d_model = d_model

    def forward(self, x):
        """
        Expects input of shape (sequence_length, batch_size, num_features)
        and returns output of the same shape. sequence_length is at most
        allowed to be self.max_len and num_features is expected to
        be exactly self.d_model

        Args:
            x: a sequence input of shape (sequence_length, batch_size, num_features) (required).
        """
        x = x * self.scaling_term  # make embedding relatively larger than positional encoding
        x = x + self.pe[: x.size(0), :]
        return x
</file>

<file path="kale/embed/README.md">
# Embedding modules

Machine learning modules for representation learning, embedding, feature extraction, and feature selection.
</file>

<file path="kale/embed/seq_nn.py">
"""
DeepDTA based models for drug-target interaction prediction problem.
"""

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_max_pool


class CNNEncoder(nn.Module):
    r"""
    The DeepDTA's CNN encoder module, which comprises three 1D-convolutional layers and one max-pooling layer.
    The module is applied to encoding drug/target sequence information, and the input should be transformed information
    with integer/label encoding. The original paper is `"DeepDTA: deep drug‚Äìtarget binding affinity prediction"
    <https://academic.oup.com/bioinformatics/article/34/17/i821/5093245>`_ .

    Args:
        num_embeddings (int): Number of embedding labels/categories, depends on the types of encoding sequence.
        embedding_dim (int): Dimension of embedding labels/categories.
        sequence_length (int): Max length of input sequence.
        num_kernels (int): Number of kernels (filters).
        kernel_length (int): Length of kernel (filter).
    """

    def __init__(self, num_embeddings, embedding_dim, sequence_length, num_kernels, kernel_length):
        super(CNNEncoder, self).__init__()
        self.embedding = nn.Embedding(num_embeddings + 1, embedding_dim)
        self.conv1 = nn.Conv1d(in_channels=sequence_length, out_channels=num_kernels, kernel_size=kernel_length)
        self.conv2 = nn.Conv1d(in_channels=num_kernels, out_channels=num_kernels * 2, kernel_size=kernel_length)
        self.conv3 = nn.Conv1d(in_channels=num_kernels * 2, out_channels=num_kernels * 3, kernel_size=kernel_length)
        self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)

    def forward(self, x):
        x = self.embedding(x)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = self.global_max_pool(x)
        x = x.squeeze(2)
        return x


class GCNEncoder(nn.Module):
    r"""
    The GraphDTA's GCN encoder module, which comprises three graph convolutional layers and one full connected layer.
    The model is a variant of DeepDTA and is applied to encoding drug molecule graph information. The original paper
    is  `"GraphDTA: Predicting drug‚Äìtarget binding affinity with graph neural networks"
    <https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btaa921/5942970>`_ .

    Args:
        in_channel (int): Dimension of each input node feature.
        out_channel (int): Dimension of each output node feature.
        dropout_rate (float): dropout rate during training.
    """

    def __init__(self, in_channel=78, out_channel=128, dropout_rate=0.2):
        super(GCNEncoder, self).__init__()
        self.conv1 = GCNConv(in_channel, in_channel)
        self.conv2 = GCNConv(in_channel, in_channel * 2)
        self.conv3 = GCNConv(in_channel * 2, in_channel * 4)
        self.fc = nn.Linear(in_channel * 4, out_channel)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x, edge_index, batch):
        x = self.relu(self.conv1(x, edge_index))
        x = self.relu(self.conv2(x, edge_index))
        x = self.relu(self.conv3(x, edge_index))
        x = global_max_pool(x, batch)
        x = self.fc(x)
        x = self.dropout(x)
        return x
</file>

<file path="kale/embed/uncertainty_fitting.py">
"""
Module from the implementation of L. A. Schobs, A. J. Swift and H. Lu, "Uncertainty Estimation for Heatmap-Based Landmark Localization,"
in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034, April 2023, doi: 10.1109/TMI.2022.3222730.

Functions related to use the validation data to fit the uncertainty boundaries with error bounds.
Also bins the test data and saves the results.
"""

import logging
import os
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd
from yacs.config import CfgNode

from kale.interpret.uncertainty_quantiles import quantile_binning_and_est_errors
from kale.loaddata.tabular_access import load_csv_columns
from kale.predict.uncertainty_binning import quantile_binning_predictions
from kale.prepdata.tabular_transform import apply_confidence_inversion


def fit_and_predict(
    target_idx: int,
    uncertainty_error_pairs: List[List],
    ue_pairs_val: str,
    ue_pairs_test: str,
    num_bins: int,
    config: CfgNode,
    groundtruth_test_errors: bool,
    save_folder: Optional[str] = None,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Loads (validation, testing data) pairs of (uncertainty, error) pairs and for each fold. Uses the validation
    set to generate quantile thresholds. Uses Isotonic Regression with validation set to estimate error bounds.
    Then bins the test data accordingly. Saves predicted bins and error bounds to a csv.

    Args:
        target_idx (int): Index of target to perform uncertainty estimation on.
        uncertainty_error_pairs (list[list]): List of lists describing the different uncertainty combinations to test.
        ue_pairs_val (str): Path to validation pairs (uncertainty, error) data.
        ue_pairs_test (str): Path to test pairs (uncertainty, error) data.
        num_bins (int): Number of bins for quantile binning.
        config (CfgNode): Configuration object with hyperparameters and other settings.
        groundtruth_test_errors (bool): Whether ground truth errors are available for test data.
        save_folder (str, optional): Path to folder to save results to. If None, results are not saved.

    Returns:
        all_uncert_boundaries (pd.DataFrame): A DataFrame with uncertainty boundaries for each fold and uncertainty pairing.
        error_bound_estimates (pd.DataFrame): A DataFrame with estimated error bounds for each fold and uncertainty pairing.
        all_testing_results (pd.DataFrame): A DataFrame with predicted test bin values for each fold and uncertainty pairing.
    """

    logger = logging.getLogger("q_bin")
    invert_confidences = config["DATASET"]["CONFIDENCE_INVERT"]
    num_folds = config["DATASET"]["NUM_FOLDS"]
    combine_middle_bins = config["PIPELINE"]["COMBINE_MIDDLE_BINS"]

    # Save results across uncertainty pairings for each target.
    all_testing_results = pd.DataFrame(load_csv_columns(ue_pairs_test, "Testing Fold", np.arange(num_folds)))
    error_bound_estimates = pd.DataFrame({"fold": np.arange(num_folds)})
    all_uncert_boundaries = pd.DataFrame({"fold": np.arange(num_folds)})

    for idx, uncertainty_pairing in enumerate(uncertainty_error_pairs):
        uncertainty_category = uncertainty_pairing[0]
        invert_uncert_bool = [x[1] for x in invert_confidences if x[0] == uncertainty_category][0]
        evaluation_metric = uncertainty_pairing[1]
        uncertainty_measure = uncertainty_pairing[2]

        running_results = []
        running_error_bounds = []
        running_uncert_boundaries = []

        for fold in range(num_folds):
            validation_pairs = load_csv_columns(
                ue_pairs_val,
                "Validation Fold",
                fold,
                ["uid", evaluation_metric, uncertainty_measure],
            )

            if groundtruth_test_errors:
                cols_to_get = ["uid", evaluation_metric, uncertainty_measure]
            else:
                cols_to_get = ["uid", uncertainty_measure]

            testing_pairs = load_csv_columns(ue_pairs_test, "Testing Fold", fold, cols_to_get)

            if invert_uncert_bool:
                validation_pairs = apply_confidence_inversion(validation_pairs, uncertainty_measure)
                testing_pairs = apply_confidence_inversion(testing_pairs, uncertainty_measure)

            # Get Quantile Thresholds, fit Isotonic Regression (IR) line and estimate Error bounds.
            # Return both and save for each fold and target.
            validation_errors = validation_pairs[evaluation_metric].values
            validation_uncerts = validation_pairs[uncertainty_measure].values
            uncert_boundaries, estimated_errors = quantile_binning_and_est_errors(
                validation_errors,
                validation_uncerts,
                num_bins,
                type="quantile",
                combine_middle_bins=combine_middle_bins,
            )

            # Predict for test data
            test_bins_pred = quantile_binning_predictions(
                dict(zip(testing_pairs.uid, testing_pairs[uncertainty_measure])), uncert_boundaries
            )
            running_results.append(test_bins_pred)
            running_error_bounds.append((estimated_errors))
            running_uncert_boundaries.append(uncert_boundaries)

        # Combine dictionaries and save if you want
        combined_dict_bins = {k: v for x in running_results for k, v in x.items()}

        all_testing_results[uncertainty_measure + " bins"] = list(combined_dict_bins.values())
        error_bound_estimates[uncertainty_measure + " bounds"] = running_error_bounds
        all_uncert_boundaries[uncertainty_measure + " bounds"] = running_uncert_boundaries

    # Save Bin predictions and error bound estimations to spreadsheets
    if save_folder is not None:
        save_bin_path = os.path.join(save_folder)
        os.makedirs(save_bin_path, exist_ok=True)
        all_testing_results.to_csv(
            os.path.join(save_bin_path, "res_predicted_bins_t" + str(target_idx) + ".csv"), index=False
        )
        error_bound_estimates.to_csv(
            os.path.join(save_bin_path, "estimated_error_bounds_t" + str(target_idx) + ".csv"), index=False
        )

        all_uncert_boundaries.to_csv(
            os.path.join(save_bin_path, "uncertainty_bounds_t" + str(target_idx) + ".csv"), index=False
        )
        logger.info(
            "Saved predicted test bins for T%s, error bounds and uncertainty bounds to: %s", target_idx, save_bin_path
        )
    return all_uncert_boundaries, error_bound_estimates, all_testing_results
</file>

<file path="kale/embed/video_feature_extractor.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""
Define the feature extractor for video including I3D, R3D_18, MC3_18 and R2PLUS1D_18 w/o SELayers.
"""

import logging

from kale.embed.video_i3d import i3d_joint
from kale.embed.video_res3d import mc3, r2plus1d, r3d
from kale.embed.video_se_i3d import se_i3d_joint
from kale.embed.video_se_res3d import se_mc3, se_r2plus1d, se_r3d
from kale.loaddata.video_access import get_image_modality


def get_video_feat_extractor(model_name, image_modality, attention, num_classes):
    """
    Get the feature extractor w/o the pre-trained model and SELayers. The pre-trained models are saved in the path
    ``$XDG_CACHE_HOME/torch/hub/checkpoints/``. For Linux, default path is ``~/.cache/torch/hub/checkpoints/``.
    For Windows, default path is ``C:/Users/$USER_NAME/.cache/torch/hub/checkpoints/``.
    Provide four pre-trained models: "rgb_imagenet", "flow_imagenet", "rgb_charades", "flow_charades".

    Args:
        model_name (string): The name of the feature extractor. (Choices=["I3D", "R3D_18", "R2PLUS1D_18", "MC3_18"])
        image_modality (string): Image type. (Choices=["rgb", "flow", "joint"])
        attention (string): The attention type. (Choices=["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC",
        "SELayerCT", "SELayerTC", "SELayerMAC"])
        num_classes (int): The class number of specific dataset. (Default: No use)

    Returns:
        feature_network (dictionary): The network to extract features.
        class_feature_dim (int): The dimension of the feature network output for ClassNet.
                            It is a convention when the input dimension and the network is fixed.
        domain_feature_dim (int): The dimension of the feature network output for DomainNet.
    """
    rgb, flow = get_image_modality(image_modality)

    attention_list = ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerCT", "SELayerTC", "SELayerMAC"]
    model_list = ["I3D", "R3D_18", "MC3_18", "R2PLUS1D_18"]

    if attention in attention_list:
        att = True
    elif attention == "None":
        att = False
    else:
        raise ValueError("Wrong MODEL.ATTENTION. Current: {}".format(attention))

    if model_name not in model_list:
        raise ValueError("Wrong MODEL.METHOD. Current:{}".format(model_name))

    # Get I3D w/o SELayers for RGB, Flow or joint input
    if model_name == "I3D":
        rgb_pretrained_model = flow_pretrained_model = None
        if rgb:
            rgb_pretrained_model = "rgb_imagenet"  # Options=["rgb_imagenet", "rgb_charades"]
        if flow:
            flow_pretrained_model = "flow_imagenet"  # Options=["flow_imagenet", "flow_charades"]

        if rgb and flow:
            class_feature_dim = 2048
            domain_feature_dim = class_feature_dim / 2
        else:
            class_feature_dim = 1024
            domain_feature_dim = class_feature_dim

        if not att:
            logging.info("{} without SELayer.".format(model_name))
            feature_network = i3d_joint(
                rgb_pt=rgb_pretrained_model, flow_pt=flow_pretrained_model, num_classes=num_classes, pretrained=True
            )
        else:
            logging.info("{} with {}.".format(model_name, attention))
            feature_network = se_i3d_joint(
                rgb_pt=rgb_pretrained_model,
                flow_pt=flow_pretrained_model,
                attention=attention,
                num_classes=num_classes,
                pretrained=True,
            )

    # Get R3D_18/R2PLUS1D_18/MC3_18 w/o SELayers for RGB, Flow or joint input
    elif model_name in ["R3D_18", "R2PLUS1D_18", "MC3_18"]:
        if rgb and flow:
            class_feature_dim = 1024
            domain_feature_dim = class_feature_dim / 2
        else:
            class_feature_dim = 512
            domain_feature_dim = class_feature_dim

        if model_name == "R3D_18":
            if not att:
                logging.info("{} without SELayer.".format(model_name))
                feature_network = r3d(rgb=rgb, flow=flow, pretrained=True)
            else:
                logging.info("{} with {}.".format(model_name, attention))
                feature_network = se_r3d(rgb=rgb, flow=flow, pretrained=True, attention=attention)

        elif model_name == "R2PLUS1D_18":
            if not att:
                logging.info("{} without SELayer.".format(model_name))
                feature_network = r2plus1d(rgb=rgb, flow=flow, pretrained=True)
            else:
                logging.info("{} with {}.".format(model_name, attention))
                feature_network = se_r2plus1d(rgb=rgb, flow=flow, pretrained=True, attention=attention)

        elif model_name == "MC3_18":
            if not att:
                logging.info("{} without SELayer.".format(model_name))
                feature_network = mc3(rgb=rgb, flow=flow, pretrained=True)
            else:
                logging.info("{} with {}.".format(model_name, attention))
                feature_network = se_mc3(rgb=rgb, flow=flow, pretrained=True, attention=attention)

    return feature_network, int(class_feature_dim), int(domain_feature_dim)
</file>

<file path="kale/embed/video_i3d.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""
Define Inflated 3D ConvNets(I3D) on Action Recognition from https://ieeexplore.ieee.org/document/8099985
Created by Xianyuan Liu from modifying https://github.com/piergiaj/pytorch-i3d/blob/master/pytorch_i3d.py and
https://github.com/deepmind/kinetics-i3d/blob/master/i3d.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.hub import load_state_dict_from_url

model_urls = {
    "rgb_imagenet": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/rgb_imagenet.pt",
    "flow_imagenet": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/flow_imagenet.pt",
    "rgb_charades": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/rgb_charades.pt",
    "flow_charades": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/flow_charades.pt",
}


class MaxPool3dSamePadding(nn.MaxPool3d):
    """
    Construct 3d max pool with same padding. PyTorch does not provide same padding.
    Same padding means the output size matches input size for stride=1.
    """

    def compute_pad(self, dim, s):
        """Get the zero padding number."""

        if s % self.stride[dim] == 0:
            return max(self.kernel_size[dim] - self.stride[dim], 0)
        else:
            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)

    def forward(self, x):
        """Compute 'same' padding. Add zero to the back position first."""

        (batch, channel, time, height, width) = x.size()
        pad_t = self.compute_pad(0, time)
        pad_h = self.compute_pad(1, height)
        pad_w = self.compute_pad(2, width)

        pad_t_front = pad_t // 2
        pad_t_back = pad_t - pad_t_front
        pad_h_front = pad_h // 2
        pad_h_back = pad_h - pad_h_front
        pad_w_front = pad_w // 2
        pad_w_back = pad_w - pad_w_front

        pad = (pad_w_front, pad_w_back, pad_h_front, pad_h_back, pad_t_front, pad_t_back)
        x = F.pad(x, pad)
        return super(MaxPool3dSamePadding, self).forward(x)


class Unit3D(nn.Module):
    """Basic unit containing Conv3D + BatchNorm + non-linearity."""

    def __init__(
        self,
        in_channels,
        output_channels,
        kernel_shape=(1, 1, 1),
        stride=(1, 1, 1),
        padding=0,
        activation_fn=F.relu,
        use_batch_norm=True,
        use_bias=False,
        name="unit_3d",
    ):
        """Initializes Unit3D module."""

        super(Unit3D, self).__init__()

        self._output_channels = output_channels
        self._kernel_shape = kernel_shape
        self._stride = stride
        self._use_batch_norm = use_batch_norm
        self._activation_fn = activation_fn
        self._use_bias = use_bias
        self.name = name
        self.padding = padding

        self.conv3d = nn.Conv3d(
            in_channels=in_channels,
            out_channels=self._output_channels,
            kernel_size=self._kernel_shape,
            stride=self._stride,
            padding=0,
            # we always want padding to be 0 here. We will dynamically pad based on input size in
            # forward function
            bias=self._use_bias,
        )

        if self._use_batch_norm:
            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)

    def compute_pad(self, dim, s):
        """Get the zero padding number."""

        if s % self._stride[dim] == 0:
            return max(self._kernel_shape[dim] - self._stride[dim], 0)
        else:
            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)

    def forward(self, x):
        """
        Connects the module to inputs. Dynamically pad based on input size in forward function.
        Args:
            x: Inputs to the Unit3D component.

        Returns:
            Outputs from the module.
        """

        # compute 'same' padding
        (batch, channel, time, height, width) = x.size()
        pad_t = self.compute_pad(0, time)
        pad_h = self.compute_pad(1, height)
        pad_w = self.compute_pad(2, width)

        pad_t_front = pad_t // 2
        pad_t_back = pad_t - pad_t_front
        pad_h_front = pad_h // 2
        pad_h_back = pad_h - pad_h_front
        pad_w_front = pad_w // 2
        pad_w_back = pad_w - pad_w_front

        pad = (pad_w_front, pad_w_back, pad_h_front, pad_h_back, pad_t_front, pad_t_back)
        x = F.pad(x, pad)

        x = self.conv3d(x)
        if self._use_batch_norm:
            x = self.bn(x)
        if self._activation_fn is not None:
            x = self._activation_fn(x)
        return x


class InceptionModule(nn.Module):
    """
    Construct Inception module. Concatenation after four branches (1x1x1 conv; 1x1x1 + 3x3x3 convs; 1x1x1 + 3x3x3
    convs; 3x3x3 max-pool + 1x1x1 conv). In `forward`, we check if SELayers are used, which are
    channel-wise (SELayerC), temporal-wise (SELayerT), channel-temporal-wise (SELayerTC & SELayerCT).
    """

    def __init__(self, in_channels, out_channels, name):
        super(InceptionModule, self).__init__()

        self.b0 = Unit3D(
            in_channels=in_channels,
            output_channels=out_channels[0],
            kernel_shape=[1, 1, 1],
            padding=0,
            name=name + "/Branch_0/Conv3d_0a_1x1",
        )
        self.b1a = Unit3D(
            in_channels=in_channels,
            output_channels=out_channels[1],
            kernel_shape=[1, 1, 1],
            padding=0,
            name=name + "/Branch_1/Conv3d_0a_1x1",
        )
        self.b1b = Unit3D(
            in_channels=out_channels[1],
            output_channels=out_channels[2],
            kernel_shape=[3, 3, 3],
            name=name + "/Branch_1/Conv3d_0b_3x3",
        )
        self.b2a = Unit3D(
            in_channels=in_channels,
            output_channels=out_channels[3],
            kernel_shape=[1, 1, 1],
            padding=0,
            name=name + "/Branch_2/Conv3d_0a_1x1",
        )
        self.b2b = Unit3D(
            in_channels=out_channels[3],
            output_channels=out_channels[4],
            kernel_shape=[3, 3, 3],
            name=name + "/Branch_2/Conv3d_0b_3x3",
        )
        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0)
        self.b3b = Unit3D(
            in_channels=in_channels,
            output_channels=out_channels[5],
            kernel_shape=[1, 1, 1],
            padding=0,
            name=name + "/Branch_3/Conv3d_0b_1x1",
        )
        self.name = name

    def _forward(self, x):
        b0 = self.b0(x)
        b1 = self.b1b(self.b1a(x))
        b2 = self.b2b(self.b2a(x))
        b3 = self.b3b(self.b3a(x))

        output = [b0, b1, b2, b3]
        return output

    def forward(self, x):
        outputs = self._forward(x)
        out = torch.cat(outputs, dim=1)

        # Check if SELayer is used.
        if "SELayerC" in dir(self):  # check channel-wise
            out = self.SELayerC(out)
        if "SELayerCoC" in dir(self):
            out = self.SELayerCoC(out)
        if "SELayerMC" in dir(self):
            out = self.SELayerMC(out)
        if "SELayerMAC" in dir(self):
            out = self.SELayerMAC(out)

        if "SELayerT" in dir(self):  # check temporal-wise
            out = self.SELayerT(out)

        if "SELayerCTc" in dir(self):  # check channel-temporal-wise
            out = self.SELayerCTc(out)
        if "SELayerCTt" in dir(self):
            out = self.SELayerCTt(out)

        if "SELayerTCt" in dir(self):  # check temporal-channel-wise
            out = self.SELayerTCt(out)
        if "SELayerTCc" in dir(self):
            out = self.SELayerTCc(out)

        return out


class InceptionI3d(nn.Module):
    """
    Inception-v1 I3D architecture.
    The model is introduced in:
        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset
        Joao Carreira, Andrew Zisserman
        https://arxiv.org/pdf/1705.07750v1.pdf.
    See also the Inception architecture, introduced in:
        Going deeper with convolutions
        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.
        http://arxiv.org/pdf/1409.4842v1.pdf.
    """

    # Endpoints of the model in order. During construction, all the endpoints up
    # to a designated `final_endpoint` are returned in a dictionary as the
    # second return value.
    VALID_ENDPOINTS = (
        "Conv3d_1a_7x7",
        "MaxPool3d_2a_3x3",
        "Conv3d_2b_1x1",
        "Conv3d_2c_3x3",
        "MaxPool3d_3a_3x3",
        "Mixed_3b",
        "Mixed_3c",
        "MaxPool3d_4a_3x3",
        "Mixed_4b",
        "Mixed_4c",
        "Mixed_4d",
        "Mixed_4e",
        "Mixed_4f",
        "MaxPool3d_5a_2x2",
        "Mixed_5b",
        "Mixed_5c",
        "Logits",
        "Predictions",
    )

    def __init__(
        self,
        num_classes=400,
        spatial_squeeze=True,
        final_endpoint="Logits",
        name="inception_i3d",
        in_channels=3,
        dropout_keep_prob=0.5,
    ):
        """
        Initializes I3D model instance.

        Args:
          num_classes: The number of outputs in the logit layer (default 400, which
              matches the Kinetics dataset). Use `replace_logits` to update num_classes.
          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits
              before returning (default True).
          final_endpoint: The model contains many possible endpoints.
              `final_endpoint` specifies the last endpoint for the model to be built
              up to. In addition to the output at `final_endpoint`, all the outputs
              at endpoints up to `final_endpoint` will also be returned, in a
              dictionary. `final_endpoint` must be one of
              InceptionI3d.VALID_ENDPOINTS (default 'Logits').
          name: A string (optional). The name of this module.

        Raises:
          ValueError: if `final_endpoint` is not recognized.
        """

        if final_endpoint not in self.VALID_ENDPOINTS:
            raise ValueError("Unknown final endpoint %s" % final_endpoint)

        super(InceptionI3d, self).__init__()
        self._num_classes = num_classes
        self._spatial_squeeze = spatial_squeeze
        self._final_endpoint = final_endpoint
        self.logits = None

        if self._final_endpoint not in self.VALID_ENDPOINTS:
            raise ValueError("Unknown final endpoint %s" % self._final_endpoint)

        """Construct I3D architecture."""
        self.end_points = {}
        end_point = "Conv3d_1a_7x7"
        self.end_points[end_point] = Unit3D(
            in_channels=in_channels,
            output_channels=64,
            kernel_shape=[7, 7, 7],
            stride=(2, 2, 2),
            padding=(3, 3, 3),
            name=name + end_point,
        )
        if self._final_endpoint == end_point:
            return

        end_point = "MaxPool3d_2a_3x3"
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0)
        if self._final_endpoint == end_point:
            return

        end_point = "Conv3d_2b_1x1"
        self.end_points[end_point] = Unit3D(
            in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0, name=name + end_point
        )
        if self._final_endpoint == end_point:
            return

        end_point = "Conv3d_2c_3x3"
        self.end_points[end_point] = Unit3D(
            in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1, name=name + end_point
        )
        if self._final_endpoint == end_point:
            return

        end_point = "MaxPool3d_3a_3x3"
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_3b"
        self.end_points[end_point] = InceptionModule(192, [64, 96, 128, 16, 32, 32], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_3c"
        self.end_points[end_point] = InceptionModule(256, [128, 128, 192, 32, 96, 64], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "MaxPool3d_4a_3x3"
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2), padding=0)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_4b"
        self.end_points[end_point] = InceptionModule(128 + 192 + 96 + 64, [192, 96, 208, 16, 48, 64], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_4c"
        self.end_points[end_point] = InceptionModule(192 + 208 + 48 + 64, [160, 112, 224, 24, 64, 64], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_4d"
        self.end_points[end_point] = InceptionModule(160 + 224 + 64 + 64, [128, 128, 256, 24, 64, 64], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_4e"
        self.end_points[end_point] = InceptionModule(128 + 256 + 64 + 64, [112, 144, 288, 32, 64, 64], name + end_point)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_4f"
        self.end_points[end_point] = InceptionModule(
            112 + 288 + 64 + 64, [256, 160, 320, 32, 128, 128], name + end_point
        )
        if self._final_endpoint == end_point:
            return

        end_point = "MaxPool3d_5a_2x2"
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2), padding=0)
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_5b"
        self.end_points[end_point] = InceptionModule(
            256 + 320 + 128 + 128, [256, 160, 320, 32, 128, 128], name + end_point
        )
        if self._final_endpoint == end_point:
            return

        end_point = "Mixed_5c"
        self.end_points[end_point] = InceptionModule(
            256 + 320 + 128 + 128, [384, 192, 384, 48, 128, 128], name + end_point
        )
        if self._final_endpoint == end_point:
            return

        end_point = "Logits"
        # self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7], stride=(1, 1, 1))
        # self.avg_pool_flow = nn.AvgPool3d(kernel_size=[1, 7, 7], stride=(1, 1, 1))
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.dropout = nn.Dropout(dropout_keep_prob)
        self.logits = Unit3D(
            in_channels=384 + 384 + 128 + 128,
            output_channels=400,
            kernel_shape=[1, 1, 1],
            padding=0,
            activation_fn=None,
            use_batch_norm=False,
            use_bias=True,
            name="logits",
        )

        self.build()

    def replace_logits(self, num_classes):
        """Update the output size with num_classes according to the specific setting."""

        self._num_classes = num_classes
        self.logits = Unit3D(
            in_channels=384 + 384 + 128 + 128,
            output_channels=self._num_classes,
            kernel_shape=[1, 1, 1],
            padding=0,
            activation_fn=None,
            use_batch_norm=False,
            use_bias=True,
            name="logits",
        )

    def build(self):
        for k in self.end_points.keys():
            self.add_module(k, self.end_points[k])

    def forward(self, x):
        """The output is the result of the final average pooling layer with 1024 dimensions."""

        # x = self._modules["Conv3d_1a_7x7"](x)  # out: [2, 64, 1, 112, 112]
        # x = self._modules["MaxPool3d_2a_3x3"](x)  # [2, 64, 1, 56, 56]
        # x = self._modules["Conv3d_2b_1x1"](x)  # [2, 64, 1, 56, 56]
        # x = self._modules["Conv3d_2c_3x3"](x)  # [2, 192, 1, 56, 56]
        # x = self._modules["MaxPool3d_3a_3x3"](x)  # [2, 192, 1, 28, 28]
        # x = self._modules["Mixed_3b"](x)  # [2, 256, 1, 28, 28]
        # x = self._modules["Mixed_3c"](x)  # [2, 480, 1, 28, 28]
        # x = self._modules["MaxPool3d_4a_3x3"](x)  # [2, 480, 1, 14, 14]
        # x = self._modules["Mixed_4b"](x)  # [2, 512, 1, 14, 14]
        # x = self._modules["Mixed_4c"](x)  # [2, 512, 1, 14, 14]
        # x = self._modules["Mixed_4d"](x)  # [2, 512, 1, 14, 14]
        # x = self._modules["Mixed_4e"](x)  # [2, 528, 1, 14, 14]
        # x = self._modules["Mixed_4f"](x)  # [2, 832, 1, 14, 14]
        # x = self._modules["MaxPool3d_5a_2x2"](x)  # [2, 832, 1, 7, 7]
        # x = self._modules["Mixed_5b"](x)  # [2, 832, 1, 7, 7]
        # x = self._modules["Mixed_5c"](x)  # [2, 1024, 1, 7, 7]

        for end_point in self.VALID_ENDPOINTS:
            if end_point in self.end_points:
                x = self._modules[end_point](x)  # use _modules to work with dataparallel

        x = self.avg_pool(x)
        # logits = self.logits(self.dropout(x))
        if self._spatial_squeeze:
            x = x.squeeze(3).squeeze(3)
        # x is batch X time X classes, which is what we want to work with
        return x

    def extract_features(self, x):
        for end_point in self.VALID_ENDPOINTS:
            if end_point in self.end_points:
                x = self._modules[end_point](x)
        return self.avg_pool(x)


def i3d(name, num_channels, num_classes, pretrained=False, progress=True):
    """Get InceptionI3d module w/o pretrained model."""
    model = InceptionI3d(in_channels=num_channels, num_classes=num_classes)

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[name], progress=progress)

        # delete the last layer's parameter and only load the parameters before the last due to different class number.
        # uncomment and change the output size of I3D when using the default classifier in I3D.

        # state_dict.pop("logits.conv3d.weight")
        # state_dict.pop("logits.conv3d.bias")
        # model.load_state_dict(state_dict, strict=False)

        model.load_state_dict(state_dict)
    return model


def i3d_joint(rgb_pt, flow_pt, num_classes, pretrained=False, progress=True):
    """Get I3D models for different inputs.

    Args:
        rgb_pt (string, optional): the name of pre-trained model for RGB input.
        flow_pt (string, optional): the name of pre-trained model for flow input.
        num_classes (int): the class number of dataset.
        pretrained (bool): choose if pretrained parameters are used. (Default: False)
        progress (bool, optional): whether or not to display a progress bar to stderr. (Default: True)

    Returns:
        models (dictionary): A dictionary contains RGB and flow models.
    """
    i3d_rgb = i3d_flow = None
    if rgb_pt is not None:
        i3d_rgb = i3d(name=rgb_pt, num_channels=3, num_classes=num_classes, pretrained=pretrained, progress=progress)
    if flow_pt is not None:
        i3d_flow = i3d(name=flow_pt, num_channels=2, num_classes=num_classes, pretrained=pretrained, progress=progress)
    models = {"rgb": i3d_rgb, "flow": i3d_flow}
    return models
</file>

<file path="kale/embed/video_res3d.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""
Define MC3_18, R3D_18, R2plus1D_18 on Action Recognition from https://arxiv.org/abs/1711.11248
Created by Xianyuan Liu from modifying https://github.com/pytorch/vision/blob/master/torchvision/models/video/resnet.py
"""

import torch.nn as nn
from torch.hub import load_state_dict_from_url

model_urls = {
    "r3d_18": "https://download.pytorch.org/models/r3d_18-b3b3357e.pth",
    "mc3_18": "https://download.pytorch.org/models/mc3_18-a90a0ba3.pth",
    "r2plus1d_18": "https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth",
}


class Conv3DSimple(nn.Conv3d):
    """3D convolutions for R3D (3x3x3 kernel)"""

    def __init__(self, in_planes, out_planes, midplanes=None, stride=1, padding=1):
        super(Conv3DSimple, self).__init__(
            in_channels=in_planes,
            out_channels=out_planes,
            kernel_size=(3, 3, 3),
            stride=stride,
            padding=padding,
            bias=False,
        )

    @staticmethod
    def get_downsample_stride(stride):
        return stride, stride, stride


class Conv2Plus1D(nn.Sequential):
    """(2+1)D convolutions for R2plus1D (1x3x3 kernel + 3x1x1 kernel)"""

    def __init__(self, in_planes, out_planes, midplanes, stride=1, padding=1):
        super(Conv2Plus1D, self).__init__(
            nn.Conv3d(
                in_planes,
                midplanes,
                kernel_size=(1, 3, 3),
                stride=(1, stride, stride),
                padding=(0, padding, padding),
                bias=False,
            ),
            nn.BatchNorm3d(midplanes),
            nn.ReLU(inplace=True),
            nn.Conv3d(
                midplanes, out_planes, kernel_size=(3, 1, 1), stride=(stride, 1, 1), padding=(padding, 0, 0), bias=False
            ),
        )

    @staticmethod
    def get_downsample_stride(stride):
        return stride, stride, stride


class Conv3DNoTemporal(nn.Conv3d):
    """3D convolutions without temporal dimension for MCx (1x3x3 kernel)"""

    def __init__(self, in_planes, out_planes, midplanes=None, stride=1, padding=1):
        super(Conv3DNoTemporal, self).__init__(
            in_channels=in_planes,
            out_channels=out_planes,
            kernel_size=(1, 3, 3),
            stride=(1, stride, stride),
            padding=(0, padding, padding),
            bias=False,
        )

    @staticmethod
    def get_downsample_stride(stride):
        return 1, stride, stride


class BasicBlock(nn.Module):
    """
    Basic ResNet building block. Each block consists of two convolutional layers with a ReLU activation function
    after each layer and residual connections. In `forward`, we check if SELayers are used, which are
    channel-wise (SELayerC) and temporal-wise (SELayerT).
    """

    expansion = 1

    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):
        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)

        super(BasicBlock, self).__init__()
        self.conv1 = nn.Sequential(
            conv_builder(inplanes, planes, midplanes, stride), nn.BatchNorm3d(planes), nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(conv_builder(planes, planes, midplanes), nn.BatchNorm3d(planes))
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample is not None:
            residual = self.downsample(x)

        # Check if SELayer is used.
        if "SELayerC" in dir(self):  # check channel-wise
            out = self.SELayerC(out)
        if "SELayerCoC" in dir(self):
            out = self.SELayerCoC(out)
        if "SELayerMC" in dir(self):
            out = self.SELayerMC(out)
        if "SELayerMAC" in dir(self):
            out = self.SELayerMAC(out)

        if "SELayerT" in dir(self):  # check temporal-wise
            out = self.SELayerT(out)

        if "SELayerCTc" in dir(self):  # check channel-temporal-wise
            out = self.SELayerCTc(out)
        if "SELayerCTt" in dir(self):
            out = self.SELayerCTt(out)

        if "SELayerTCt" in dir(self):  # check temporal-channel-wise
            out = self.SELayerTCt(out)
        if "SELayerTCc" in dir(self):
            out = self.SELayerTCc(out)

        out += residual
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    """
    BottleNeck building block. Default: No use. Each block consists of two 1*n*n and one n*n*n convolutional layers
    with a ReLU activation function after each layer and residual connections.
    """

    expansion = 4

    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)

        # 1x1x1
        self.conv1 = nn.Sequential(
            nn.Conv3d(inplanes, planes, kernel_size=1, bias=False), nn.BatchNorm3d(planes), nn.ReLU(inplace=True)
        )
        # Second kernel
        self.conv2 = nn.Sequential(
            conv_builder(planes, planes, midplanes, stride), nn.BatchNorm3d(planes), nn.ReLU(inplace=True)
        )

        # 1x1x1
        self.conv3 = nn.Sequential(
            nn.Conv3d(planes, planes * self.expansion, kernel_size=1, bias=False),
            nn.BatchNorm3d(planes * self.expansion),
        )
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.conv2(out)
        out = self.conv3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class BasicStem(nn.Sequential):
    """The default conv-batchnorm-relu stem. The first layer normally. (64 3x7x7 kernels)"""

    def __init__(self):
        super(BasicStem, self).__init__(
            nn.Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
        )


class BasicFLowStem(nn.Sequential):
    """The default stem for optical flow."""

    def __init__(self):
        super(BasicFLowStem, self).__init__(
            nn.Conv3d(2, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
        )


class R2Plus1dStem(nn.Sequential):
    """
    R(2+1)D stem is different than the default one as it uses separated 3D convolution.
    (45 1x7x7 kernels + 64 3x1x1 kernel)
    """

    def __init__(self):
        super(R2Plus1dStem, self).__init__(
            nn.Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False),
            nn.BatchNorm3d(45),
            nn.ReLU(inplace=True),
            nn.Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
        )


class R2Plus1dFlowStem(nn.Sequential):
    """R(2+1)D stem for optical flow."""

    def __init__(self):
        super(R2Plus1dFlowStem, self).__init__(
            nn.Conv3d(2, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False),
            nn.BatchNorm3d(45),
            nn.ReLU(inplace=True),
            nn.Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
        )


class VideoResNet(nn.Module):
    def __init__(self, block, conv_makers, layers, stem, num_classes=400, zero_init_residual=False):
        """Generic resnet video generator.

        Args:
            block (nn.Module): resnet building block
            conv_makers (list(functions)): generator function for each layer
            layers (List[int]): number of blocks per layer
            stem (nn.Module, optional): Resnet stem, if None, defaults to conv-bn-relu. Defaults to None.
            num_classes (int, optional): Dimension of the final FC layer. Defaults to 400.
            zero_init_residual (bool, optional): Zero init bottleneck residual BN. Defaults to False.
        """
        super(VideoResNet, self).__init__()
        self.inplanes = 64

        self.stem = stem()

        self.layer1 = self._make_layer(block, conv_makers[0], 64, layers[0], stride=1)
        self.layer2 = self._make_layer(block, conv_makers[1], 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, conv_makers[2], 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, conv_makers[3], 512, layers[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        # init weights
        self._initialize_weights()

        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)

    def replace_fc(self, num_classes, block=BasicBlock):
        """Update the output size with num_classes according to the specific setting."""

        self.fc = nn.Linear(512 * block.expansion, num_classes)

    def forward(self, x):
        x = self.stem(x)  # [b, 64, 16, 112, 112]

        x = self.layer1(x)  # [b, 64, 16, 112, 112]
        x = self.layer2(x)  # [b, 128, 8, 56, 56]
        x = self.layer3(x)  # [b, 256, 4, 28, 28]
        x = self.layer4(x)  # [b, 512, 2, 14, 14]

        x = self.avgpool(x)  # [b, 512, 1, 1, 1]
        # Flatten the layer to fc
        x = x.flatten(1)  # [b, 512]
        # x = self.fc(x)

        return x

    def _make_layer(self, block, conv_builder, planes, blocks, stride=1):
        downsample = None

        if stride != 1 or self.inplanes != planes * block.expansion:
            ds_stride = conv_builder.get_downsample_stride(stride)
            downsample = nn.Sequential(
                nn.Conv3d(self.inplanes, planes * block.expansion, kernel_size=1, stride=ds_stride, bias=False),
                nn.BatchNorm3d(planes * block.expansion),
            )
        layers = []
        layers.append(block(self.inplanes, planes, conv_builder, stride, downsample))

        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, conv_builder))

        return nn.Sequential(*layers)

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)


def _video_resnet(arch, pretrained=False, progress=True, **kwargs):
    model = VideoResNet(**kwargs)

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        model.load_state_dict(state_dict)
    return model


def _video_resnet_flow(arch, pretrained=False, progress=True, **kwargs):
    model = VideoResNet(**kwargs)

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        state_dict.pop("stem.0.weight")
        model.load_state_dict(state_dict, strict=False)
    return model


def r3d_18_rgb(pretrained=False, progress=True, **kwargs):
    """Construct 18 layer Resnet3D model for RGB as in
    https://arxiv.org/abs/1711.11248

    Args:
        pretrained (bool): If True, returns a model pre-trained on Kinetics-400
        progress (bool): If True, displays a progress bar of the download to stderr

    Returns:
        nn.Module: R3D-18 network
    """

    return _video_resnet(
        "r3d_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
        **kwargs,
    )


def r3d_18_flow(pretrained=False, progress=True, **kwargs):
    """Construct 18 layer Resnet3D model for optical flow."""

    return _video_resnet_flow(
        "r3d_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicFLowStem,
        **kwargs,
    )


def mc3_18_rgb(pretrained=False, progress=True, **kwargs):
    """Constructor for 18 layer Mixed Convolution network for RGB as in
    https://arxiv.org/abs/1711.11248

    Args:
        pretrained (bool): If True, returns a model pre-trained on Kinetics-400
        progress (bool): If True, displays a progress bar of the download to stderr

    Returns:
        nn.Module: MC3 Network definition
    """

    return _video_resnet(
        "mc3_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] + [Conv3DNoTemporal] * 3,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
        **kwargs,
    )


def mc3_18_flow(pretrained=False, progress=True, **kwargs):
    """Constructor for 18 layer Mixed Convolution network for optical flow."""

    return _video_resnet_flow(
        "mc3_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] + [Conv3DNoTemporal] * 3,
        layers=[2, 2, 2, 2],
        stem=BasicFLowStem,
        **kwargs,
    )


def r2plus1d_18_rgb(pretrained=False, progress=True, **kwargs):
    """Constructor for the 18 layer deep R(2+1)D network for RGB as in
    https://arxiv.org/abs/1711.11248

    Args:
        pretrained (bool): If True, returns a model pre-trained on Kinetics-400
        progress (bool): If True, displays a progress bar of the download to stderr

    Returns:
        nn.Module: R(2+1)D-18 network
    """

    return _video_resnet(
        "r2plus1d_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv2Plus1D] * 4,
        layers=[2, 2, 2, 2],
        stem=R2Plus1dStem,
        **kwargs,
    )


def r2plus1d_18_flow(pretrained=False, progress=True, **kwargs):
    """Constructor for the 18 layer deep R(2+1)D network for optical flow."""

    return _video_resnet_flow(
        "r2plus1d_18",
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv2Plus1D] * 4,
        layers=[2, 2, 2, 2],
        stem=R2Plus1dFlowStem,
        **kwargs,
    )


def r3d(rgb=False, flow=False, pretrained=False, progress=True):
    """Get R3D_18 models."""
    r3d_rgb = r3d_flow = None
    if rgb:
        r3d_rgb = r3d_18_rgb(pretrained=pretrained, progress=progress)
    if flow:
        r3d_flow = r3d_18_flow(pretrained=pretrained, progress=progress)
    models = {"rgb": r3d_rgb, "flow": r3d_flow}
    return models


def mc3(rgb=False, flow=False, pretrained=False, progress=True):
    """Get MC3_18 models."""
    mc3_rgb = mc3_flow = None
    if rgb:
        mc3_rgb = mc3_18_rgb(pretrained=pretrained, progress=progress)
    if flow:
        mc3_flow = mc3_18_flow(pretrained=pretrained, progress=progress)
    models = {"rgb": mc3_rgb, "flow": mc3_flow}
    return models


def r2plus1d(rgb=False, flow=False, pretrained=False, progress=True):
    """Get R2PLUS1D_18 models."""
    r2plus1d_rgb = r2plus1d_flow = None
    if rgb:
        r2plus1d_rgb = r2plus1d_18_rgb(pretrained=pretrained, progress=progress)
    if flow:
        r2plus1d_flow = r2plus1d_18_flow(pretrained=pretrained, progress=progress)
    models = {"rgb": r2plus1d_rgb, "flow": r2plus1d_flow}
    return models
</file>

<file path="kale/embed/video_se_i3d.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""Add SELayers to I3D"""

import torch.nn as nn
from torch.hub import load_state_dict_from_url

from kale.embed.video_i3d import InceptionI3d
from kale.embed.video_selayer import get_selayer, SELayerC, SELayerT

model_urls = {
    "rgb_imagenet": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/rgb_imagenet.pt",
    "flow_imagenet": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/flow_imagenet.pt",
    "rgb_charades": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/rgb_charades.pt",
    "flow_charades": "https://github.com/XianyuanLiu/pytorch-i3d/raw/master/models/flow_charades.pt",
}


class SEInceptionI3DRGB(nn.Module):
    """Add the several SELayers to I3D for RGB input.
    Args:
        num_channels (int): the channel number of the input.
        num_classes (int): the class number of dataset.
        attention (string): the name of the SELayer.
            (Options: ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerMAC", "SELayerCT" and "SELayerTC"])

    Returns:
        model (VideoResNet): I3D model with SELayers.
    """

    def __init__(self, num_channels, num_classes, attention):
        super(SEInceptionI3DRGB, self).__init__()
        model = InceptionI3d(in_channels=num_channels, num_classes=num_classes)
        temporal_length = 16

        # Add channel-wise SELayer
        if attention in ["SELayerC", "SELayerCoC", "SELayerMC", "SELayerMAC"]:
            se_layer = get_selayer(attention)
            model.Mixed_3b.add_module(attention, se_layer(256))
            model.Mixed_3c.add_module(attention, se_layer(480))
            model.Mixed_4b.add_module(attention, se_layer(512))
            model.Mixed_4c.add_module(attention, se_layer(512))
            model.Mixed_4d.add_module(attention, se_layer(512))
            model.Mixed_4e.add_module(attention, se_layer(528))
            model.Mixed_4f.add_module(attention, se_layer(832))
            model.Mixed_5b.add_module(attention, se_layer(832))
            model.Mixed_5c.add_module(attention, se_layer(1024))

        # Add temporal-wise SELayer
        elif attention == "SELayerT":
            se_layer = get_selayer(attention)
            model.Mixed_3b.add_module(attention, se_layer(temporal_length // 2))
            model.Mixed_3c.add_module(attention, se_layer(temporal_length // 2))
            model.Mixed_4b.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_4c.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_4d.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_4e.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_4f.add_module(attention, se_layer(temporal_length // 4))
            # model.Mixed_5b.add_module(attention, SELayerT(temporal_length//8))
            # model.Mixed_5c.add_module(attention, SELayerT(temporal_length//8))

        # Add channel-temporal-wise SELayer
        elif attention == "SELayerCT":
            model.Mixed_3b.add_module(attention + "c", SELayerC(256))
            model.Mixed_3c.add_module(attention + "c", SELayerC(480))
            model.Mixed_4b.add_module(attention + "c", SELayerC(512))
            model.Mixed_4c.add_module(attention + "c", SELayerC(512))
            model.Mixed_4d.add_module(attention + "c", SELayerC(512))
            model.Mixed_4e.add_module(attention + "c", SELayerC(528))
            model.Mixed_4f.add_module(attention + "c", SELayerC(832))
            model.Mixed_5b.add_module(attention + "c", SELayerC(832))
            model.Mixed_5c.add_module(attention + "c", SELayerC(1024))

            model.Mixed_3b.add_module(attention + "t", SELayerT(temporal_length // 2))
            model.Mixed_3c.add_module(attention + "t", SELayerT(temporal_length // 2))
            model.Mixed_4b.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4c.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4d.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4e.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4f.add_module(attention + "t", SELayerT(temporal_length // 4))
            # model.Mixed_5b.add_module(attention + "t", SELayerT(temporal_length // 8))
            # model.Mixed_5c.add_module(attention + "t", SELayerT(temporal_length // 8))

        # Add temporal-channel-wise SELayer
        elif attention == "SELayerTC":
            model.Mixed_3b.add_module(attention + "t", SELayerT(temporal_length // 2))
            model.Mixed_3c.add_module(attention + "t", SELayerT(temporal_length // 2))
            model.Mixed_4b.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4c.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4d.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4e.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4f.add_module(attention + "t", SELayerT(temporal_length // 4))
            # model.Mixed_5b.add_module(attention + "t", SELayerT(temporal_length // 8))
            # model.Mixed_5c.add_module(attention + "t", SELayerT(temporal_length // 8))

            model.Mixed_3b.add_module(attention + "c", SELayerC(256))
            model.Mixed_3c.add_module(attention + "c", SELayerC(480))
            model.Mixed_4b.add_module(attention + "c", SELayerC(512))
            model.Mixed_4c.add_module(attention + "c", SELayerC(512))
            model.Mixed_4d.add_module(attention + "c", SELayerC(512))
            model.Mixed_4e.add_module(attention + "c", SELayerC(528))
            model.Mixed_4f.add_module(attention + "c", SELayerC(832))
            model.Mixed_5b.add_module(attention + "c", SELayerC(832))
            model.Mixed_5c.add_module(attention + "c", SELayerC(1024))

        else:
            raise ValueError("Wrong MODEL.ATTENTION. Current:{}".format(attention))

        self.model = model

    def forward(self, x):
        return self.model(x)


class SEInceptionI3DFlow(nn.Module):
    """Add the several SELayers to I3D for optical flow input."""

    def __init__(self, num_channels, num_classes, attention):
        super(SEInceptionI3DFlow, self).__init__()
        model = InceptionI3d(in_channels=num_channels, num_classes=num_classes)
        temporal_length = 16

        # Add channel-wise SELayer
        if attention in ["SELayerC", "SELayerCoC", "SELayerMC", "SELayerMAC"]:
            se_layer = get_selayer(attention)
            model.Mixed_3b.add_module(attention, se_layer(256))
            model.Mixed_3c.add_module(attention, se_layer(480))
            model.Mixed_4b.add_module(attention, se_layer(512))
            model.Mixed_4c.add_module(attention, se_layer(512))
            model.Mixed_4d.add_module(attention, se_layer(512))
            model.Mixed_4e.add_module(attention, se_layer(528))
            model.Mixed_4f.add_module(attention, se_layer(832))
            model.Mixed_5b.add_module(attention, se_layer(832))
            model.Mixed_5c.add_module(attention, se_layer(1024))

        # Add temporal-wise SELayer
        elif attention == "SELayerT":
            se_layer = get_selayer(attention)
            model.Mixed_3b.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_3c.add_module(attention, se_layer(temporal_length // 4))
            model.Mixed_4b.add_module(attention, se_layer(temporal_length // 8))
            model.Mixed_4c.add_module(attention, se_layer(temporal_length // 8))
            model.Mixed_4d.add_module(attention, se_layer(temporal_length // 8))
            model.Mixed_4e.add_module(attention, se_layer(temporal_length // 8))
            model.Mixed_4f.add_module(attention, se_layer(temporal_length // 8))

        # Add channel-temporal-wise SELayer
        elif attention == "SELayerCT":
            model.Mixed_3b.add_module(attention + "c", SELayerC(256))
            model.Mixed_3c.add_module(attention + "c", SELayerC(480))
            model.Mixed_4b.add_module(attention + "c", SELayerC(512))
            model.Mixed_4c.add_module(attention + "c", SELayerC(512))
            model.Mixed_4d.add_module(attention + "c", SELayerC(512))
            model.Mixed_4e.add_module(attention + "c", SELayerC(528))
            model.Mixed_4f.add_module(attention + "c", SELayerC(832))
            model.Mixed_5b.add_module(attention + "c", SELayerC(832))
            model.Mixed_5c.add_module(attention + "c", SELayerC(1024))

            model.Mixed_3b.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_3c.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4b.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4c.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4d.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4e.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4f.add_module(attention + "t", SELayerT(temporal_length // 8))

        # Add temporal-channel-wise SELayer
        elif attention == "SELayerTC":
            model.Mixed_3b.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_3c.add_module(attention + "t", SELayerT(temporal_length // 4))
            model.Mixed_4b.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4c.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4d.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4e.add_module(attention + "t", SELayerT(temporal_length // 8))
            model.Mixed_4f.add_module(attention + "t", SELayerT(temporal_length // 8))

            model.Mixed_3b.add_module(attention + "c", SELayerC(256))
            model.Mixed_3c.add_module(attention + "c", SELayerC(480))
            model.Mixed_4b.add_module(attention + "c", SELayerC(512))
            model.Mixed_4c.add_module(attention + "c", SELayerC(512))
            model.Mixed_4d.add_module(attention + "c", SELayerC(512))
            model.Mixed_4e.add_module(attention + "c", SELayerC(528))
            model.Mixed_4f.add_module(attention + "c", SELayerC(832))
            model.Mixed_5b.add_module(attention + "c", SELayerC(832))
            model.Mixed_5c.add_module(attention + "c", SELayerC(1024))

        else:
            raise ValueError("Wrong MODEL.ATTENTION. Current:{}".format(attention))

        self.model = model

    def forward(self, x):
        return self.model(x)


def se_inception_i3d(name, num_channels, num_classes, attention, pretrained=False, progress=True, rgb=True):
    """Get InceptionI3d module w/o SELayer and pretrained model."""
    if rgb:
        model = SEInceptionI3DRGB(num_channels, num_classes, attention)
    else:
        model = SEInceptionI3DFlow(num_channels, num_classes, attention)

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[name], progress=progress)

        # delete the last layer's parameter and only load the parameters before the last due to different class number.
        # uncomment and change the output size of I3D when using the default classifier in I3D.

        # state_dict.pop("logits.conv3d.weight")
        # state_dict.pop("logits.conv3d.bias")
        # model.load_state_dict(state_dict, strict=False)

        # Create new OrderedDict that add `model.`
        from collections import OrderedDict

        new_state_dict = OrderedDict()
        for k, v in state_dict.items():
            name = "model.{}".format(k)
            new_state_dict[name] = v

        # Load params except SELayer
        model.load_state_dict(new_state_dict, strict=False)
    return model


def se_i3d_joint(rgb_pt, flow_pt, num_classes, attention, pretrained=False, progress=True):
    """Get I3D models with SELayers for different inputs.

    Args:
        rgb_pt (string, optional): the name of pre-trained model for RGB input.
        flow_pt (string, optional): the name of pre-trained model for optical flow input.
        num_classes (int): the class number of dataset.
        attention (string, optional): the name of the SELayer.
        pretrained (bool): choose if pretrained parameters are used. (Default: False)
        progress (bool, optional): whether or not to display a progress bar to stderr. (Default: True)

    Returns:
        models (dictionary): A dictionary contains models for RGB and optical flow.
    """

    i3d_rgb = i3d_flow = None
    if rgb_pt is not None:
        i3d_rgb = se_inception_i3d(
            name=rgb_pt,
            num_channels=3,
            num_classes=num_classes,
            attention=attention,
            pretrained=pretrained,
            progress=progress,
            rgb=True,
        )
    if flow_pt is not None:
        i3d_flow = se_inception_i3d(
            name=flow_pt,
            num_channels=2,
            num_classes=num_classes,
            attention=attention,
            pretrained=pretrained,
            progress=progress,
            rgb=False,
        )
    models = {"rgb": i3d_rgb, "flow": i3d_flow}
    return models
</file>

<file path="kale/embed/video_se_res3d.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""Add SELayers to MC3_18, R3D_18, R2plus1D_18"""

from torch.hub import load_state_dict_from_url

from kale.embed.video_res3d import (
    BasicBlock,
    BasicFLowStem,
    BasicStem,
    Conv2Plus1D,
    Conv3DNoTemporal,
    Conv3DSimple,
    R2Plus1dFlowStem,
    R2Plus1dStem,
    VideoResNet,
)
from kale.embed.video_selayer import get_selayer, SELayerC, SELayerT

model_urls = {
    "r3d_18": "https://download.pytorch.org/models/r3d_18-b3b3357e.pth",
    "mc3_18": "https://download.pytorch.org/models/mc3_18-a90a0ba3.pth",
    "r2plus1d_18": "https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth",
}


def _se_video_resnet_rgb(arch, attention, pretrained=False, progress=True, **kwargs):
    """Add the several SELayers to MC3_18, R3D_18, R2plus1D_18 for RGB input.

    Args:
        arch (string): the name of basic architecture. (Options: ["r3d_18", "mc3_18" and "r2plus1d_18"])
        attention (string): the name of the SELayer.
            (Options: ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerMAC", "SELayerCT", and "SELayerTC"])
        pretrained (bool): choose if pretrained parameters are used. (Default: False)
        progress (bool, optional): whether or not to display a progress bar to stderr. (Default: True)

    Returns:
        model (VideoResNet): 3D convolution-based model with SELayers.
    """
    model = VideoResNet(**kwargs)
    temporal_length = 16

    # Add channel-wise SELayer
    if attention in ["SELayerC", "SELayerCoC", "SELayerMC", "SELayerMAC"]:
        se_layer = get_selayer(attention)
        model.layer1._modules["0"].add_module(attention, se_layer(64))
        model.layer1._modules["1"].add_module(attention, se_layer(64))
        model.layer2._modules["0"].add_module(attention, se_layer(128))
        model.layer2._modules["1"].add_module(attention, se_layer(128))
        model.layer3._modules["0"].add_module(attention, se_layer(256))
        model.layer3._modules["1"].add_module(attention, se_layer(256))
        model.layer4._modules["0"].add_module(attention, se_layer(512))
        model.layer4._modules["1"].add_module(attention, se_layer(512))

    # Add temporal-wise SELayer
    elif attention == "SELayerT":
        se_layer = get_selayer(attention)
        model.layer1._modules["0"].add_module(attention, se_layer(temporal_length))
        model.layer1._modules["1"].add_module(attention, se_layer(temporal_length))
        model.layer2._modules["0"].add_module(attention, se_layer(temporal_length // 2))
        model.layer2._modules["1"].add_module(attention, se_layer(temporal_length // 2))
        model.layer3._modules["0"].add_module(attention, se_layer(temporal_length // 4))
        model.layer3._modules["1"].add_module(attention, se_layer(temporal_length // 4))

    # Add channel-temporal-wise SELayer
    elif attention == "SELayerCT":
        model.layer1._modules["0"].add_module(attention + "c", SELayerC(64))
        model.layer1._modules["1"].add_module(attention + "c", SELayerC(64))
        model.layer2._modules["0"].add_module(attention + "c", SELayerC(128))
        model.layer2._modules["1"].add_module(attention + "c", SELayerC(128))
        model.layer3._modules["0"].add_module(attention + "c", SELayerC(256))
        model.layer3._modules["1"].add_module(attention + "c", SELayerC(256))
        model.layer4._modules["0"].add_module(attention + "c", SELayerC(512))
        model.layer4._modules["1"].add_module(attention + "c", SELayerC(512))

        model.layer1._modules["0"].add_module(attention + "t", SELayerT(temporal_length))
        model.layer1._modules["1"].add_module(attention + "t", SELayerT(temporal_length))
        model.layer2._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer2._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer3._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 4))
        model.layer3._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 4))

    # Add temporal-channel-wise SELayer
    elif attention == "SELayerTC":
        model.layer1._modules["0"].add_module(attention + "t", SELayerT(temporal_length))
        model.layer1._modules["1"].add_module(attention + "t", SELayerT(temporal_length))
        model.layer2._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer2._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer3._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 4))
        model.layer3._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 4))

        model.layer1._modules["0"].add_module(attention + "c", SELayerC(64))
        model.layer1._modules["1"].add_module(attention + "c", SELayerC(64))
        model.layer2._modules["0"].add_module(attention + "c", SELayerC(128))
        model.layer2._modules["1"].add_module(attention + "c", SELayerC(128))
        model.layer3._modules["0"].add_module(attention + "c", SELayerC(256))
        model.layer3._modules["1"].add_module(attention + "c", SELayerC(256))
        model.layer4._modules["0"].add_module(attention + "c", SELayerC(512))
        model.layer4._modules["1"].add_module(attention + "c", SELayerC(512))

    else:
        raise ValueError("Wrong MODEL.ATTENTION. Current:{}".format(attention))

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        model.load_state_dict(state_dict, strict=False)
    return model


def _se_video_resnet_flow(arch, attention, pretrained=False, progress=True, **kwargs):
    """Add the several SELayers to MC3_18, R3D_18, R2plus1D_18 for optical flow input."""
    model = VideoResNet(**kwargs)
    temporal_length = 16

    # Add channel-wise SELayer
    if attention in ["SELayerC", "SELayerCoC", "SELayerMC", "SELayerMAC"]:
        se_layer = get_selayer(attention)
        model.layer1._modules["0"].add_module(attention, se_layer(64))
        model.layer1._modules["1"].add_module(attention, se_layer(64))
        model.layer2._modules["0"].add_module(attention, se_layer(128))
        model.layer2._modules["1"].add_module(attention, se_layer(128))
        model.layer3._modules["0"].add_module(attention, se_layer(256))
        model.layer3._modules["1"].add_module(attention, se_layer(256))
        model.layer4._modules["0"].add_module(attention, se_layer(512))
        model.layer4._modules["1"].add_module(attention, se_layer(512))

    # Add temporal-wise SELayer
    elif attention == "SELayerT":
        se_layer = get_selayer(attention)
        model.layer1._modules["0"].add_module(attention, se_layer(temporal_length // 2))
        model.layer1._modules["1"].add_module(attention, se_layer(temporal_length // 2))
        model.layer2._modules["0"].add_module(attention, se_layer(temporal_length // 4))
        model.layer2._modules["1"].add_module(attention, se_layer(temporal_length // 4))

    # Add channel-temporal-wise SELayer
    elif attention == "SELayerCT":
        model.layer1._modules["0"].add_module(attention + "c", SELayerC(64))
        model.layer1._modules["1"].add_module(attention + "c", SELayerC(64))
        model.layer2._modules["0"].add_module(attention + "c", SELayerC(128))
        model.layer2._modules["1"].add_module(attention + "c", SELayerC(128))
        model.layer3._modules["0"].add_module(attention + "c", SELayerC(256))
        model.layer3._modules["1"].add_module(attention + "c", SELayerC(256))
        model.layer4._modules["0"].add_module(attention + "c", SELayerC(512))
        model.layer4._modules["1"].add_module(attention + "c", SELayerC(512))

        model.layer1._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer1._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer2._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 4))
        model.layer2._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 4))

    # Add temporal-channel-wise SELayer
    elif attention == "SELayerTC":
        model.layer1._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer1._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 2))
        model.layer2._modules["0"].add_module(attention + "t", SELayerT(temporal_length // 4))
        model.layer2._modules["1"].add_module(attention + "t", SELayerT(temporal_length // 4))

        model.layer1._modules["0"].add_module(attention + "c", SELayerC(64))
        model.layer1._modules["1"].add_module(attention + "c", SELayerC(64))
        model.layer2._modules["0"].add_module(attention + "c", SELayerC(128))
        model.layer2._modules["1"].add_module(attention + "c", SELayerC(128))
        model.layer3._modules["0"].add_module(attention + "c", SELayerC(256))
        model.layer3._modules["1"].add_module(attention + "c", SELayerC(256))
        model.layer4._modules["0"].add_module(attention + "c", SELayerC(512))
        model.layer4._modules["1"].add_module(attention + "c", SELayerC(512))

    else:
        raise ValueError("Wrong MODEL.ATTENTION. Current:{}".format(attention))

    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        state_dict.pop("stem.0.weight")
        model.load_state_dict(state_dict, strict=False)
    return model


def se_r3d_18_rgb(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_rgb(
        "r3d_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
        **kwargs,
    )


def se_r3d_18_flow(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_flow(
        "r3d_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicFLowStem,
        **kwargs,
    )


def se_mc3_18_rgb(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_rgb(
        "mc3_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] + [Conv3DNoTemporal] * 3,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
        **kwargs,
    )


def se_mc3_18_flow(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_flow(
        "mc3_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv3DSimple] + [Conv3DNoTemporal] * 3,
        layers=[2, 2, 2, 2],
        stem=BasicFLowStem,
        **kwargs,
    )


def se_r2plus1d_18_rgb(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_rgb(
        "r2plus1d_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv2Plus1D] * 4,
        layers=[2, 2, 2, 2],
        stem=R2Plus1dStem,
        **kwargs,
    )


def se_r2plus1d_18_flow(attention, pretrained=False, progress=True, **kwargs):
    return _se_video_resnet_flow(
        "r2plus1d_18",
        attention,
        pretrained,
        progress,
        block=BasicBlock,
        conv_makers=[Conv2Plus1D] * 4,
        layers=[2, 2, 2, 2],
        stem=R2Plus1dFlowStem,
        **kwargs,
    )


def se_r3d(attention, rgb=False, flow=False, pretrained=False, progress=True):
    """Get R3D_18 models with SELayers for different inputs.

    Args:
        attention (string): the name of the SELayer.
        rgb (bool): choose if RGB model is needed. (Default: False)
        flow (bool): choose if optical flow model is needed. (Default: False)
        pretrained (bool): choose if pretrained parameters are used. (Default: False)
        progress (bool, optional): whether or not to display a progress bar to stderr. (Default: True)

    Returns:
        models (dictionary): A dictionary contains models for RGB and optical flow.
    """

    r3d_rgb = r3d_flow = None
    if rgb:
        r3d_rgb = se_r3d_18_rgb(attention=attention, pretrained=pretrained, progress=progress)
    if flow:
        r3d_flow = se_r3d_18_flow(attention=attention, pretrained=pretrained, progress=progress)
    models = {"rgb": r3d_rgb, "flow": r3d_flow}
    return models


def se_mc3(attention, rgb=False, flow=False, pretrained=False, progress=True):
    """Get MC3_18 models with SELayers for different inputs."""
    mc3_rgb = mc3_flow = None
    if rgb:
        mc3_rgb = se_mc3_18_rgb(attention=attention, pretrained=pretrained, progress=progress)
    if flow:
        mc3_flow = se_mc3_18_flow(attention=attention, pretrained=pretrained, progress=progress)
    models = {"rgb": mc3_rgb, "flow": mc3_flow}
    return models


def se_r2plus1d(attention, rgb=False, flow=False, pretrained=False, progress=True):
    """Get R2+1D_18 models with SELayers for different inputs."""
    r2plus1d_rgb = r2plus1d_flow = None
    if rgb:
        r2plus1d_rgb = se_r2plus1d_18_rgb(attention=attention, pretrained=pretrained, progress=progress)
    if flow:
        r2plus1d_flow = se_r2plus1d_18_flow(attention=attention, pretrained=pretrained, progress=progress)
    models = {"rgb": r2plus1d_rgb, "flow": r2plus1d_flow}
    return models
</file>

<file path="kale/embed/video_selayer.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
# =============================================================================

"""Python implementation of Squeeze-and-Excitation Layers (SELayer)
Initial implementation: channel-wise (SELayerC)
Improved implementation: temporal-wise (SELayerT), convolution-based channel-wise (SELayerCoC), max-pooling-based
channel-wise (SELayerMC), multi-pooling-based channel-wise (SELayerMAC)

[Redundancy and repeat of code will be reduced in the future.]

References:
    Hu Jie, Li Shen, and Gang Sun. "Squeeze-and-excitation networks." In CVPR, pp. 7132-7141. 2018.
    For initial implementation, please go to https://github.com/hujie-frank/SENet
"""

import torch
import torch.nn as nn


def get_selayer(attention):
    """Get SELayers referring to attention.

    Args:
        attention (string): the name of the SELayer.
            (Options: ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerMAC"])

    Returns:
        se_layer (SELayer, optional): the SELayer.
    """
    if attention == "SELayerC":
        se_layer = SELayerC
    elif attention == "SELayerCoC":
        se_layer = SELayerCoC
    elif attention == "SELayerMC":
        se_layer = SELayerMC
    elif attention == "SELayerMAC":
        se_layer = SELayerMAC
    elif attention == "SELayerCoC":
        se_layer = SELayerCoC
    elif attention == "SELayerT":
        se_layer = SELayerT
    else:
        raise ValueError("Wrong MODEL.ATTENTION. Current:{}".format(attention))
    return se_layer


class SELayer(nn.Module):
    """Helper class for SELayer design."""

    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.channel = channel
        self.reduction = reduction

    def forward(self, x):
        return NotImplementedError()


class SELayerC(SELayer):
    """Construct channel-wise SELayer."""

    def __init__(self, channel, reduction=16):
        super(SELayerC, self).__init__(channel, reduction)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(self.channel, self.channel // self.reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(self.channel // self.reduction, self.channel, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, c, _, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1, 1)
        # out = x * y.expand_as(x)
        y = y - 0.5
        out = x + x * y.expand_as(x)
        return out


class SELayerT(SELayer):
    """Construct temporal-wise SELayer."""

    def __init__(self, channel, reduction=2):
        super(SELayerT, self).__init__(channel, reduction)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(self.channel, self.channel // self.reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(self.channel // self.reduction, self.channel, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, _, t, _, _ = x.size()
        output = x.transpose(1, 2).contiguous()
        y = self.avg_pool(output).view(b, t)
        y = self.fc(y).view(b, t, 1, 1, 1)
        y = y.transpose(1, 2).contiguous()
        # out = x * y.expand_as(x)
        y = y - 0.5
        out = x + x * y.expand_as(x)
        return out


class SELayerCoC(SELayer):
    """Construct convolution-based channel-wise SELayer."""

    def __init__(self, channel, reduction=16):
        super(SELayerCoC, self).__init__(channel, reduction)
        self.conv1 = nn.Conv3d(
            in_channels=self.channel, out_channels=self.channel // self.reduction, kernel_size=1, bias=False
        )
        self.bn1 = nn.BatchNorm3d(num_features=self.channel // self.reduction)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.sigmoid = nn.Sigmoid()
        self.conv2 = nn.Conv3d(
            in_channels=self.channel // self.reduction, out_channels=self.channel, kernel_size=1, bias=False
        )
        self.bn2 = nn.BatchNorm3d(num_features=self.channel)

    def forward(self, x):
        b, c, t, _, _ = x.size()  # n, c, t, h, w
        y = self.conv1(x)  # n, c/r, t, h, w
        y = self.bn1(y)  # n, c/r, t, h, w
        y = self.avg_pool(y)  # n, c/r, 1, 1, 1
        y = self.conv2(y)  # n, c, 1, 1, 1
        y = self.bn2(y)  # n, c, 1, 1, 1
        y = self.sigmoid(y)  # n, c, 1, 1, 1
        # out = x * y.expand_as(x)  # n, c, t, h, w
        y = y - 0.5
        out = x + x * y.expand_as(x)
        return out


class SELayerMC(SELayer):
    """Construct channel-wise SELayer with max pooling."""

    def __init__(self, channel, reduction=16):
        super(SELayerMC, self).__init__(channel, reduction)
        self.max_pool = nn.AdaptiveMaxPool3d(1)
        self.fc = nn.Sequential(
            nn.Linear(self.channel, self.channel // self.reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(self.channel // self.reduction, self.channel, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, c, _, _, _ = x.size()
        y = self.max_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1, 1)
        # out = x * y.expand_as(x)
        y = y - 0.5
        out = x + x * y.expand_as(x)
        return out


class SELayerMAC(SELayer):
    """Construct channel-wise SELayer with the mix of average pooling and max pooling."""

    def __init__(self, channel, reduction=16):
        super(SELayerMAC, self).__init__(channel, reduction)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.max_pool = nn.AdaptiveMaxPool3d(1)
        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1, 2), bias=False)
        self.fc = nn.Sequential(
            nn.Linear(self.channel, self.channel // self.reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(self.channel // self.reduction, self.channel, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, c, _, _, _ = x.size()
        y_avg = self.avg_pool(x)
        y_max = self.max_pool(x)
        y = torch.cat((y_avg, y_max), dim=2).squeeze().unsqueeze(dim=1)
        y = self.conv(y).squeeze()
        y = self.fc(y).view(b, c, 1, 1, 1)
        # out = x * y.expand_as(x)
        y = y - 0.5
        out = x + x * y.expand_as(x)
        return out
</file>

<file path="kale/evaluate/cross_validation.py">
"""
Functions implementing cross-validation methods for assessing model fit.
"""

import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.preprocessing import OneHotEncoder


def leave_one_group_out(x, y, groups, estimator, use_domain_adaptation=False) -> dict:
    """
    Perform leave one group out cross validation for a given estimator.

    Args:
        x (np.ndarray or torch.tensor): Input data [n_samples, n_features].
        y (np.ndarray or torch.tensor): Target labels [n_samples].
        groups (np.ndarray or torch.tensor): Group labels to be left out [n_samples].
        estimator (estimator object): Machine learning estimator to be evaluated from kale or scikit-learn.
        use_domain_adaptation (bool): Whether to use domain adaptation, i.e., leveraging test data, during training.

    Returns:
        dict: A dictionary containing results for each target group with 3 keys.
            - 'Target': A list of unique target groups or classes. The final entry is "Average".
            - 'Num_samples': A list where each entry indicates the number of samples in its corresponding target group.
                            The final entry represents the total number of samples.
            - 'Accuracy': A list where each entry indicates the accuracy score for its corresponding target group.
                            The final entry represents the overall mean accuracy.
    """
    enc = OneHotEncoder(handle_unknown="ignore")
    group_mat = enc.fit_transform(groups.reshape(-1, 1)).toarray()
    target, num_samples, accuracy = np.unique(groups).tolist(), [], []
    y_pred = np.zeros(y.shape)  # Store all predicted labels to compute accuracy

    for train, test in LeaveOneGroupOut().split(x, y, groups=groups):
        x_src, x_tgt = x[train], x[test]
        y_src, y_tgt = y[train], y[test]

        if use_domain_adaptation:
            estimator.fit(np.concatenate((x_src, x_tgt)), y_src, np.concatenate((group_mat[train], group_mat[test])))
        else:
            estimator.fit(x_src, y_src)

        y_pred[test] = estimator.predict(x_tgt)
        num_samples.append(x_tgt.shape[0])
        accuracy.append(accuracy_score(y_tgt, y_pred[test]))

    target.append("Average")
    num_samples.append(x.shape[0])
    accuracy.append(accuracy_score(y, y_pred))

    return {
        "Target": target,
        "Num_samples": num_samples,
        "Accuracy": accuracy,
    }
</file>

<file path="kale/evaluate/metrics.py">
# =============================================================================
# Author: Shuo Zhou, sz144@outlook.com
#         Sina Tabakhi, sina.tabakhi@gmail.com
#         Peizhen Bai, https://github.com/peizhenbai
# =============================================================================

"""Commonly used metrics, losses, and distances, part from domain adaptation package
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/losses.py
"""
from enum import Enum

import torch
import torch.nn as nn
from sklearn import metrics
from torch.autograd import grad
from torch.nn import functional as F


def cross_entropy_logits(output, target, weights=None):
    """Computes cross entropy with logits

    Args:
        output (Tensor): The output of the last layer of the network, before softmax.
        target (Tensor): The ground truth label.
        weights (Tensor, optional): The weight of each sample. Defaults to None.

    Examples:
        See DANN, WDGRL, and MMD trainers in kale.pipeline.domain_adapter
    """

    class_output = F.log_softmax(output, dim=1)
    max_class = class_output.max(1)
    y_hat = max_class[1]  # get the index of the max log-probability
    correct = y_hat.eq(target.view(target.size(0)).type_as(y_hat))
    if weights is None:
        loss = nn.NLLLoss()(class_output, target.type_as(y_hat).view(target.size(0)))
    else:
        losses = nn.NLLLoss(reduction="none")(class_output, target.type_as(y_hat).view(target.size(0)))
        loss = torch.sum(weights * losses) / torch.sum(weights)
    return loss, correct


def topk_accuracy(output, target, topk=(1,)):
    """Computes the top-k accuracy for the specified values of k.

    Args:
        output (Tensor): The output of the last layer of the network, before softmax. Shape: (batch_size, class_count).
        target (Tensor): The ground truth label. Shape: (batch_size)
        topk (tuple(int)): Compute accuracy at top-k for the values of k specified in this parameter.
    Returns:
        list(Tensor): A list of tensors of the same length as topk.
        Each tensor consists of boolean variables to show if this prediction ranks top k with each value of k.
        True means the prediction ranks top k and False means not.
        The shape of tensor is batch_size, i.e. the number of predictions.

    Examples:
        >>> output = torch.tensor(([0.3, 0.2, 0.1], [0.3, 0.2, 0.1]))
        >>> target = torch.tensor((0, 1))
        >>> top1, top2 = topk_accuracy(output, target, topk=(1, 2)) # get the boolean value
        >>> top1_value = top1.double().mean() # get the top 1 accuracy score
        >>> top2_value = top2.double().mean() # get the top 2 accuracy score
    """

    maxk = max(topk)

    # returns the k largest elements and their indexes of inputs along a given dimension.
    _, pred = output.topk(maxk, 1, True, True)

    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    result = []
    for k in topk:
        correct_k = torch.ge(correct[:k].float().sum(0), 1)
        result.append(correct_k)
    return result


def multitask_topk_accuracy(output, target, topk=(1,)):
    """Computes the top-k accuracy for the specified values of k for multitask input.

    Args:
        output (tuple(Tensor)): A tuple of generated predictions. Each tensor is of shape [batch_size, class_count],
            class_count can vary per task basis, i.e. outputs[i].shape[1] can differ from outputs[j].shape[1].
        target (tuple(Tensor)): A tuple of ground truth. Each tensor is of shape [batch_size]
        topk (tuple(int)): Compute accuracy at top-k for the values of k specified in this parameter.
    Returns:
        list(Tensor): A list of tensors of the same length as topk.
        Each tensor consists of boolean variables to show if predictions of multitask ranks top k with each value of k.
        True means predictions of this output for all tasks ranks top k and False means not.
        The shape of tensor is batch_size, i.e. the number of predictions.

        Examples:
            >>> first_output = torch.tensor(([0.3, 0.2, 0.1], [0.3, 0.2, 0.1]))
            >>> first_target = torch.tensor((0, 2))
            >>> second_output = torch.tensor(([0.2, 0.1], [0.2, 0.1]))
            >>> second_target = torch.tensor((0, 1))
            >>> output = (first_output, second_output)
            >>> target = (first_target, second_target)
            >>> top1, top2 = multitask_topk_accuracy(output, target, topk=(1, 2)) # get the boolean value
            >>> top1_value = top1.double().mean() # get the top 1 accuracy score
            >>> top2_value = top2.double().mean() # get the top 2 accuracy score
    """

    maxk = max(topk)
    batch_size = target[0].size(0)
    task_count = len(output)
    all_correct = torch.zeros(maxk, batch_size).type(torch.ByteTensor).to(output[0].device)

    for output, target in zip(output, target):
        # returns the k largest elements and their indexes of inputs along a given dimension.
        _, pred = output.topk(maxk, 1, True, True)

        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))
        all_correct.add_(correct)

    result = []
    for k in topk:
        all_correct_k = torch.ge(all_correct[:k].float().sum(0), task_count)
        result.append(all_correct_k)
    return result


def entropy_logits(linear_output):
    """Computes entropy logits in CDAN with entropy conditioning (CDAN+E)

    Examples:
        See CDANTrainer in kale.pipeline.domain_adapter
    """
    p = F.softmax(linear_output, dim=1)
    loss_ent = -torch.sum(p * (torch.log(p + 1e-5)), dim=1)
    return loss_ent


def entropy_logits_loss(linear_output):
    """Computes entropy logits loss in semi-supervised or few-shot domain adaptation

    Examples:
        See FewShotDANNTrainer in kale.pipeline.domain_adapter
    """
    return torch.mean(entropy_logits(linear_output))


def gradient_penalty(critic, h_s, h_t):
    """Computes gradient penalty in Wasserstein distance guided representation learning

    Examples:
        See WDGRLTrainer and WDGRLTrainerMod in kale.pipeline.domain_adapter
    """

    alpha = torch.rand(h_s.size(0), 1)
    alpha = alpha.expand(h_s.size()).type_as(h_s)
    # try:
    differences = h_t - h_s

    interpolates = h_s + (alpha * differences)
    interpolates = torch.cat((interpolates, h_s, h_t), dim=0).requires_grad_()

    preds = critic(interpolates)
    gradients = grad(
        preds,
        interpolates,
        grad_outputs=torch.ones_like(preds),
        retain_graph=True,
        create_graph=True,
    )[0]
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()
    # except:
    #     gradient_penalty = 0

    return gradient_penalty


def gaussian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
    """
    Code from XLearn: computes the full kernel matrix, which is less than optimal since we don't use all of it
    with the linear MMD estimate.

    Examples:
        See DANTrainer and JANTrainer in kale.pipeline.domain_adapter
    """
    n_samples = int(source.size()[0]) + int(target.size()[0])
    total = torch.cat([source, target], dim=0)
    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    l2_distance = ((total0 - total1) ** 2).sum(2)
    if fix_sigma:
        bandwidth = fix_sigma
    else:
        bandwidth = torch.sum(l2_distance.data) / (n_samples**2 - n_samples)
    bandwidth /= kernel_mul ** (kernel_num // 2)
    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
    kernel_val = [torch.exp(-l2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
    return sum(kernel_val)  # /len(kernel_val)


def compute_mmd_loss(kernel_values, batch_size):
    """Computes the Maximum Mean Discrepancy (MMD) between domains.

    Examples:
        See DANTrainer and JANTrainer in kale.pipeline.domain_adapter
    """
    loss = 0
    for i in range(batch_size):
        s1, s2 = i, (i + 1) % batch_size
        t1, t2 = s1 + batch_size, s2 + batch_size
        loss += kernel_values[s1, s2] + kernel_values[t1, t2]
        loss -= kernel_values[s1, t2] + kernel_values[s2, t1]
    return loss / float(batch_size)


def hsic(kx, ky, device):
    """
    Perform independent test with Hilbert-Schmidt Independence Criterion (HSIC) between two sets of variables x and y.

    Args:
        kx (2-D tensor): kernel matrix of x, shape (n_samples, n_samples)
        ky (2-D tensor): kernel matrix of y, shape (n_samples, n_samples)
        device (torch.device): the desired device of returned tensor

    Returns:
        [tensor]: Independent test score >= 0

    Reference:
        [1] Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Sch√∂lkopf, Bernhard. Measuring Statistical Dependence
            with Hilbert-Schmidt Norms. In Algorithmic Learning Theory (ALT), pp. 63‚Äì77. 2005.
        [2] Gretton, Arthur, Fukumizu, Kenji, Teo, Choon H., Song, Le, Sch√∂lkopf, Bernhard, and Smola, Alex J. A Kernel
            Statistical Test of Independence. In Advances in Neural Information Processing Systems, pp. 585‚Äì592. 2008.
    """

    n = kx.shape[0]
    if ky.shape[0] != n:
        raise ValueError("kx and ky are expected to have the same sample sizes.")
    ctr_mat = torch.eye(n, device=device) - torch.ones((n, n), device=device) / n
    return torch.trace(torch.mm(torch.mm(torch.mm(kx, ctr_mat), ky), ctr_mat)) / (n**2)


def euclidean(x1, x2):
    """
    Compute the Euclidean distance between two sets of variables.

    Args:
        x1 (torch.Tensor): variables set 1
        x2 (torch.Tensor): variables set 2

    Returns:
        torch.Tensor: Euclidean distance
    """
    return ((x1 - x2) ** 2).sum().sqrt()


def _moment_k(x: torch.Tensor, domain_labels: torch.Tensor, k_order=2):
    """Compute the k-th moment distance

    Args:
        x (torch.Tensor): input data, shape (n_samples, n_features)
        domain_labels (torch.Tensor): labels indicating which domain the instance is from, shape (n_samples,)
        k_order (int, optional): moment order. Defaults to 2.

    Returns:
        torch.Tensor: the k-th moment distance

    The code is based on:
        https://github.com/KaiyangZhou/Dassl.pytorch/blob/master/dassl/engine/da/m3sda.py#L153
        https://github.com/VisionLearningGroup/VisionLearningGroup.github.io/blob/master/M3SDA/code_MSDA_digit/metric/msda.py#L6
    """
    unique_domain_ = torch.unique(domain_labels)
    n_unique_domain_ = len(unique_domain_)
    x_k_order = []
    for domain_label_ in unique_domain_:
        domain_idx = torch.where(domain_labels == domain_label_)[0]
        x_mean = x[domain_idx].mean(0)
        if k_order == 1:
            x_k_order.append(x_mean)
        else:
            x_k_order.append(((x[domain_idx] - x_mean) ** k_order).mean(0))
    moment_sum = 0
    n_pair = 0
    for i in range(n_unique_domain_):
        for j in range(i + 1, n_unique_domain_):
            moment_sum += euclidean(x_k_order[i], x_k_order[j])
            n_pair += 1
    return moment_sum / n_pair


class protonet_loss:
    """ProtoNet loss function.

    This is a loss function for prototypical networks. It computes the loss and accuracy of the model by measuring the Euclidean distance between features of samples in support and query sets.
    Because this loss requests some constant hyperparameters, it is not a general function but defined as a class.

    - :math:`N`-way: The number of classes under a particular setting. The model is presented with samples from these :math:`N` classes and needs to classify them. For example, 3-way means the model has to classify 3 different classes.

    - :math:`K`-shot: The number of samples for each class in the support set. For example, in a 2-shot setting, two support samples are provided per class.

    - Support set: It is a small, labeled dataset used to train the model with a few samples of each class. The support set consists of :math:`N` classes (:math:`N`-way), with :math:`K` samples (:math:`K`-shot) for each class. For example, under a 3-way-2-shot setting, the support set has 3 classes with 2 samples per class, totaling 6 samples.

    - Query set: It evaluates the model's ability to generalize what it has learned from the support set. It contains samples from the same :math:`N` classes but not included in the support set. Continuing with the 3-way-2-shot example, the query set would include additional samples from the 3 classes, which the model must classify after learning from the support set.

    Args:
        num_classes (int): Number of classes in a task. Default: 5
        num_query_samples (int): Number of samples per class in the query set. Default: 15
        device (torch.device): The device in computation. Default: torch.device("cuda")

    Examples:
        >>> loss_fn = protonet_loss(num_classes=5, num_query_samples=15, device=torch.device("cuda"))
        >>> loss, acc = loss_fn(feature_support, feature_query)

    Reference:
        Snell, J., Swersky, K. and Zemel, R., 2017. Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems, 30.
    """

    def __init__(
        self, num_classes: int = 5, num_query_samples: int = 15, device: torch.device = torch.device("cuda")
    ) -> None:
        super().__init__()
        self.num_classes = num_classes
        self.num_query_samples = num_query_samples
        self.device = device

    def __call__(self, feature_support: torch.Tensor, feature_query: torch.Tensor) -> tuple:
        """
        Args:
            feature_support (torch.Tensor): Feature vectors of support samples: (num_classes, k_shot, feature_dim)
            feature_query (torch.Tensor): Feature vectors of query samples: (num_classes * num_query_samples, feature_dim)

        Returns:
            tuple: (loss, acc)
            loss (torch.Tensor): Loss value
            acc (torch.Tensor): Accuracy value
        """
        feature_support = feature_support.to(self.device)
        feature_query = feature_query.to(self.device)
        prototypes = feature_support.mean(dim=1)
        dists = self.euclidean_dist_for_tensor_group(feature_query, prototypes)
        log_p_y = F.log_softmax(-dists, dim=1)
        log_p_y = log_p_y.view(self.num_classes, self.num_query_samples, -1)
        labels = torch.arange(self.num_classes).to(self.device)
        labels = labels.view(self.num_classes, 1, 1)
        labels = labels.expand(self.num_classes, self.num_query_samples, 1).long()
        loss = -log_p_y.gather(2, labels).squeeze().view(-1).mean()
        _, y_hat = log_p_y.max(2)
        acc = torch.eq(y_hat, labels.squeeze()).float().mean()
        return loss, acc

    def euclidean_dist_for_tensor_group(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """
        Compute the Euclidean distance between two batches.

        Args:
            x (torch.Tensor): Variables set 1: (N, D)
            y (torch.Tensor): Variables set 2: (M, D)

        Returns:
            torch.Tensor: Euclidean distance: (N, M)
        """
        n = x.size(0)
        m = y.size(0)
        d = x.size(1)
        if d != y.size(1):
            raise Exception
        x = x.unsqueeze(1).expand(n, m, d)
        y = y.unsqueeze(0).expand(n, m, d)
        return torch.pow(x - y, 2).sum(2)


def auprc_auroc_ap(target: torch.Tensor, score: torch.Tensor):
    """
    auprc: area under the precision-recall curve
    auroc: area under the receiver operating characteristic curve
    ap: average precision

    Copy-paste from https://github.com/NYXFLOWER/GripNet
    """
    y = target.detach().cpu().numpy()
    pred = score.detach().cpu().numpy()
    auroc, ave_precision = metrics.roc_auc_score(y, pred), metrics.average_precision_score(y, pred)
    precision, recall, _ = metrics.precision_recall_curve(y, pred)
    auprc = metrics.auc(recall, precision)

    return auprc, auroc, ave_precision


def concord_index(y, y_pred):
    """
    Calculate the Concordance Index (CI), which is a metric to measure the proportion of `concordant pairs
    <https://en.wikipedia.org/wiki/Concordant_pair>`_ between real and
    predict values.

    Args:
        y (array): real values.
        y_pred (array): predicted values.
    """
    total_loss = 0
    pair = 0
    for i in range(1, len(y)):
        for j in range(0, i):
            if i is not j:
                if y[i] > y[j]:
                    pair += 1
                    total_loss += 1 * (y_pred[i] > y_pred[j]) + 0.5 * (y_pred[i] == y_pred[j])

    if pair:
        return total_loss / pair
    else:
        return 0


class DistanceMetric(Enum):
    COSINE = "COSINE"


def calculate_distance(
    x1: torch.Tensor, x2: torch.Tensor = None, eps: float = 1e-8, metric: DistanceMetric = DistanceMetric.COSINE
) -> torch.Tensor:
    r"""Returns similarity between :math:`x_1` and :math:`x_2`, computed along `dim`=1. This method calculates the
    similarity between each pair of data points in two input matrices.

    Note that this implementation differs from the existing implementations in
    `PyTorch <https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html>`__, as they calculate the
    similarity between each row of one matrix with its corresponding row in the other matrix (i.e., pairwise distance
    between columns of input matrices).

    Args:
        x1 (torch.Tensor): The tensor input data.
        x2 (torch.Tensor, optional): The tensor input data. (default ``None``)
        eps (float, optional): Small value to avoid division by zero. (default: 1e-8)
        metric (DistanceMetric, optional): The metric to compute distance between input matrices. (default: ``DistanceMetric.COSINE``)

        Returns:
        torch.Tensor: The computed similarity tensor between :math:`x_1` and :math:`x_2`.
    """
    if metric == DistanceMetric.COSINE:
        x2 = x1 if x2 is None else x2
        w1 = torch.norm(x1, p=2, dim=1, keepdim=True)
        w2 = w1 if x2 is None else torch.norm(x2, p=2, dim=1, keepdim=True)
        return torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)

    raise Exception("This metric is not still implemented")
</file>

<file path="kale/evaluate/README.md">
# Evaluation modules

Performance evaluation using some metrices
</file>

<file path="kale/evaluate/similarity_metrics.py">
"""
Authors: Lawrence Schobs, lawrenceschobs@gmail.com

Functions related to similarity metrics including similarity measures and correlations.
"""

import logging
import os
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from kale.prepdata.tabular_transform import apply_confidence_inversion


def jaccard_similarity(list1: list, list2: list) -> float:
    """
    Calculates the Jaccard Index (JI) between two lists.

    Args:
        list1 (list): List of elements in set A.
        list2 (list): List of elements in set B.
    Returns:
        float: The Jaccard Index between list1 and list2.
    Example:
        >>> jaccard_similarity([1,2,3], [2,3,4])
        0.5
    """

    if len(list1) == 0 or len(list2) == 0:
        return 0
    else:
        intersection = len(set(list1).intersection(list2))  # no need to call list here
        union = len(list1 + list2) - intersection  # you only need to call len once here
        return intersection / union  # also no need to cast to float as this will be done for you


def evaluate_correlations(
    bin_predictions: Dict[str, pd.DataFrame],
    uncertainty_error_pairs: List[Tuple[str, str, str]],
    cmaps: List[Dict[Any, Any]],
    num_bins: int,
    confidence_invert_tuples: List[Tuple[str, bool]],
    num_folds: int = 8,
    error_scaling_factor: float = 1,
    combine_middle_bins: bool = False,
    save_path: Optional[str] = None,
    to_log: bool = False,
) -> Dict[str, Dict[str, Dict[str, Any]]]:
    """
    Calculates the correlation between error and uncertainty for each bin and for each target,
    using a piece-wise linear regression model.

    Designed for use in Quantile Binning (/pykale/examples/landmark_uncertainty/main.py).

    Args:
        bin_predictions: A dictionary of Pandas DataFrames containing model predictions for each testing fold.
        uncertainty_error_pairs: A list of tuples specifying the names of the uncertainty,
            error, and uncertainty inversion keys for each pair.
        cmaps: A dictionary of colour maps to use for plotting the results.
        num_bins: The number of quantile bins to divide the data into.
        confidence_invert_tuples: A list of tuples specifying whether to invert the uncertainty values for each method.
                          First element is a string specifying the uncertainty method name and the second element is
                          a boolean whether to invert e.g. [["E-MHA", True], ["E-CPV", False]]
        num_folds: The number of folds to use for cross-validation (default: 8).
        error_scaling_factor: The scale factor to transform error by (default: 1).
        combine_middle_bins: Whether to combine the middle bins into one bin (default: False).
        save_path: The path to save the correlation plots (default: None).
        to_log: Whether to use logarithmic scaling for the x and y axes of the plots (default: False).
    Returns:
        A dictionary containing the correlation statistics for each model and uncertainty method.
        The dictionary has the following structure:
        {
            <model_name>: {
                <uncertainty_name>: {
                    "all_folds": {
                        "r": <correlation coefficient>,
                        "p": <p-value>,
                        "fit_params": <regression line parameters>,
                        "ci": <confidence intervals for the regression line parameters>
                    },
                    "quantiles": {
                        <quantile_index>: {
                            "r": <correlation coefficient>,
                            "p": <p-value>,
                            "fit_params": <regression line parameters>,
                            "ci": <confidence intervals for the regression line parameters>
                        }
                    }
                }
            }
        }
        The "all_folds" key contains the correlation statistics for all testing folds combined.
        The "quantiles" key contains the correlation statistics for each quantile bin separately.
    """
    from kale.interpret.uncertainty_quantiles import fit_line_with_ci

    logger = logging.getLogger("qbin")
    # define dict to save correlations to.
    correlation_dict: Dict[str, Dict[str, Dict[str, Any]]] = {}

    num_bins_quantiles = num_bins
    # If we are combining the middle bins, we only have the 2 edge bins and the middle bins are combined into 1 bin.
    if combine_middle_bins:
        num_bins = 3

    # Loop over models (model) and uncertainty methods (uncert_pair)
    for _, (model, data_structs) in enumerate(bin_predictions.items()):
        correlation_dict[model] = {}
        for uncert_pair in uncertainty_error_pairs:  # uncert_pair = [pair name, error name , uncertainty name]
            uncertainty_type = uncert_pair[0]
            logger.info("All folds correlation for Model: %s, Uncertainty type %s :", uncertainty_type, model)
            fold_errors = np.array(
                data_structs[(data_structs["Testing Fold"].isin(np.arange(num_folds)))][
                    [uncertainty_type + " Error"]
                ].values.tolist()
            ).flatten()

            fold_uncertainty_values = data_structs[(data_structs["Testing Fold"].isin(np.arange(num_folds)))][
                [uncertainty_type + " Uncertainty"]
            ]

            invert_uncert_bool = [x[1] for x in confidence_invert_tuples if x[0] == uncertainty_type][0]
            if invert_uncert_bool:
                fold_uncertainty_values = apply_confidence_inversion(fold_uncertainty_values, uncert_pair[2])

            fold_uncertainty_values = np.array(fold_uncertainty_values.values.tolist()).flatten()

            # Get the quantiles of the data w.r.t to the uncertainty values.
            # Get the true uncertianty quantiles
            sorted_uncertainties = np.sort(fold_uncertainty_values)

            quantiles = np.arange(1 / num_bins_quantiles, 1, 1 / num_bins_quantiles)[: num_bins_quantiles - 1]
            quantile_thresholds = [np.quantile(sorted_uncertainties, q) for q in quantiles]

            # If we are combining the middle bins, combine the middle lists into 1 list.
            if combine_middle_bins:
                quantile_thresholds = [quantile_thresholds[0], quantile_thresholds[-1]]

            # fit a piece-wise linear regression line between quantiles, and return correlation values.
            save_path_fig = (
                os.path.join(save_path, model + "_" + uncertainty_type + "_correlation_pwr_all_targets.pdf")
                if save_path
                else None
            )
            corr_dict = fit_line_with_ci(
                fold_errors,
                fold_uncertainty_values,
                quantile_thresholds,
                cmaps,
                error_scaling_factor=error_scaling_factor,
                save_path=save_path_fig,
                to_log=to_log,
            )

            correlation_dict[model][uncertainty_type] = corr_dict

    return correlation_dict
</file>

<file path="kale/evaluate/uncertainty_metrics.py">
"""
Authors: Lawrence Schobs, lawrenceschobs@gmail.com
Module from the implementation of L. A. Schobs, A. J. Swift and H. Lu, "Uncertainty Estimation for Heatmap-Based Landmark Localization,"
in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034, April 2023, doi: 10.1109/TMI.2022.3222730.

Functions related to  evaluating the quantile binning method in terms of:
   A) Binning accuracy to ground truth bins: evaluate_jaccard, bin_wise_jaccard.
   B) Binning error bound accuracy: evaluate_bounds, bin_wise_bound_eval
   C) Binning attributes such as mean errors of bins (get_mean_errors, bin_wise_errors).

"""

from typing import Dict, List

import numpy as np
import pandas as pd

from kale.evaluate.similarity_metrics import jaccard_similarity
from kale.prepdata.string_transform import strip_for_bound


def evaluate_bounds(
    estimated_bounds: Dict[str, pd.DataFrame],
    bin_predictions: Dict[str, pd.DataFrame],
    uncertainty_pairs: List,
    num_bins: int,
    targets: List[int],
    num_folds: int = 8,
    show_fig: bool = False,
    combine_middle_bins: bool = False,
) -> Dict:
    """
    Evaluates error bounds for given uncertainty pairs and estimated bounds.

    Args:
        estimated_bounds (Dict[str, pd.DataFrame]): Dictionary of error bounds for each model.
        bin_predictions (Dict[str, pd.DataFrame]): Dictionary of bin predictions for each model.
        uncertainty_pairs (List[List[str]]): List of uncertainty pairs to be evaluated.
        num_bins (int): Number of bins to be used.
        targets (List[str]): List of targets to be evaluated.
        num_folds (int, optional): Number of folds for cross-validation. Defaults to 8.
        show_fig (bool, optional): Flag to show the figure. Defaults to False.
        combine_middle_bins (bool, optional): Flag to combine the middle bins. Defaults to False.

    Returns:
        Dict: Dictionary containing evaluation results.
    """

    if combine_middle_bins:
        num_bins = 3

    # Initialize results dicts
    all_bound_percents = {}
    all_bound_percents_notargetsep = {}

    all_concat_errorbound_bins_target_sep_foldwise = [{} for x in range(len(targets))]  # type: List[Dict]
    all_concat_errorbound_bins_target_sep_all = [{} for x in range(len(targets))]  # type: List[Dict]

    # Loop over combinations of models (model) and uncertainty types (uncert_pair)
    for i, (model, data_structs) in enumerate(bin_predictions.items()):
        error_bounds = estimated_bounds[model + " Error Bounds"]

        for uncert_pair in uncertainty_pairs:
            uncertainty_type = uncert_pair[0]

            fold_learned_bounds_mean_targets = []
            fold_learned_bounds_mean_bins = [[] for x in range(num_bins)]  # type: List[List]
            fold_learned_bounds_bins_targetsnotsep = [[] for x in range(num_bins)]  # type: List[List]
            fold_all_bins_concat_targets_sep_foldwise = [
                [[] for y in range(num_bins)] for x in range(len(targets))
            ]  # type: List[List]
            fold_all_bins_concat_targets_sep_all = [
                [[] for y in range(num_bins)] for x in range(len(targets))
            ]  # type: List[List]

            for fold in range(num_folds):
                # Get the ids for this fold
                fold_errors = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Error"]
                ]
                fold_bins = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Uncertainty bins"]
                ]
                fold_bounds = strip_for_bound(
                    error_bounds[error_bounds["fold"] == fold][uncertainty_type + " Uncertainty bounds"].values
                )

                return_dict = bin_wise_bound_eval(
                    fold_bounds,
                    fold_errors,
                    fold_bins,
                    targets,
                    uncertainty_type,
                    num_bins=num_bins,
                    show_fig=show_fig,
                )
                fold_learned_bounds_mean_targets.append(return_dict["mean all targets"])

                for idx_bin in range(len(return_dict["mean all bins"])):
                    fold_learned_bounds_mean_bins[idx_bin].append(return_dict["mean all bins"][idx_bin])
                    fold_learned_bounds_bins_targetsnotsep[idx_bin] = (
                        fold_learned_bounds_bins_targetsnotsep[idx_bin] + return_dict["mean all"][idx_bin]
                    )

                    for target_idx in range(len(targets)):
                        fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin] = (
                            fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin]
                            + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin]
                        )
                        combined = (
                            fold_all_bins_concat_targets_sep_all[target_idx][idx_bin]
                            + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin]
                        )

                        fold_all_bins_concat_targets_sep_all[target_idx][idx_bin] = combined

            # Reverses order so they are worst to best i.e. B5 -> B1
            all_bound_percents[model + " " + uncertainty_type] = fold_learned_bounds_mean_bins[::-1]
            all_bound_percents_notargetsep[model + " " + uncertainty_type] = fold_learned_bounds_bins_targetsnotsep[
                ::-1
            ]

            for target_idx in range(len(all_concat_errorbound_bins_target_sep_foldwise)):
                all_concat_errorbound_bins_target_sep_foldwise[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_foldwise[target_idx]
                all_concat_errorbound_bins_target_sep_all[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_all[target_idx]

    return {
        "Error Bounds All": all_bound_percents,
        "all_bound_percents_notargetsep": all_bound_percents_notargetsep,
        "all errorbound concat bins targets sep foldwise": all_concat_errorbound_bins_target_sep_foldwise,
        "all errorbound concat bins targets sep all": all_concat_errorbound_bins_target_sep_all,
    }


def bin_wise_bound_eval(
    fold_bounds_all_targets: list,
    fold_errors: pd.DataFrame,
    fold_bins: pd.DataFrame,
    targets: list,
    uncertainty_type: str,
    num_bins: int = 5,
    show_fig: bool = False,
) -> dict:
    """
    Helper function for `evaluate_bounds`. Evaluates the accuracy of estimated error bounds for each quantile bin
    for a given uncertainty type, over a single fold and for multiple targets.

    Args:
        fold_bounds_all_targets (list): A list of lists of estimated error bounds for each target.
        fold_errors (pd.DataFrame): A Pandas DataFrame containing the true errors for this fold.
        fold_bins (pd.DataFrame): A Pandas DataFrame containing the predicted quantile bins for this fold.
        targets (list): A list of targets to measure uncertainty estimation.
        uncertainty_type (str): The name of the uncertainty type to calculate accuracy for.
        num_bins (int): The number of quantile bins.
        show_fig (bool): Whether to show a figure depicting error bound accuracy (default=False).

    Returns:
        dict: A dictionary containing the following error bound accuracy statistics:
              - 'mean all targets': The mean accuracy over all targets and quantile bins.
              - 'mean all bins': A list of mean accuracy values for each quantile bin (all targets included).
              - 'mean all': A list of accuracy values for each quantile bin and target, weighted by # targets in each bin.
              - 'all bins concatenated targets separated': A list of accuracy values for each quantile bin, concatenated
               for each target separately.

    Example:
        >>> bin_wise_bound_eval(fold_bounds_all_targets, fold_errors, fold_bins, [0,1], 'S-MHA', num_bins=5)
    """
    all_target_perc = []
    all_qs_perc: List[List[float]] = [[] for x in range(num_bins)]  #
    all_qs_size: List[List[float]] = [[] for x in range(num_bins)]

    all_qs_errorbound_concat_targets_sep: List[List[List[float]]] = [
        [[] for y in range(num_bins)] for x in range(len(targets))
    ]

    for i_ti, target_idx in enumerate(targets):
        true_errors_ti = fold_errors[(fold_errors["target_idx"] == target_idx)][["uid", uncertainty_type + " Error"]]
        pred_bins_ti = fold_bins[(fold_errors["target_idx"] == target_idx)][
            ["uid", uncertainty_type + " Uncertainty bins"]
        ]

        # Zip to dictionary
        true_errors_ti = dict(zip(true_errors_ti.uid, true_errors_ti[uncertainty_type + " Error"]))
        pred_bins_ti = dict(zip(pred_bins_ti.uid, pred_bins_ti[uncertainty_type + " Uncertainty bins"]))

        # The error bounds are from B1 -> B5 i.e. best quantile of predictions to worst quantile of predictions
        fold_bounds = fold_bounds_all_targets[i_ti]

        # For each bin, see what % of targets are between the error bounds.
        # If bin=0 then lower bound = 0, if bin=Q then no upper bound
        # Keep track of #samples in each bin for weighted mean.

        # turn dictionary of predicted bins into [[num_bins]] array
        pred_bins_keys = []
        pred_bins_errors = []
        for i in range(num_bins):
            inner_list_bin = list([key for key, val in pred_bins_ti.items() if str(i) == str(val)])
            inner_list_errors = []

            for id_ in inner_list_bin:
                inner_list_errors.append(list([val for key, val in true_errors_ti.items() if str(key) == str(id_)])[0])

            pred_bins_errors.append(inner_list_errors)
            pred_bins_keys.append(inner_list_bin)

        bins_acc = []
        bins_sizes = []
        for q in range((num_bins)):
            inner_bin_correct = 0

            inbin_errors = pred_bins_errors[q]

            for error in inbin_errors:
                if q == 0:
                    lower = 0
                    upper = fold_bounds[q]

                    if error <= upper and error > lower:
                        inner_bin_correct += 1

                elif q < (num_bins) - 1:
                    lower = fold_bounds[q - 1]
                    upper = fold_bounds[q]

                    if error <= upper and error > lower:
                        inner_bin_correct += 1

                else:
                    lower = fold_bounds[q - 1]
                    upper = 999999999999999999999999999999

                    if error > lower:
                        inner_bin_correct += 1

            if inner_bin_correct == 0:
                accuracy_bin = 0.0
            elif len(inbin_errors) == 0:
                accuracy_bin = 1.0
            else:
                accuracy_bin = inner_bin_correct / len(inbin_errors)
            bins_sizes.append(len(inbin_errors))
            bins_acc.append(accuracy_bin)

            all_qs_perc[q].append(accuracy_bin)
            all_qs_size[q].append(len(inbin_errors))
            all_qs_errorbound_concat_targets_sep[i_ti][q].append(accuracy_bin)

        # Weighted average over all bins
        weighted_mean_ti = 0.0
        total_weights = 0.0
        for l_idx in range(len(bins_sizes)):
            bin_acc = bins_acc[l_idx]
            bin_size = bins_sizes[l_idx]
            weighted_mean_ti += bin_acc * bin_size
            total_weights += bin_size
        weighted_ave = weighted_mean_ti / total_weights
        all_target_perc.append(weighted_ave)

    # Weighted average for each of the quantile bins.
    weighted_ave_binwise = []
    for binidx in range(len(all_qs_perc)):
        bin_accs = all_qs_perc[binidx]
        bin_asizes = all_qs_size[binidx]

        weighted_mean_bin = 0.0
        total_weights_bin = 0.0
        for l_idx in range(len(bin_accs)):
            b_acc = bin_accs[l_idx]
            b_siz = bin_asizes[l_idx]
            weighted_mean_bin += b_acc * b_siz
            total_weights_bin += b_siz

        # Avoid div by 0
        if weighted_mean_bin == 0 or total_weights_bin == 0:
            weighted_ave_bin = 0.0
        else:
            weighted_ave_bin = weighted_mean_bin / total_weights_bin
        weighted_ave_binwise.append(weighted_ave_bin)

    # No weighted average, just normal average
    normal_ave_bin_wise = []
    for binidx in range(len(all_qs_perc)):
        bin_accs = all_qs_perc[binidx]
        normal_ave_bin_wise.append(np.mean(bin_accs))

    return {
        "mean all targets": np.mean(all_target_perc),
        "mean all bins": weighted_ave_binwise,
        "mean all": all_qs_perc,
        "all bins concatenated targets seperated": all_qs_errorbound_concat_targets_sep,
    }


def get_mean_errors(
    bin_predictions: Dict[str, "pd.DataFrame"],
    uncertainty_pairs: List,
    num_bins: int,
    targets: List[int],
    num_folds: int = 8,
    error_scaling_factor: float = 1.0,
    combine_middle_bins: bool = False,
) -> Dict:
    """
    Evaluate uncertainty estimation's mean error of each bin.
    For each bin, we calculate the mean localization error for each target and for all targets.
    We calculate the mean error for each dictionary in the bin_predictions dict. For each bin, we calculate: a) the mean
    and std over all folds and all targets b) the mean and std for each target over all folds.

    Args:
        bin_predictions (Dict): Dict of Pandas DataFrames where each DataFrame has errors, predicted bins for all
        uncertainty measures for a model.
        uncertainty_pairs (List[Tuple[str, str]]): List of tuples describing the different uncertainty combinations to test.
        num_bins (int): Number of quantile bins.
        targets (List[str]): List of targets to measure uncertainty estimation.
        num_folds (int, optional): Number of folds. Defaults to 8.
        error_scaling_factor (int, optional): Scale error factor. Defaults to 1.
        combine_middle_bins (bool, optional): Combine middle bins if True. Defaults to False.

    Returns:
        Dict[str, Union[Dict[str, List[List[float]]], List[Dict[str, List[float]]]]]: Dictionary with mean error for all
         targets combined and targets separated.
            Keys that are returned:
                "all mean error bins nosep":  For every fold, the mean error for each bin. All targets are combined in the same list.
                "all mean error bins targets sep":   For every fold, the mean error for each bin. Each target is in a separate list.
                "all error concat bins targets nosep":  For every fold, every error value in a list. Each target is in the same list. The list is flattened for all the folds.
                "all error concat bins targets sep foldwise":  For every fold, every error value in a list. Each target is in a separate list. Each list has a list of results by fold.
                "all error concat bins targets sep all": For every fold, every error value in a list. Each target is in a separate list. The list is flattened for all the folds.

    """
    # If we are combining the middle bins, we only have the 2 edge bins and the middle bins are combined into 1 bin.
    if combine_middle_bins:
        num_bins = 3

    # initialize empty dicts
    all_mean_error_bins = {}
    all_mean_error_bins_targets_sep = {}
    all_concat_error_bins_target_sep_foldwise: List[Dict] = [{} for x in range(len(targets))]
    all_concat_error_bins_target_sep_all: List[Dict] = [{} for x in range(len(targets))]

    all_concat_error_bins_target_nosep = {}
    # Loop over models (model) and uncertainty methods (uncert_pair)
    for i, (model, data_structs) in enumerate(bin_predictions.items()):
        for uncert_pair in uncertainty_pairs:  # uncert_pair = [pair name, error name , uncertainty name]
            uncertainty_type = uncert_pair[0]

            # Initialize lists to store fold-wise results
            fold_mean_targets = []
            fold_mean_bins: List[List[float]] = [[] for x in range(num_bins)]
            fold_all_bins: List[List[float]] = [[] for x in range(num_bins)]
            fold_all_bins_concat_targets_sep_foldwise: List[List[List[float]]] = [
                [[] for y in range(num_bins)] for x in range(len(targets))
            ]
            fold_all_bins_concat_targets_sep_all: List[List[List[float]]] = [
                [[] for y in range(num_bins)] for x in range(len(targets))
            ]

            fold_all_bins_concat_targets_nosep: List[List[float]] = [[] for x in range(num_bins)]

            for fold in range(num_folds):
                # Get the errors and predicted bins for this fold
                fold_errors = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Error"]
                ]
                fold_bins = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Uncertainty bins"]
                ]

                return_dict = bin_wise_errors(
                    fold_errors,
                    fold_bins,
                    num_bins,
                    targets,
                    uncertainty_type,
                    error_scaling_factor=error_scaling_factor,
                )
                fold_mean_targets.append(return_dict["mean all targets"])

                for idx_bin in range(len(return_dict["mean all bins"])):
                    fold_mean_bins[idx_bin].append(return_dict["mean all bins"][idx_bin])
                    fold_all_bins[idx_bin] = fold_all_bins[idx_bin] + return_dict["all bins"][idx_bin]

                    concat_no_sep = [x[idx_bin] for x in return_dict["all bins concatenated targets seperated"]]

                    flattened_concat_no_sep = [x for sublist in concat_no_sep for x in sublist]
                    flattened_concat_no_sep = [x for sublist in flattened_concat_no_sep for x in sublist]

                    fold_all_bins_concat_targets_nosep[idx_bin] = (
                        fold_all_bins_concat_targets_nosep[idx_bin] + flattened_concat_no_sep
                    )

                    for target_idx in range(len(targets)):
                        fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin] = (
                            fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin]
                            + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin]
                        )

                        if return_dict["all bins concatenated targets seperated"][target_idx][idx_bin] != []:
                            combined = (
                                fold_all_bins_concat_targets_sep_all[target_idx][idx_bin]
                                + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin][0]
                            )
                        else:
                            combined = fold_all_bins_concat_targets_sep_all[target_idx][idx_bin]

                        fold_all_bins_concat_targets_sep_all[target_idx][idx_bin] = combined

            # reverse orderings
            fold_mean_bins = fold_mean_bins[::-1]
            fold_all_bins = fold_all_bins[::-1]
            fold_all_bins_concat_targets_nosep = fold_all_bins_concat_targets_nosep[::-1]
            fold_all_bins_concat_targets_sep_foldwise = [x[::-1] for x in fold_all_bins_concat_targets_sep_foldwise]
            fold_all_bins_concat_targets_sep_all = [x[::-1] for x in fold_all_bins_concat_targets_sep_all]

            all_mean_error_bins[model + " " + uncertainty_type] = fold_mean_bins
            all_mean_error_bins_targets_sep[model + " " + uncertainty_type] = fold_all_bins

            all_concat_error_bins_target_nosep[model + " " + uncertainty_type] = fold_all_bins_concat_targets_nosep

            for target_idx in range(len(fold_all_bins_concat_targets_sep_foldwise)):
                all_concat_error_bins_target_sep_foldwise[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_foldwise[target_idx]
                all_concat_error_bins_target_sep_all[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_all[target_idx]

    return {
        "all mean error bins nosep": all_mean_error_bins,
        "all mean error bins targets sep": all_mean_error_bins_targets_sep,
        "all error concat bins targets nosep": all_concat_error_bins_target_nosep,
        "all error concat bins targets sep foldwise": all_concat_error_bins_target_sep_foldwise,
        "all error concat bins targets sep all": all_concat_error_bins_target_sep_all,
    }


def evaluate_jaccard(bin_predictions, uncertainty_pairs, num_bins, targets, num_folds=8, combine_middle_bins=False):
    """
        Evaluate uncertainty estimation's ability to predict true error quantiles.
        For each bin, we calculate the jaccard index (JI) between the pred bins and GT error quantiles.
        We calculate the JI for each dictionary in the bin_predictions dict. For each bin, we calculate: a) the mean and
        std over all folds and all targets b) the mean and std for each target over all folds.

    Args:
        bin_predictions (Dict): dict of Pandas Dataframes where each dataframe has errors, predicted bins for all uncertainty measures for a model,
        uncertainty_pairs ([list]): list of lists describing the different uncert combinations to test,
        num_bins (int): Number of quantile bins,
        targets (list) list of targets to measure uncertainty estimation,
        num_folds (int): Number of folds,


    Returns:
        [Dict]: Dicts with JI for all targets combined and targets seperated.
    """

    # If combining middle bins, we need the original number of bins to calcualate the true quantiles of the error.
    # Then, we combine all the middle bins of the true quantiles, giving us 3 bins.
    # There are only 3 bins that have been predicted, so set num_bins to 3.

    if combine_middle_bins:
        num_bins_for_quantiles = num_bins
        num_bins = 3
    else:
        num_bins_for_quantiles = num_bins
    # initialize empty dicts
    all_jaccard_data = {}
    all_jaccard_bins_targets_sep = {}

    all_recall_data = {}
    all_recall_bins_targets_sep = {}

    all_precision_data = {}
    all_precision__bins_targets_sep = {}

    all_concat_jacc_bins_target_sep_foldwise = [{} for x in range(len(targets))]
    all_concat_jacc_bins_target_sep_all = [{} for x in range(len(targets))]
    # Loop over models (model) and uncertainty methods (uncert_pair)
    for i, (model, data_structs) in enumerate(bin_predictions.items()):
        for uncert_pair in uncertainty_pairs:  # uncert_pair = [pair name, error name , uncertainty name]
            uncertainty_type = uncert_pair[0]

            fold_mean_targets = []
            fold_mean_bins = [[] for x in range(num_bins)]
            fold_all_bins = [[] for x in range(num_bins)]

            fold_mean_targets_recall = []
            fold_mean_bins_recall = [[] for x in range(num_bins)]
            fold_all_bins_recall = [[] for x in range(num_bins)]

            fold_mean_targets_precision = []
            fold_mean_bins_precision = [[] for x in range(num_bins)]
            fold_all_bins_precision = [[] for x in range(num_bins)]

            fold_all_bins_concat_targets_sep_foldwise = [[[] for y in range(num_bins)] for x in range(len(targets))]
            fold_all_bins_concat_targets_sep_all = [[[] for y in range(num_bins)] for x in range(len(targets))]

            for fold in range(num_folds):
                # Get the errors and predicted bins for this fold
                fold_errors = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Error"]
                ]
                fold_bins = data_structs[(data_structs["Testing Fold"] == fold)][
                    ["uid", "target_idx", uncertainty_type + " Uncertainty bins"]
                ]

                return_dict = bin_wise_jaccard(
                    fold_errors,
                    fold_bins,
                    num_bins,
                    num_bins_for_quantiles,
                    targets,
                    uncertainty_type,
                    combine_middle_bins,
                )

                fold_mean_targets.append(return_dict["mean all targets"])
                fold_mean_targets_recall.append(return_dict["mean all targets recall"])
                fold_mean_targets_precision.append(return_dict["mean all targets precision"])

                for idx_bin in range(len(return_dict["mean all bins"])):
                    fold_mean_bins[idx_bin].append(return_dict["mean all bins"][idx_bin])
                    fold_all_bins[idx_bin] = fold_all_bins[idx_bin] + return_dict["all bins"][idx_bin]

                    fold_mean_bins_recall[idx_bin].append(return_dict["mean all bins recall"][idx_bin])
                    fold_all_bins_recall[idx_bin] = (
                        fold_all_bins_recall[idx_bin] + return_dict["all bins recall"][idx_bin]
                    )

                    fold_mean_bins_precision[idx_bin].append(return_dict["mean all bins precision"][idx_bin])
                    fold_all_bins_precision[idx_bin] = (
                        fold_all_bins_precision[idx_bin] + return_dict["all bins precision"][idx_bin]
                    )

                    # Get the jaccard saved for the individual targets, flattening the folds and also not flattening the folds
                    for target_idx in range(len(targets)):
                        fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin] = (
                            fold_all_bins_concat_targets_sep_foldwise[target_idx][idx_bin]
                            + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin]
                        )
                        combined = (
                            fold_all_bins_concat_targets_sep_all[target_idx][idx_bin]
                            + return_dict["all bins concatenated targets seperated"][target_idx][idx_bin]
                        )

                        fold_all_bins_concat_targets_sep_all[target_idx][idx_bin] = combined

            all_jaccard_data[model + " " + uncertainty_type] = fold_mean_bins
            all_jaccard_bins_targets_sep[model + " " + uncertainty_type] = fold_all_bins

            all_recall_data[model + " " + uncertainty_type] = fold_mean_bins_recall
            all_recall_bins_targets_sep[model + " " + uncertainty_type] = fold_all_bins_recall

            all_precision_data[model + " " + uncertainty_type] = fold_mean_bins_precision
            all_precision__bins_targets_sep[model + " " + uncertainty_type] = fold_all_bins_precision

            for target_idx in range(len(all_concat_jacc_bins_target_sep_foldwise)):
                all_concat_jacc_bins_target_sep_foldwise[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_foldwise[target_idx]
                all_concat_jacc_bins_target_sep_all[target_idx][
                    model + " " + uncertainty_type
                ] = fold_all_bins_concat_targets_sep_all[target_idx]

    return {
        "Jaccard All": all_jaccard_data,
        "Jaccard targets seperated": all_jaccard_bins_targets_sep,
        "Recall All": all_recall_data,
        "Recall targets seperated": all_recall_bins_targets_sep,
        "Precision All": all_precision_data,
        "Precision targets seperated": all_precision__bins_targets_sep,
        "all jacc concat bins targets sep foldwise": all_concat_jacc_bins_target_sep_foldwise,
        "all jacc concat bins targets sep all": all_concat_jacc_bins_target_sep_all,
    }


def bin_wise_errors(fold_errors, fold_bins, num_bins, targets, uncertainty_key, error_scaling_factor):
    """
    Helper function for get_mean_errors. Calculates the mean error for each bin and for each target.

    Args:
        fold_errors (Pandas Dataframe): Pandas Dataframe of errors for this fold.
        fold_bins (Pandas Dataframe): Pandas Dataframe of predicted quantile bins for this fold.
        num_bins (int): Number of quantile bins,
        targets (list) list of targets to measure uncertainty estimation,
        uncertainty_key (string): Name of uncertainty type to calculate accuracy for,


    Returns:
        [Dict]: Dict with mean error statistics.
    """

    all_target_error = []
    all_qs_error = [[] for x in range(num_bins)]
    all_qs_error_concat_targets_sep = [[[] for y in range(num_bins)] for x in range(len(targets))]

    for i, target_idx in enumerate(targets):
        true_errors_ti = fold_errors[(fold_errors["target_idx"] == target_idx)][["uid", uncertainty_key + " Error"]]
        pred_bins_ti = fold_bins[(fold_errors["target_idx"] == target_idx)][
            ["uid", uncertainty_key + " Uncertainty bins"]
        ]

        # Zip to dictionary
        true_errors_ti = dict(
            zip(true_errors_ti.uid, true_errors_ti[uncertainty_key + " Error"] * error_scaling_factor)
        )
        pred_bins_ti = dict(zip(pred_bins_ti.uid, pred_bins_ti[uncertainty_key + " Uncertainty bins"]))

        pred_bins_keys = []
        pred_bins_errors = []

        # This is saving them from best quantile of predictions to worst quantile of predictions in terms of uncertainty
        for j in range(num_bins):
            inner_list = list([key for key, val in pred_bins_ti.items() if str(j) == str(val)])
            inner_list_errors = []

            for id_ in inner_list:
                inner_list_errors.append(list([val for key, val in true_errors_ti.items() if str(key) == str(id_)])[0])

            pred_bins_errors.append(inner_list_errors)
            pred_bins_keys.append(inner_list)

        # Now for each bin, get the mean error
        inner_errors = []
        for bin in range(num_bins):
            # pred_b_keys = pred_bins_keys[bin]
            pred_b_errors = pred_bins_errors[bin]

            # test for empty bin, it would've created a mean_error==nan , so don't add it!
            if pred_b_errors == []:
                continue

            mean_error = np.mean(pred_b_errors)
            all_qs_error[bin].append(mean_error)
            all_qs_error_concat_targets_sep[i][bin].append(pred_b_errors)
            inner_errors.append(mean_error)

        all_target_error.append(np.mean(inner_errors))

    mean_all_targets = np.mean(all_target_error)
    mean_all_bins = []
    for x in all_qs_error:
        if x == []:
            mean_all_bins.append(None)
        else:
            mean_all_bins.append(np.mean(x))

    return {
        "mean all targets": mean_all_targets,
        "mean all bins": mean_all_bins,
        "all bins": all_qs_error,
        "all bins concatenated targets seperated": all_qs_error_concat_targets_sep,
    }


def bin_wise_jaccard(
    fold_errors: pd.DataFrame,
    fold_bins: pd.DataFrame,
    num_bins: int,
    num_bins_quantiles: int,
    targets: list,
    uncertainty_key: str,
    combine_middle_bins: bool,
) -> dict:
    """
    Helper function for evaluate_jaccard. Calculates the Jaccard Index statistics for each quantile bin and target.

    If combine_middle_bins is True, then the middle bins are combined into one bin. e.g. if num_bins_quantiles = 10,
    it will return 3 bins: 1, 2-9, 10.
    You may find the first bin and the last bin are the most accurate, so combining the middle bins may be useful.

    Args:
        fold_errors (Pandas Dataframe): Pandas Dataframe of errors for this fold.
        fold_bins (Pandas Dataframe): Pandas Dataframe of predicted quantile bins for this fold.
        num_bins (int): Number of quantile bins,
        targets (list) list of targets to measure uncertainty estimation,
        uncertainty_key (string): Name of uncertainty type to calculate accuracy for,


    Returns:
        [Dict]: Dict with JI statistics.

    Raises:
        None.

    Example:
        >>> bin_wise_jaccard(fold_errors, fold_bins, 10, 5, [0,1], 'S-MHA', True)
    """

    all_target_jacc: List[float] = []
    all_qs_jacc: List[List[float]] = [[] for x in range(num_bins)]

    all_qs_jacc_concat_targets_sep: List[List[List[float]]] = [
        [[] for y in range(num_bins)] for x in range(len(targets))
    ]

    all_target_recall: List[float] = []
    all_qs_recall: List[List[float]] = [[] for x in range(num_bins)]

    all_target_precision: List[float] = []
    all_qs_precision: List[List[float]] = [[] for x in range(num_bins)]

    for i, target_idx in enumerate(targets):
        true_errors_ti = fold_errors[(fold_errors["target_idx"] == target_idx)][["uid", uncertainty_key + " Error"]]
        pred_bins_ti = fold_bins[(fold_errors["target_idx"] == target_idx)][
            ["uid", uncertainty_key + " Uncertainty bins"]
        ]

        # Zip to dictionary
        true_errors_ti = dict(zip(true_errors_ti.uid, true_errors_ti[uncertainty_key + " Error"]))
        pred_bins_ti = dict(zip(pred_bins_ti.uid, pred_bins_ti[uncertainty_key + " Uncertainty bins"]))

        pred_bins_keys = []
        pred_bins_errors = []

        # This is saving them from best quantile of predictions to worst quantile of predictions in terms of uncertainty
        for j in range(num_bins):
            inner_list = list([key for key, val in pred_bins_ti.items() if str(j) == str(val)])
            inner_list_errors = []

            for id_ in inner_list:
                inner_list_errors.append(list([val for key, val in true_errors_ti.items() if str(key) == str(id_)])[0])

            pred_bins_errors.append(inner_list_errors)
            pred_bins_keys.append(inner_list)

        # Get the true error quantiles
        sorted_errors = [v for k, v in sorted(true_errors_ti.items(), key=lambda item: item[1])]

        quantiles = np.arange(1 / num_bins_quantiles, 1, 1 / num_bins_quantiles)[: num_bins_quantiles - 1]
        quantile_thresholds = [np.quantile(sorted_errors, q) for q in quantiles]

        # If we are combining the middle bins, combine the middle lists into 1 list.

        if combine_middle_bins:
            quantile_thresholds = [quantile_thresholds[0], quantile_thresholds[-1]]

        errors_groups = []
        key_groups = []

        for q in range(len(quantile_thresholds) + 1):
            inner_list_e = []
            inner_list_id = []
            for i_te, (id_, error) in enumerate(true_errors_ti.items()):
                if q == 0:
                    lower = 0
                    upper = quantile_thresholds[q]

                    if error <= upper:
                        inner_list_e.append(error)
                        inner_list_id.append(id_)

                elif q < len(quantile_thresholds):
                    lower = quantile_thresholds[q - 1]
                    upper = quantile_thresholds[q]

                    if error <= upper and error > lower:
                        inner_list_e.append(error)
                        inner_list_id.append(id_)

                else:
                    lower = quantile_thresholds[q - 1]
                    upper = 999999999999999999999999999999

                    if error > lower:
                        inner_list_e.append(error)
                        inner_list_id.append(id_)

            errors_groups.append(inner_list_e)
            key_groups.append(inner_list_id)

        # flip them so they go from B5 to B1
        pred_bins_keys = pred_bins_keys[::-1]
        pred_bins_errors = pred_bins_errors[::-1]
        errors_groups = errors_groups[::-1]
        key_groups = key_groups[::-1]

        # Now for each bin, get the jaccard similarity
        inner_jaccard_sims = []
        inner_recalls = []
        inner_precisions = []
        for bin in range(num_bins):
            pred_b_keys = pred_bins_keys[bin]
            gt_bins_keys = key_groups[bin]

            j_sim = jaccard_similarity(pred_b_keys, gt_bins_keys)
            all_qs_jacc[bin].append(j_sim)
            all_qs_jacc_concat_targets_sep[i][bin].append(j_sim)

            inner_jaccard_sims.append(j_sim)

            # If quantile threshold is the same as the last quantile threshold,
            # the GT set is empty (rare, but can happen if distribution of errors is quite uniform).
            if len(gt_bins_keys) == 0:
                recall = 1.0
                precision = 0.0
            else:
                recall = sum(el in gt_bins_keys for el in pred_b_keys) / len(gt_bins_keys)

                if len(pred_b_keys) == 0 and len(gt_bins_keys) > 0:
                    precision = 0.0
                else:
                    precision = sum(1 for x in pred_b_keys if x in gt_bins_keys) / len(pred_b_keys)

            inner_recalls.append(recall)
            inner_precisions.append(precision)
            all_qs_recall[bin].append(recall)
            all_qs_precision[bin].append(precision)

        all_target_jacc.append(np.mean(inner_jaccard_sims))
        all_target_recall.append(np.mean(inner_recalls))
        all_target_precision.append(np.mean(inner_precisions))

    return {
        "mean all targets": np.mean(all_target_jacc),
        "mean all bins": [np.mean(x) for x in all_qs_jacc],
        "all bins": all_qs_jacc,
        "mean all targets recall": np.mean(all_target_recall),
        "mean all bins recall": [np.mean(x) for x in all_qs_recall],
        "all bins recall": all_qs_recall,
        "mean all targets precision": np.mean(all_target_precision),
        "mean all bins precision": [np.mean(x) for x in all_qs_precision],
        "all bins precision": all_qs_precision,
        "all bins concatenated targets seperated": all_qs_jacc_concat_targets_sep,
    }
</file>

<file path="kale/interpret/model_weights.py">
# =============================================================================
# Author: Shuo Zhou, shuo.zhou@sheffield.ac.uk
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

import numpy as np
from tensorly.base import fold, unfold


def select_top_weight(weights, select_ratio: float = 0.05):
    """Select top weights in magnitude, and the rest of weights will be zeros

    Args:
        weights (array-like): model weights, can be a vector or a higher order tensor
        select_ratio (float, optional): ratio of top weights to be selected. Defaults to 0.05.

    Returns:
        array-like: top weights in the same shape with the input model weights
    """
    if not isinstance(weights, np.ndarray):
        weights = np.array(weights)
    orig_shape = weights.shape

    if len(orig_shape) > 1:
        weights = unfold(weights, mode=0)[0]
    n_top_weights = int(weights.size * select_ratio)
    top_weight_idx = (-1 * abs(weights)).argsort()[:n_top_weights]
    top_weights = np.zeros(weights.size)
    top_weights[top_weight_idx] = weights[top_weight_idx]
    if len(orig_shape) > 1:
        top_weights = fold(top_weights, mode=0, shape=orig_shape)

    return top_weights
</file>

<file path="kale/interpret/README.md">
# Modules to help interpretation

Analysis and montitoring with visulisation, interpretation, screen-printout, etc. including utilites.
</file>

<file path="kale/interpret/uncertainty_quantiles.py">
"""
Authors: Lawrence Schobs, lawrenceschobs@gmail.com

Module from the implementation of L. A. Schobs, A. J. Swift and H. Lu, "Uncertainty Estimation for Heatmap-Based Landmark Localization,"
in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034, April 2023, doi: 10.1109/TMI.2022.3222730.

Functions related to interpreting the uncertainty quantiles from the quantile binning method in terms of:
   A) Correlation of uncertainty with error (fit_line_with_ci)
   B) Perform Isotonic regression on uncertainty & error pairs (quantile_binning_and_est_errors)
   C) Plot boxplots: generic_box_plot_loop, format_plot, box_plot_per_model, box_plot_comparing_q
   D) Plot cumularive error plots: plot_cumulative
   E) Big caller functions for analysis loop for QBinning:  generate_fig_individual_bin_comparison, generate_fig_comparing_bins

"""
import logging
import math
import os
from typing import Any, Dict, List, Optional, Tuple, Union

import matplotlib.lines as mlines
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
import pandas as pd
import pwlf
from matplotlib.ticker import ScalarFormatter
from scipy import stats
from sklearn.isotonic import IsotonicRegression

from kale.evaluate.similarity_metrics import evaluate_correlations
from kale.evaluate.uncertainty_metrics import evaluate_bounds, evaluate_jaccard, get_mean_errors
from kale.prepdata.tabular_transform import generate_struct_for_qbin
from kale.utils.save_xlsx import generate_summary_df


def fit_line_with_ci(
    errors: np.ndarray,
    uncertainties: np.ndarray,
    quantile_thresholds: List[float],
    cmaps: List[Dict[str, str]],
    to_log: bool = False,
    error_scaling_factor: float = 1.0,
    save_path: Optional[str] = None,
) -> Dict[str, List[Any]]:
    """
    Calculates Spearman correlation between errors and uncertainties.
    Plots piecewise linear regression with bootstrap confidence intervals.
    Breakpoints in linear regression are defined by the uncertainty quantiles of the data.

    Args:
        errors (np.ndarray): Array of errors.
        uncertainties (np.ndarray): Array of uncertainties.
        quantile_thresholds (List[float]): List of quantile thresholds.
        cmaps (List[str]): List of colormap names.
        to_log (bool, optional): Whether to apply logarithmic transformation on axes. Defaults to False.
        error_scaling_factor (float, optional): Scaling factor for error. Defaults to 1.0.
        save_path (Optional[str], optional): Path to save the plot, if None, the plot will be shown. Defaults to None.

    Returns:
        Dict[str, Tuple[float, float]]: Dictionary containing Spearman and Pearson correlation coefficients and p-values.
    """

    plt.figure(figsize=(12, 8))
    ax = plt.gca()

    if to_log:
        plt.xscale("log", base=2)
        plt.yscale("log", base=2)

    X = uncertainties
    y = errors * error_scaling_factor
    plt.xlim(max(X), min(X))

    # Calculate correlations
    spm_corr, spm_p = stats.spearmanr(X, y, alternative="greater")
    pear_corr, pear__p = stats.pearsonr(X, y)
    correlation_dict = {"spearman": [spm_corr, spm_p], "pearson": [pear_corr, pear__p]}

    # Bootstrap sample for confidence intervals
    for i in range(0, 1000):
        sample_index = np.random.choice(range(0, len(y)), int(len(y) * 0.6))

        X_samples = X[sample_index]
        y_samples = y[sample_index]
        my_pwlf = pwlf.PiecewiseLinFit(X_samples, y_samples)
        my_pwlf.fit_with_breaks(quantile_thresholds)
        xHat = np.linspace(min(X), max(X), num=10000)
        yHat = my_pwlf.predict(xHat)

        # Plot each bootstrap as a grey line
        plt.plot(xHat[::-1], yHat[::-1], "-", color="grey", alpha=0.2, zorder=2)

    # Display all datapoints on plot
    plt.scatter(X, y, marker="o", color=cmaps[2], zorder=1, alpha=0.2)

    # Final fit on ALL the data
    my_pwlf = pwlf.PiecewiseLinFit(X, y)

    # Manually set the piecewise breaks as the quantile thresholds.
    my_pwlf.fit_with_breaks(quantile_thresholds)

    # Plot the final fitted line, changing the color of the line segments for each quantile for visualisation.
    bin_label_locs = []
    for idx in range(len(quantile_thresholds) + 1):
        color = "blue" if idx % 2 == 0 else "red"
        xHat = np.linspace(min(X), max(X), num=20000)
        yHat = my_pwlf.predict(xHat)

        if idx == 0:
            min_ = min(X)
            max_ = quantile_thresholds[idx]

        elif idx <= len(quantile_thresholds) - 1:
            min_ = quantile_thresholds[idx - 1]
            max_ = quantile_thresholds[idx]
        else:
            min_ = quantile_thresholds[idx - 1]
            max_ = max(X)

        plot_indicies = [i for i in range(len(xHat)) if min_ <= xHat[i] < max_]
        bin_label_locs.append((min_ + max_) / 2)
        quantile_data_indicies = [i for i in range(len(X)) if min_ <= X[i] < max_]
        # shade background to make slices per quantile
        q_spm_corr, q_spm_p = stats.spearmanr(X[quantile_data_indicies], y[quantile_data_indicies])
        plt.axvspan(xHat[plot_indicies][0], xHat[plot_indicies][-1], facecolor=color, alpha=0.1)
        plt.plot(xHat[plot_indicies], yHat[plot_indicies], "-", color=color, zorder=3)

    ax.set_xticks(bin_label_locs)
    new_labels = [r"$Q_{{{}}}$".format(x + 1) for x in range(len(quantile_thresholds) + 1)]
    ax.xaxis.set_major_formatter(plt.FixedFormatter(new_labels))
    ax.yaxis.set_major_formatter(ScalarFormatter())

    # Put text on plot showing the correlation
    p_vl = 0.001 if np.round(spm_p, 3) == 0 else np.round(spm_p, 3)
    bold_text = (
        r"$\bf{\rho:} $" + r"$\bf{{{}}}$".format(np.round(spm_corr, 2)) + ", p-value < " + r"${{{}}}$".format(p_vl)
    )

    plt.text(
        0.5,
        0.9,
        bold_text,
        size=25,
        color="black",
        horizontalalignment="center",
        verticalalignment="center",
        transform=ax.transAxes,
    )
    ax.set_xlabel("Uncertainty Quantile", fontsize=25)
    ax.set_ylabel("Error (mm)", fontsize=25)
    plt.subplots_adjust(bottom=0.2)
    plt.subplots_adjust(left=0.15)

    plt.xticks(fontsize=25)
    plt.yticks(fontsize=25)

    if save_path is not None:
        plt.gcf().set_size_inches(16.0, 8.0)
        plt.tight_layout()
        plt.savefig(save_path, dpi=600, bbox_inches="tight", pad_inches=0.1)
        plt.close()
    else:
        plt.gcf().set_size_inches(16.0, 10.0)
        plt.show()
        plt.close()

    return correlation_dict


def quantile_binning_and_est_errors(
    errors: List[float],
    uncertainties: List[float],
    num_bins: int,
    type: str = "quantile",
    acceptable_thresh: float = 5,
    combine_middle_bins: bool = False,
) -> Tuple[List[List[float]], List[float]]:
    """
    Calculate quantile thresholds, and isotonically regress errors and uncertainties
    and get estimated error bounds.

    Args:
        errors (List[float]): List of errors.
        uncertainties (List[float]): List of uncertainties.
        num_bins (int): Number of quantile bins.
        type (str, optional): Type of thresholds to calculate, "quantile" recommended.
                              Defaults to "quantile".
        acceptable_thresh (float, optional): Acceptable error threshold. Only relevant
                                             if type="error-wise". Defaults to 5.
        combine_middle_bins (bool, optional): Whether to combine middle bins.
                                              Defaults to False.

    Returns:
        Tuple[List[List[float]], List[float]]: List of quantile thresholds and
                                               estimated error bounds.
    """

    if len(errors) != len(uncertainties):
        raise ValueError(
            "Length of errors and uncertainties must be the same. errors is length %s and uncertainties is length %s"
            % (len(errors), len(uncertainties))
        )

    valid_types = {"quantile", "error-wise"}
    if type not in valid_types:
        raise ValueError("results: type must be one of %r. " % valid_types)

    # Isotonically regress line
    ir = IsotonicRegression(out_of_bounds="clip", increasing=True)

    _ = ir.fit_transform(uncertainties, errors)

    uncert_boundaries = []
    estimated_errors = []

    # Estimate error bounds for each quantile bin
    if type == "quantile":
        quantiles = np.arange(1 / num_bins, 1, 1 / num_bins)[: num_bins - 1]
        for q in range(len(quantiles)):
            q_conf_higher = [np.quantile(uncertainties, quantiles[q])]
            q_error_higher = ir.predict(q_conf_higher)

            estimated_errors.append(q_error_higher[0])
            uncert_boundaries.append(q_conf_higher)

    elif type == "error_wise":
        quantiles = np.arange(num_bins - 1)
        estimated_errors = [[(acceptable_thresh * x)] for x in quantiles]

        uncert_boundaries = [(ir.predict(x)).tolist() for x in estimated_errors]
        raise NotImplementedError("error_wise Quantile Binning not implemented yet")

    # IF combine bins, we grab only the values for the two outer bins
    if combine_middle_bins:
        estimated_errors = [estimated_errors[0], estimated_errors[-1]]
        uncert_boundaries = [uncert_boundaries[0], uncert_boundaries[-1]]

    return uncert_boundaries, estimated_errors


def generic_box_plot_loop(
    cmaps: List[str],
    target_uncert_dicts: Dict[str, List[List[float]]],
    uncertainty_types_list: List[List[str]],
    models: List[str],
    x_axis_labels: List[str],
    x_label: str,
    y_label: str,
    num_bins: int,
    list_comp_bool: bool,
    width: float,
    y_lim_min: float,
    font_size_1: int,
    font_size_2: int,
    show_sample_info: str = "None",
    save_path: Optional[str] = None,
    y_lim: int = 120,
    convert_to_percent: bool = True,
    to_log: bool = False,
    show_individual_dots: bool = True,
) -> None:
    """
    This function generates box plots for multiple types of data coming from various models. It is highly customizable
    and can handle different specifications for plot attributes.

    Customizations include:

    1. Color specification: User can provide a list of color specifications for each box plot using `cmaps` parameter.
    2. Axis labels: The x and y axis labels can be customized using `x_label` and `y_label` parameters.
    3. Box width: The width of each box plot can be adjusted using `width` parameter.
    4. Font sizes: Two different font sizes can be used in the plot, adjustable by `font_size_1` and `font_size_2`.
    5. Limits of y-axis: The upper and lower limits of the y-axis can be set using `y_lim` and `y_lim_min` parameters.
    6. Logarithmic scale: If `to_log` is set to True, the y-axis will be in logarithmic scale.
    7. Display of individual data points: The user can choose to display individual data points in each box plot
       by setting `show_individual_dots` to True.
    8. Data transformation: The data can be transformed to percentages using `convert_to_percent` parameter.
    9. Display of sample information: The user can choose to display information about the number of samples in each
       box plot by setting `show_sample_info` to "None", "All", or "Average".

    The function creates box plots for each combination of model and uncertainty type. It can save the resulting plot
    to a specified location.



    Args:
        cmaps (List[str]): Colors for the box plots.
        target_uncert_dicts (Dict[str, List[List[float]]]): Dictionary with lists of [error, uncertainty values] for all targets
          and corresponding data.
        uncertainty_types_list (List[List[str]]): List of lists containing uncertainty types.
        models (List[str]): List of models for which box plots are being made.
        x_axis_labels (List[str]): Labels for the x-axis.
        x_label (str): The label for the x-axis.
        y_label (str): The label for the y-axis.
        num_bins (int): The number of bins to be used for the box plot.
        list_comp_bool (bool): Flag to determine if list comprehension should be used.
        width (float): The width of the boxes in the box plot.
        y_lim_min (float): The minimum limit for the y-axis.
        font_size_1 (int): Font size for the first element.
        font_size_2 (int): Font size for the second element.
        show_sample_info (str): Information about the samples to be displayed. Default is "None".
        save_path (Optional[str]): The path where the plot will be saved. If None, the plot won't be saved. Default is None.
        y_lim (int): The maximum limit for the y-axis. Default is 120.
        convert_to_percent (bool): Flag to determine if data should be converted to percentages. Default is True.
        to_log (bool): Flag to determine if a logarithmic scale should be used. Default is False.
        show_individual_dots (bool): Flag to determine if individual data points should be shown. Default is True.

    Returns:
        None. The function displays and/or saves a plot.
    """
    hatch_type = "o"

    plt.style.use("fivethirtyeight")

    orders = []
    ax = plt.gca()
    ax.xaxis.grid(False)

    bin_label_locs: List[float] = []
    all_rects = []
    outer_min_x_loc = 0.0
    middle_min_x_loc = 0.0
    inner_min_x_loc = 0.0

    circ_patches = []
    max_bin_height = 0.0

    all_sample_label_x_locs = []
    all_sample_percs = []

    for i, (uncert_pair) in enumerate(uncertainty_types_list):
        uncertainty_type = (uncert_pair)[0]

        for j in range(num_bins):
            inbetween_locs = []
            average_samples_per_bin = []

            for hash_idx, model_type in enumerate(models):
                if j == 0:
                    if hash_idx == 1:
                        circ11 = patches.Patch(
                            facecolor=cmaps[i],
                            label=model_type + " " + uncertainty_type,
                            hatch=hatch_type,
                            edgecolor="black",
                        )
                    else:
                        circ11 = patches.Patch(facecolor=cmaps[i], label=model_type + " " + uncertainty_type)
                    circ_patches.append(circ11)

                dict_key = [
                    x for x in list(target_uncert_dicts.keys()) if (model_type in x) and (uncertainty_type in x)
                ][0]
                model_data = target_uncert_dicts[dict_key]

                if list_comp_bool:
                    all_b_data = [x for x in model_data[j] if x is not None]
                else:
                    all_b_data = model_data[j]

                orders.append(model_type + uncertainty_type)

                x_loc = [(outer_min_x_loc + inner_min_x_loc + middle_min_x_loc)]
                inbetween_locs.append(x_loc[0])

                # Turn data to percentages
                if convert_to_percent:
                    displayed_data = [(x) * 100 for x in all_b_data]
                else:
                    displayed_data = all_b_data
                rect = ax.boxplot(
                    displayed_data, positions=x_loc, sym="", widths=width, showmeans=True, patch_artist=True
                )

                if show_individual_dots:
                    # Add some random "jitter" to the x-axis
                    x = np.random.normal(x_loc, 0.01, size=len(displayed_data))
                    ax.plot(
                        x,
                        displayed_data,
                        color=cmaps[len(uncertainty_types_list)],
                        marker=".",
                        linestyle="None",
                        alpha=0.75,
                    )

                # Set color, pattern, median line and mean marker.
                for r in rect["boxes"]:
                    r.set(color="black", linewidth=1)
                    r.set(facecolor=cmaps[i])

                    if hash_idx == 1:
                        r.set_hatch(hatch_type)

                for median in rect["medians"]:
                    median.set(color="crimson", linewidth=3)

                for mean in rect["means"]:
                    mean.set(markerfacecolor="crimson", markeredgecolor="black", markersize=10)

                max_bin_height = max(max(rect["caps"][-1].get_ydata()), max_bin_height)

                """If we are showing sample statistics, keep track of it and display after on top of biggest whisker."""
                if show_sample_info != "None":
                    flattened_model_data = [x for xss in model_data for x in xss]
                    percent_size = np.round(len(all_b_data) / len(flattened_model_data) * 100, 1)
                    average_samples_per_bin.append(percent_size)

                    if show_sample_info == "All":
                        """This adds the number of samples on top of the top whisker"""
                        (x_l, y), (x_r, _) = rect["caps"][-1].get_xydata()
                        x_line_center = (x_l + x_r) / 2
                        all_sample_label_x_locs.append(x_line_center)
                        all_sample_percs.append(percent_size)
                all_rects.append(rect)

                inner_min_x_loc += 0.1 + width

            """ Keep track of average sample statistics. Plot at the END so we know what the max height for all Qs are."""
            if show_sample_info == "Average":
                middle_x = np.mean(inbetween_locs)
                mean_perc = np.round(np.mean(average_samples_per_bin), 1)
                std_perc = np.round(np.std(average_samples_per_bin), 1)
                all_sample_label_x_locs.append(middle_x)
                all_sample_percs.append([mean_perc, std_perc])

            if list_comp_bool:
                bin_label_locs = bin_label_locs + inbetween_locs
            else:
                bin_label_locs.append(np.mean(inbetween_locs))

            middle_min_x_loc += 0.02

        # If lots of bins we must make the gap between plots bigger to prevent overlapping x-tick labels.
        if list_comp_bool:
            if num_bins > 9:
                middle_min_x_loc += 0.25
            else:
                middle_min_x_loc += 0.12
        else:
            if num_bins > 10:
                outer_min_x_loc += 0.35
            else:
                outer_min_x_loc += 0.25

    format_plot(
        ax,
        save_path,
        show_sample_info,
        to_log,
        circ_patches,
        y_lim,
        y_lim_min,
        convert_to_percent,
        x_label,
        y_label,
        font_size_1,
        font_size_2,
        bin_label_locs,
        x_axis_labels,
        num_bins,
        uncertainty_types_list,
        all_sample_percs,
        all_sample_label_x_locs,
        max_bin_height,
    )


def format_plot(
    ax,
    save_path: Optional[str],
    show_sample_info: str,
    to_log: bool,
    circ_patches: List[patches.Patch],
    y_lim: float,
    y_lim_min: float,
    convert_to_percent: bool,
    x_label: str,
    y_label: str,
    font_size_1: int,
    font_size_2: int,
    bin_label_locs: List[float],
    x_axis_labels: List[str],
    num_bins: int,
    uncertainty_types_list: List[List[str]],
    all_sample_percs: List[List[float]],
    all_sample_label_x_locs: List[List[Any]],
    max_bin_height: float,
    comparing_q: bool = False,
) -> None:
    """
    This function takes a matplotlib Axes object and formats the plot according to the provided parameters.

    Args:
        ax: A matplotlib axes object to be formatted.
        save_path: The path where the plot should be saved. If None, the plot will be shown using plt.show().
        show_sample_info: Determines how sample information is displayed. Can be "None", "Average", or "All".
        to_log: If True, sets the y-axis to log scale.
        circ_patches: List of matplotlib patches to be added to the legend.
        y_lim: The upper limit for the y-axis.
        y_lim_min: The lower limit for the y-axis.
        convert_to_percent: If True, converts y-axis values to percentages.
        x_label: The label for the x-axis.
        y_label: The label for the y-axis.
        font_size_1: The font size for the axis labels.
        font_size_2: The font size for the tick labels.
        bin_label_locs: The x-axis locations of the bin labels.
        x_axis_labels: The labels for the x-axis.
        num_bins: The number of bins.
        uncertainty_types_list: The list of uncertainty types.
        all_sample_percs: The percentage of samples for each bin.
        all_sample_label_x_locs: The x-axis locations of the sample percentage labels.
        max_bin_height: The maximum height of a bin in the plot.
        comparing_q: If True, it uses a ticker.FixedFormatter for the x-axis.

    Returns:
        None
    """

    # Show the average samples on top of boxplots, aligned. if lots of bins we can lower the height.
    if show_sample_info != "None":
        for idx_text, perc_info in enumerate(all_sample_percs):
            if show_sample_info == "Average":
                ax.text(
                    all_sample_label_x_locs[idx_text],
                    max_bin_height * 0.8,  # Position
                    r"$\bf{PSB}$" + ": \n" + r"${} \pm$".format(perc_info[0]) + "\n" + r"${}$".format(perc_info[1]),
                    verticalalignment="bottom",  # Centered bottom with line
                    horizontalalignment="center",  # Centered with horizontal line
                    fontsize=25,
                )
            elif show_sample_info == "All":
                if idx_text % 2 == 0:
                    label_height = max_bin_height + 3
                else:
                    label_height = max_bin_height + 1
                ax.text(
                    all_sample_label_x_locs[idx_text][0],
                    label_height,  # Position
                    r"$\bf{PSB}$" + ": \n" + str(perc_info) + "%",
                    verticalalignment="bottom",  # Centered bottom with line
                    horizontalalignment="center",  # Centered with horizontal line
                    fontsize=25,
                )

    ax.set_xlabel(x_label, fontsize=font_size_1)
    ax.set_ylabel(y_label, fontsize=font_size_1)
    ax.set_xticks(bin_label_locs)

    plt.subplots_adjust(bottom=0.15)
    plt.subplots_adjust(left=0.15)

    plt.xticks(fontsize=font_size_2)
    plt.yticks(fontsize=font_size_2)

    if comparing_q:
        ax.xaxis.set_major_formatter(ticker.FixedFormatter(x_axis_labels))

    else:
        if num_bins <= 5:
            ax.xaxis.set_major_formatter(ticker.FixedFormatter(x_axis_labels[:-1] * (len(uncertainty_types_list) * 2)))
        # If too many bins, only show the first and last or it will appear too squished, indicate direction with arrow.
        elif num_bins < 15:
            number_blanks_0 = ["" for x in range(math.floor((num_bins - 3) / 2))]
            number_blanks_1 = ["" for x in range(num_bins - 3 - len(number_blanks_0))]
            new_labels = (
                [x_axis_labels[0]] + number_blanks_0 + [r"$\rightarrow$"] + number_blanks_1 + [x_axis_labels[-1]]
            )
            ax.xaxis.set_major_formatter(ticker.FixedFormatter(new_labels * (len(uncertainty_types_list) * 2)))
        # if more than 15 bins, we must move the first and last labels inwards to prevent overlap.
        else:
            number_blanks_0 = ["" for x in range(math.floor((num_bins - 5) / 2))]
            number_blanks_1 = ["" for x in range(num_bins - 5 - len(number_blanks_0))]
            new_labels = (
                [""]
                + [x_axis_labels[0]]
                + number_blanks_0
                + [r"$\rightarrow$"]
                + number_blanks_1
                + [x_axis_labels[-1]]
                + [""]
            )
            ax.xaxis.set_major_formatter(ticker.FixedFormatter(new_labels * (len(uncertainty_types_list) * 2)))

    if to_log:
        ax.set_yscale("symlog", base=2)
        ax.yaxis.set_major_formatter(ScalarFormatter())
        ax.set_ylim(y_lim_min, y_lim)

    else:
        ax.set_ylim((y_lim_min, y_lim))

    # If using percent, doesnt make sense to show any y tick above 100
    if convert_to_percent and y_lim > 100:
        plt.yticks(np.arange(0, y_lim, 20))

    # Add more to legend, add the mean symbol and median symbol.
    red_triangle_mean = mlines.Line2D(
        [], [], color="crimson", marker="^", markeredgecolor="black", linestyle="None", markersize=10, label="Mean"
    )
    circ_patches.append(red_triangle_mean)

    red_line_median = mlines.Line2D(
        [], [], color="crimson", marker="", markeredgecolor="black", markersize=10, label="Median"
    )
    circ_patches.append(red_line_median)

    if show_sample_info == "Average":
        circ_patches.append(patches.Patch(color="none", label=r"$\bf{PSB}$" + r": % Samples per Bin"))

    num_cols_legend = math.ceil(len(circ_patches) / 2)
    ax.legend(
        handles=circ_patches,
        fontsize=20,
        ncol=num_cols_legend,
        columnspacing=2,
        loc="upper center",
        bbox_to_anchor=(0.5, 1.18),
        fancybox=True,
        shadow=False,
    )

    if save_path is not None:
        plt.gcf().set_size_inches(16.0, 10.0)
        plt.tight_layout()
        plt.savefig(save_path, dpi=600, bbox_inches="tight", pad_inches=0.1)
        plt.close()
    else:
        plt.gcf().set_size_inches(16.0, 10.0)
        plt.show()
        plt.close()


def box_plot_per_model(
    cmaps: List[str],
    target_uncert_dicts: Dict[str, List[List[float]]],
    uncertainty_types_list: List[List[str]],
    models: List[str],
    x_axis_labels: List[str],
    x_label: str,
    y_label: str,
    num_bins: int,
    show_sample_info: str = "None",
    save_path: Optional[str] = None,
    y_lim: int = 120,
    convert_to_percent: bool = True,
    to_log: bool = False,
    show_individual_dots: bool = True,
) -> None:
    """
    Generates a box plot to visualize and compare the performance of different models across uncertainty bins.

    This function creates a box plot for each model, grouped by uncertainty types, and displays the
    distribution of data within each bin. Individual data points can be shown as dots and additional
    information such as the percentage of samples per bin can be displayed on top of the box plots.

    Args:
        cmaps (List[str]): List of colors for matplotlib.
        target_uncert_dicts (Dict[str, List[List[float]]]): Dict of pandas dataframes for the data to display.
        uncertainty_types_list (List[List[str]]): List of lists describing the different uncertainty combinations to test.
        models (List[str]): The models we want to compare, keys in target_uncert_dicts.
        x_axis_labels (List[str]): List of strings for the x-axis labels, one for each bin.
        x_label (str): x-axis label.
        y_label (str): y-axis label.
        num_bins (int): Number of uncertainty bins.
        show_sample_info (str): Show sample information. Options: "None", "All", "Average". Default is "None".
        save_path (Optional[str]): Path to save plot to. If None, displays on screen (default=None).
        y_lim (int): y-axis limit of graph (default=120).
        convert_to_percent (bool): Flag to turn data into percentages. Default is True.
        to_log (bool): Flag to set y-axis scale to log. Default is False.
        show_individual_dots (bool): Flag to show individual data points as dots. Default is True.
    """

    hatch_type = "o"
    plt.style.use("fivethirtyeight")

    orders = []
    ax = plt.gca()

    ax.xaxis.grid(False)

    bin_label_locs: List[float] = []
    all_rects = []
    outer_min_x_loc = 0.0
    middle_min_x_loc = 0.0
    inner_min_x_loc = 0.0

    circ_patches = []
    max_bin_height = 0.0

    all_sample_label_x_locs = []
    all_sample_percs = []

    for i, (uncert_pair) in enumerate(uncertainty_types_list):
        uncertainty_type = (uncert_pair)[0]
        for hash_idx, model_type in enumerate(models):
            inbetween_locs = []
            average_samples_per_bin = []

            for j in range(num_bins):
                if j == 0:
                    if hash_idx == 1:
                        circ11 = patches.Patch(
                            facecolor=cmaps[i],
                            label=model_type + " " + uncertainty_type,
                            hatch=hatch_type,
                            edgecolor="black",
                        )
                    else:
                        circ11 = patches.Patch(facecolor=cmaps[i], label=model_type + " " + uncertainty_type)
                    circ_patches.append(circ11)

                dict_key = [
                    x for x in list(target_uncert_dicts.keys()) if (model_type in x) and (uncertainty_type in x)
                ][0]
                model_data = target_uncert_dicts[dict_key]
                all_b_data = [x for x in model_data[j] if x is not None]

                orders.append(model_type + uncertainty_type)

                width = 0.25

                x_loc = [(outer_min_x_loc + inner_min_x_loc + middle_min_x_loc)]
                inbetween_locs.append(x_loc[0])

                # Turn data to percentages
                if convert_to_percent:
                    displayed_data = [(x) * 100 for x in all_b_data]
                else:
                    displayed_data = all_b_data
                rect = ax.boxplot(
                    displayed_data, positions=x_loc, sym="", widths=width, showmeans=True, patch_artist=True
                )

                if show_individual_dots:
                    # Add some random "jitter" to the x-axis
                    x = np.random.normal(x_loc, 0.01, size=len(displayed_data))
                    ax.plot(
                        x,
                        displayed_data,
                        color=cmaps[len(uncertainty_types_list)],
                        marker=".",
                        linestyle="None",
                        alpha=0.75,
                    )

                # Set color, pattern, median line and mean marker.
                for r in rect["boxes"]:
                    r.set(color="black", linewidth=1)
                    r.set(facecolor=cmaps[i])

                    if hash_idx == 1:
                        r.set_hatch(hatch_type)
                for median in rect["medians"]:
                    median.set(color="crimson", linewidth=3)

                for mean in rect["means"]:
                    mean.set(markerfacecolor="crimson", markeredgecolor="black", markersize=10)

                max_bin_height = max(max(rect["caps"][-1].get_ydata()), max_bin_height)

                """If we are showing sample statistics, keep track of it and display after on top of biggest whisker."""
                if show_sample_info != "None":
                    flattened_model_data = [x for xss in model_data for x in xss]
                    percent_size = np.round(len(all_b_data) / len(flattened_model_data) * 100, 1)
                    average_samples_per_bin.append(percent_size)

                    if show_sample_info == "All":
                        """This adds the number of samples on top of the top whisker"""
                        (x_l, y), (x_r, _) = rect["caps"][-1].get_xydata()
                        x_line_center = (x_l + x_r) / 2
                        all_sample_label_x_locs.append(x_line_center)
                        all_sample_percs.append(percent_size)
                all_rects.append(rect)

                inner_min_x_loc += 0.1 + width

            """ Keep track of average sample statistics. Plot at the END so we know what the max height for all Qs are."""
            if show_sample_info == "Average":
                middle_x = np.mean(inbetween_locs)
                mean_perc = np.round(np.mean(average_samples_per_bin), 1)
                std_perc = np.round(np.std(average_samples_per_bin), 1)
                all_sample_label_x_locs.append(middle_x)
                all_sample_percs.append([mean_perc, std_perc])

            bin_label_locs = bin_label_locs + inbetween_locs

            # IF lots of bins we must make the gap between plots bigger to prevent overlapping x-tick labels.
            if num_bins > 9:
                middle_min_x_loc += 0.25
            else:
                middle_min_x_loc += 0.12

        outer_min_x_loc += 0.24

    format_plot(
        ax,
        save_path,
        show_sample_info,
        to_log,
        circ_patches,
        y_lim,
        -0.1,
        convert_to_percent,
        x_label,
        y_label,
        30,
        30,
        bin_label_locs,
        x_axis_labels,
        num_bins,
        uncertainty_types_list,
        all_sample_percs,
        all_sample_label_x_locs,
        max_bin_height,
    )


def box_plot_comparing_q(
    target_uncert_dicts_list: List[Dict[str, List[List[float]]]],
    uncertainty_type_tuple: List,
    model: List[str],
    x_axis_labels: List[str],
    x_label: str,
    y_label: str,
    num_bins_display: int,
    hatch_type: str,
    color: str,
    show_sample_info: str = "None",
    save_path: Optional[str] = None,
    y_lim: int = 120,
    convert_to_percent: bool = True,
    to_log: bool = False,
    show_individual_dots: bool = True,
) -> None:
    """
    Creates a box plot of data, using Q (# Bins) on the x-axis.
    Only compares 1 model & 1 uncertainty type using Q on the x-axis.

    Args:
        target_uncert_dicts_list (List[Dict[str, List[List[float]]]]):
            List of Dict of pandas dataframe for the data to dsiplay, 1 for each value for Q.
        uncertainty_type_tuple (Tuple[str, str]):
            Tuple describing the single uncertainty/error type to display.
        model (Tuple[str, str]):
            The model we are comparing over our values of Q.
        x_axis_labels (List[str]):
            List of strings for the x-axis labels, one for each bin.
        x_label (str):
            X-axis label.
        y_label (str):
            Y-axis label.
        num_bins_display (List[int]):
            List of values of Q (#bins) we are comparing on our x-axis.
        hatch_type (str):
            Hatch type for the box plot.
        color (str):
            color for the box plot.
        show_sample_info (str, optional):
            Whether or not to show sample info on the plot.
            Options are "None", "All", or "Average". Defaults to "None".
        save_path (str, optional):
            Path to save plot to. If None, displays on screen. Defaults to None.
        y_lim (int, optional):
            Y-axis limit of graph. Defaults to 120.
        convert_to_percent (bool, optional):
            Whether to turn data to percentages. Defaults to True.
        to_log (bool, optional):
            Whether to set the y-axis to logarithmic scale. Defaults to False.
        show_individual_dots (bool, optional):
            Whether to show individual data points. Defaults to True.
    """

    plt.style.use("fivethirtyeight")

    orders = []
    ax = plt.gca()
    ax.xaxis.grid(False)

    bin_label_locs = []
    all_rects = []
    outer_min_x_loc = 0.0
    inner_min_x_loc = 0.0
    middle_min_x_loc = 0.0

    circ_patches = []

    uncertainty_type = uncertainty_type_tuple[0][0]
    model_type = model[0]

    # Set legend
    circ11 = patches.Patch(
        hatch=hatch_type,
        facecolor=color,
        label=model_type + " " + uncertainty_type,
        edgecolor="black",
    )
    circ_patches.append(circ11)

    max_bin_height = 0
    all_sample_label_x_locs = []
    all_sample_percs = []

    for idx, q_value in enumerate(x_axis_labels):
        inbetween_locs = []
        target_uncert_dicts = target_uncert_dicts_list[idx]

        # Get key for the model and uncetainty type for data
        dict_key = [x for x in list(target_uncert_dicts.keys()) if (model_type in x) and (uncertainty_type in x)][0]
        model_data = target_uncert_dicts[dict_key]
        average_samples_per_bin = []
        # Loop through each bin and display the data
        for j in range(len(model_data)):
            all_b_data = [x for x in model_data[j] if x is not None]

            orders.append(model_type + uncertainty_type)

            width = 0.2 * (4 / 5) ** idx

            x_loc = [(outer_min_x_loc + inner_min_x_loc + middle_min_x_loc)]
            inbetween_locs.append(x_loc[0])

            # Turn data to percentages
            if convert_to_percent:
                displayed_data = [(x) * 100 for x in all_b_data]
            else:
                displayed_data = all_b_data
            rect = ax.boxplot(displayed_data, positions=x_loc, sym="", widths=width, showmeans=True, patch_artist=True)

            max_bin_height = max(max(rect["caps"][-1].get_ydata()), max_bin_height)

            if show_individual_dots:
                # Add some random "jitter" to the x-axis
                x = np.random.normal(x_loc, 0.01, size=len(displayed_data))
                ax.plot(x, displayed_data, color="crimson", marker=".", linestyle="None", alpha=0.2)

            # Set color, pattern, median line and mean marker.
            for r in rect["boxes"]:
                r.set(color="black", linewidth=1)
                r.set(facecolor=color)
                r.set_hatch(hatch_type)
            for median in rect["medians"]:
                median.set(color="crimson", linewidth=3)

            for mean in rect["means"]:
                mean.set(markerfacecolor="crimson", markeredgecolor="black", markersize=10)

            """If we are showing sample statistics, keep track of it and display after on top of biggest whisker."""
            if show_sample_info != "None":
                flattened_model_data = [x for xss in model_data for x in xss]
                percent_size = np.round(len(all_b_data) / len(flattened_model_data) * 100, 1)
                average_samples_per_bin.append(percent_size)

                if show_sample_info == "All":
                    """This adds the number of samples on top of the top whisker"""
                    (x_l, y), (x_r, _) = rect["caps"][-1].get_xydata()
                    x_line_center = (x_l + x_r) / 2
                    all_sample_label_x_locs.append([x_line_center, y + 0.5])
                    all_sample_percs.append(percent_size)

            all_rects.append(rect)
            inner_min_x_loc += 0.02 + width

        outer_min_x_loc += 0.2
        bin_label_locs.append(np.mean(inbetween_locs))

        """ Keep track of average sample statistics. Plot at the END so we know what the max height for all Qs are."""
        if show_sample_info == "Average":
            middle_x = np.mean(inbetween_locs)
            mean_perc = np.round(np.mean(average_samples_per_bin), 1)
            std_perc = np.round(np.std(average_samples_per_bin), 1)
            all_sample_label_x_locs.append(middle_x)
            all_sample_percs.append([mean_perc, std_perc])

    format_plot(
        ax,
        save_path,
        show_sample_info,
        to_log,
        circ_patches,
        y_lim,
        -0.1,
        convert_to_percent,
        x_label,
        y_label,
        30,
        25,
        bin_label_locs,
        x_axis_labels,
        num_bins_display,
        uncertainty_type_tuple,
        all_sample_percs,
        all_sample_label_x_locs,
        max_bin_height,
        comparing_q=True,
    )


def plot_cumulative(
    cmaps: List[str],
    data_struct: Dict[str, pd.DataFrame],
    models: List[str],
    uncertainty_types: List[Tuple[str, str]],
    bins: Union[List[int], np.ndarray],
    title: str,
    compare_to_all: bool = False,
    save_path: Optional[str] = None,
    error_scaling_factor: float = 1,
) -> None:
    """
    Plots cumulative errors.

    Args:
        cmaps: A list of colors for matplotlib.
        data_struct: A dictionary containing the dataframes for each model.
        models: A list of models we want to compare, keys in `data_struct`.
        uncertainty_types: A list of lists describing the different uncertainty combinations to test.
        bins: A list of bins to show error form.
        title: The title of the plot.
        compare_to_all: Whether to compare the given subset of bins to all the data (default=False).
        save_path: The path to save plot to. If None, displays on screen (default=None).
        error_scaling_factor (float, optional): Scaling factor for error. Defaults to 1.0.
    """

    # make sure bins is a list and not a single value
    bins = [bins] if not isinstance(bins, (list, np.ndarray)) else bins

    plt.style.use("ggplot")

    _ = plt.figure()

    ax = plt.gca()
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)

    ax.set_xlabel("Error (mm)", fontsize=10)
    ax.set_ylabel("Number of images in %", fontsize=10)
    plt.title(title)

    ax.set_xscale("log")
    line_styles = [":", "-", "dotted", "-."]
    for i, (uncert_pair) in enumerate(uncertainty_types):
        uncertainty = (uncert_pair)[0]
        color = cmaps[i]
        for hash_idx, model_type in enumerate(models):
            line = line_styles[hash_idx]

            # Filter only the bins selected
            dataframe = data_struct[model_type]
            model_un_errors = (
                dataframe[dataframe[uncertainty + " Uncertainty bins"].isin(bins)][uncertainty + " Error"].values
                * error_scaling_factor
            )

            p = 100 * np.arange(len(model_un_errors)) / (len(model_un_errors) - 1)

            sorted_errors = np.sort(model_un_errors)

            ax.plot(
                sorted_errors,
                p,
                label=model_type + " " + uncertainty,
                color=color,
                linestyle=line,
                dash_capstyle="round",
            )

            if compare_to_all:
                dataframe = data_struct[model_type]
                model_un_errors = dataframe[uncertainty + " Error"].values * error_scaling_factor

                p = 100 * np.arange(len(model_un_errors)) / (len(model_un_errors) - 1)

                sorted_errors = np.sort(model_un_errors)
                line = line_styles[len(models) + hash_idx]
                ax.plot(
                    sorted_errors,
                    p,
                    label=model_type + " " + uncertainty,
                    color=color,
                    linestyle=line,
                    dash_capstyle="round",
                )

    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles, labels, prop={"size": 10})
    plt.axvline(x=5, color=cmaps[3])

    for axis in [ax.xaxis, ax.yaxis]:
        axis.set_major_formatter(ScalarFormatter())

    plt.xticks([1, 2, 3, 4, 5, 10, 20, 30])

    ax.xaxis.label.set_color("black")
    ax.yaxis.label.set_color("black")

    ax.tick_params(axis="x", colors="black")
    ax.tick_params(axis="y", colors="black")

    if save_path is not None:
        plt.savefig(save_path + "cumulative_error.pdf", dpi=100, bbox_inches="tight", pad_inches=0.2)
        plt.close()
    else:
        plt.gcf().set_size_inches(16.0, 10.0)
        plt.show()
        plt.close()


def generate_fig_individual_bin_comparison(data: Tuple, display_settings: dict) -> None:
    """Generate figures to compare localization errors, error bounds accuracy, and Jaccard index across uncertainty bins.

    Args:
        data: A tuple containing various inputs needed to generate the figures, including:
            - uncertainty_error_pairs (List[Tuple[int, float]]): A list of tuples specifying the uncertainty thresholds
            and corresponding error thresholds to use for binning the data.
            - models_to_compare (List[str]): A list of model names to compare.
            - dataset (str): The name of the dataset being used.
            - target_indices (List[int]): A list of target indices to include in the analysis.
            - num_bins (int): The number of uncertainty bins to use.
            - cmaps (List[str]): A list of colormap names to use for the figures.
            - save_folder (str): The directory in which to save the generated figures.
            - save_file_preamble (str): A string to use as the prefix for the filenames of the generated figures.
            - combine_middle_bins (bool): Whether to combine the middle bins or not.
            - save_figures_bool (bool): Whether to save the generated figures or not. If False, displays instead
            - confidence_invert (bool): Whether to invert the confidence values to uncertainty or not.
            - samples_as_dots_bool (bool): Whether to show individual samples as dots in the box plots or not.
            - show_sample_info_mode (str): The mode for showing sample information in the box plots.
            - box_plot_error_lim (float): The y-axis limit for the error box plots.
            - show_individual_target_plots (bool): Whether to generate separate plots for each individual target.
            - interpret (bool): Whether to perform interpretation analysis i.e. visualization.
            - num_folds (int): The number of folds to use in cross-validation.
            - ind_targets_to_show (List[int]): A list of target indices to include in individual target plots.
            - error_scaling_factor (float, optional): Scaling factor for error. Defaults to 1.0.


        display_settings: A dictionary containing boolean flags indicating which figures to generate.

    Returns:
        None
    """
    logger = logging.getLogger("qbin")
    [
        uncertainty_error_pairs,
        models_to_compare,
        dataset,
        target_indices,
        num_bins,
        cmaps,
        save_folder,
        save_file_preamble,
        combine_middle_bins,
        save_figures_bool,
        confidence_invert,
        samples_as_dots_bool,
        show_sample_info_mode,
        box_plot_error_lim,
        show_individual_target_plots,
        interpret,
        num_folds,
        ind_targets_to_show,
        error_scaling_factor,
    ] = data

    # If combining the middle bins we just have the 2 edge bins, and the combined middle ones.

    bins_all_targets, bins_targets_sep, bounds_all_targets, bounds_targets_sep = generate_struct_for_qbin(
        models_to_compare, target_indices, save_folder, dataset
    )

    # Get mean errors bin-wise, get all errors concatenated together bin-wise, and seperate by target.
    all_error_data_dict = get_mean_errors(
        bins_all_targets,
        uncertainty_error_pairs,
        num_bins,
        target_indices,
        num_folds=num_folds,
        error_scaling_factor=error_scaling_factor,
        combine_middle_bins=combine_middle_bins,
    )
    all_error_data = all_error_data_dict["all mean error bins nosep"]

    all_bins_concat_targets_nosep_error = all_error_data_dict[
        "all error concat bins targets nosep"
    ]  # shape is [num bins]

    all_bins_concat_targets_sep_all_error = all_error_data_dict[
        "all error concat bins targets sep all"
    ]  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

    # Get jaccard
    all_jaccard_data_dict = evaluate_jaccard(
        bins_all_targets,
        uncertainty_error_pairs,
        num_bins,
        target_indices,
        num_folds=num_folds,
        combine_middle_bins=combine_middle_bins,
    )
    all_jaccard_data = all_jaccard_data_dict["Jaccard All"]
    all_recall_data = all_jaccard_data_dict["Recall All"]
    all_precision_data = all_jaccard_data_dict["Precision All"]

    all_bins_concat_targets_sep_all_jacc = all_jaccard_data_dict[
        "all jacc concat bins targets sep all"
    ]  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

    bound_return_dict = evaluate_bounds(
        bounds_all_targets,
        bins_all_targets,
        uncertainty_error_pairs,
        num_bins,
        target_indices,
        num_folds,
        combine_middle_bins=combine_middle_bins,
    )

    all_bound_data = bound_return_dict["Error Bounds All"]

    all_bins_concat_targets_sep_all_errorbound = bound_return_dict[
        "all errorbound concat bins targets sep all"
    ]  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

    generate_summary_df(
        all_error_data_dict,
        [["all mean error bins nosep", "All Targets"]],
        "Mean error",
        os.path.join(save_folder, "target_errors.xlsx"),
    )

    if interpret:
        # If we have combined the middle bins, we are only displaying 3 bins (outer edges, and combined middle bins).
        if combine_middle_bins:
            num_bins_display = 3
        else:
            num_bins_display = num_bins

        if save_figures_bool:
            save_location = save_folder
        else:
            save_location = None

        # Plot piecewise linear regression for error/uncertainty prediction.
        if display_settings["correlation"]:
            _ = evaluate_correlations(
                bins_all_targets,
                uncertainty_error_pairs,
                cmaps,
                num_bins,
                confidence_invert,
                num_folds=num_folds,
                error_scaling_factor=error_scaling_factor,
                combine_middle_bins=combine_middle_bins,
                save_path=save_location,
                to_log=True,
            )

        # Plot cumulative error figure for all predictions
        if display_settings["cumulative_error"]:
            plot_cumulative(
                cmaps,
                bins_all_targets,
                models_to_compare,
                uncertainty_error_pairs,
                np.arange(num_bins),
                "Cumulative error for ALL predictions, dataset " + dataset,
                save_path=save_location,
                error_scaling_factor=error_scaling_factor,
            )
            # Plot cumulative error figure for B1 only predictions
            plot_cumulative(
                cmaps,
                bins_all_targets,
                models_to_compare,
                uncertainty_error_pairs,
                0,
                "Cumulative error for B1 predictions, dataset " + dataset,
                save_path=save_location,
                error_scaling_factor=error_scaling_factor,
            )

            # Plot cumulative error figure comparing B1 and ALL, for both models
            for model_type in models_to_compare:
                plot_cumulative(
                    cmaps,
                    bins_all_targets,
                    [model_type],
                    uncertainty_error_pairs,
                    0,
                    model_type + ". Cumulative error comparing ALL and B1, dataset " + dataset,
                    compare_to_all=True,
                    save_path=save_location,
                    error_scaling_factor=error_scaling_factor,
                )

        # Set x_axis labels for following plots.
        x_axis_labels = [r"$B_{{{}}}$".format(num_bins_display + 1 - (i + 1)) for i in range(num_bins_display + 1)]

        # get error bounds
        if display_settings["errors"]:
            # mean error concat for each bin
            logger.info("mean error concat all L")
            if save_figures_bool:
                if samples_as_dots_bool:
                    dotted_addition = "_dotted"
                else:
                    dotted_addition = "_undotted"
                save_location = os.path.join(
                    save_folder, save_file_preamble + dotted_addition + "_error_all_targets.pdf"
                )

            box_plot_per_model(
                cmaps,
                all_bins_concat_targets_nosep_error,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Localization Error (mm)",
                num_bins=num_bins_display,
                convert_to_percent=False,
                show_sample_info=show_sample_info_mode,
                show_individual_dots=samples_as_dots_bool,
                y_lim=box_plot_error_lim,
                to_log=True,
                save_path=save_location,
            )

            if show_individual_target_plots:
                # plot the concatentated errors for each target seperately
                for idx_l, target_data in enumerate(all_bins_concat_targets_sep_all_error):
                    if idx_l in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder,
                                save_file_preamble + dotted_addition + "_error_target_" + str(idx_l) + ".pdf",
                            )

                        logger.info("individual error for T%s", idx_l)

                        box_plot_per_model(
                            cmaps,
                            target_data,
                            uncertainty_error_pairs,
                            models_to_compare,
                            x_axis_labels=x_axis_labels,
                            x_label="Uncertainty Thresholded Bin",
                            y_label="Error (mm)",
                            num_bins=num_bins_display,
                            convert_to_percent=False,
                            show_sample_info=show_sample_info_mode,
                            show_individual_dots=samples_as_dots_bool,
                            y_lim=box_plot_error_lim,
                            to_log=True,
                            save_path=save_location,
                        )

            logger.info("Mean error")

            if save_figures_bool:
                save_location = os.path.join(
                    save_folder, save_file_preamble + dotted_addition + "mean_error_folds_all_targets.pdf"
                )

            box_plot_per_model(
                cmaps,
                all_error_data,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Mean Error (mm)",
                num_bins=num_bins_display,
                convert_to_percent=False,
                y_lim=box_plot_error_lim,
                to_log=True,
                save_path=save_location,
            )

        # Plot Error Bound Accuracy
        if display_settings["error_bounds"]:
            logger.info(" errorbound acc for all targets.")
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_errorbound_all_targets.pdf")

            generic_box_plot_loop(
                cmaps,
                all_bound_data,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Error Bound Accuracy (%)",
                num_bins=num_bins_display,
                save_path=save_location,
                y_lim=120,
                width=0.2,
                y_lim_min=-2,
                font_size_1=30,
                font_size_2=30,
                show_individual_dots=False,
                list_comp_bool=False,
            )

            if show_individual_target_plots:
                # plot the concatentated error bounds for each target seperately
                for idx_l, target_data in enumerate(all_bins_concat_targets_sep_all_errorbound):
                    if idx_l in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder, save_file_preamble + "_errorbound_target_" + str(idx_l) + ".pdf"
                            )

                        logger.info("individual errorbound acc for T%s", idx_l)

                        generic_box_plot_loop(
                            cmaps,
                            target_data,
                            uncertainty_error_pairs,
                            models_to_compare,
                            x_axis_labels=x_axis_labels,
                            x_label="Uncertainty Thresholded Bin",
                            y_label="Error Bound Accuracy (%)",
                            num_bins=num_bins_display,
                            save_path=save_location,
                            y_lim=120,
                            width=0.2,
                            y_lim_min=-2,
                            font_size_1=30,
                            font_size_2=30,
                            show_individual_dots=False,
                            list_comp_bool=False,
                        )

        # Plot Jaccard Index
        if display_settings["jaccard"]:
            logger.info("Plot jaccard for all targets.")
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_jaccard_all_targets.pdf")

            generic_box_plot_loop(
                cmaps,
                all_jaccard_data,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Jaccard Index (%)",
                num_bins=num_bins_display,
                save_path=save_location,
                y_lim=70,
                width=0.2,
                y_lim_min=-2,
                font_size_1=30,
                font_size_2=30,
                show_individual_dots=False,
                list_comp_bool=False,
            )

            # mean recall for each bin
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_recall_jaccard_all_targets.pdf")

            generic_box_plot_loop(
                cmaps,
                all_recall_data,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Ground Truth Bins Recall",
                num_bins=num_bins_display,
                convert_to_percent=True,
                save_path=save_location,
                y_lim=120,
                width=0.2,
                y_lim_min=-2,
                font_size_1=30,
                font_size_2=30,
                show_individual_dots=False,
                list_comp_bool=False,
            )

            # mean precision for each bin
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_precision_jaccard_all_targets.pdf")

            generic_box_plot_loop(
                cmaps,
                all_precision_data,
                uncertainty_error_pairs,
                models_to_compare,
                x_axis_labels=x_axis_labels,
                x_label="Uncertainty Thresholded Bin",
                y_label="Ground Truth Bins Precision",
                num_bins=num_bins_display,
                convert_to_percent=True,
                save_path=save_location,
                y_lim=120,
                width=0.2,
                y_lim_min=-2,
                font_size_1=30,
                font_size_2=30,
                show_individual_dots=False,
                list_comp_bool=False,
            )

            if show_individual_target_plots:
                # plot the jaccard index for each target seperately

                for idx_l, target_data in enumerate(all_bins_concat_targets_sep_all_jacc):
                    if idx_l in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder, save_file_preamble + "jaccard_target_" + str(idx_l) + ".pdf"
                            )

                        logger.info("individual jaccard for T%s", idx_l)

                        generic_box_plot_loop(
                            cmaps,
                            target_data,
                            uncertainty_error_pairs,
                            models_to_compare,
                            x_axis_labels=x_axis_labels,
                            x_label="Uncertainty Thresholded Bin",
                            y_label="Jaccard Index (%)",
                            num_bins=num_bins_display,
                            save_path=save_location,
                            y_lim=70,
                            width=0.2,
                            y_lim_min=-2,
                            font_size_1=30,
                            font_size_2=30,
                            show_individual_dots=False,
                            list_comp_bool=False,
                        )


def generate_fig_comparing_bins(
    data: Tuple,
    display_settings: Dict[str, Any],
) -> None:
    """
    Generate figures comparing localization error, error bounds accuracy, and Jaccard index for different binning
    configurations.

    Args:
        data (Tuple): A tuple containing various inputs needed to generate the figures. The tuple should include the following elements:
            - uncertainty_error_pair (Tuple[float, float]): A tuple representing the mean and standard deviation of
            the noise uncertainty used during training and evaluation.
            - model (str): The name of the model being evaluated.
            - dataset (str): The name of the dataset being used.
            - targets (List[int]): A list of target indices being evaluated.
            - all_values_q (List[int]): A list of integers representing the number of bins being used for each evaluation.
            - cmaps (List[str]): A list of colormap names to use for plotting.
            - all_fitted_save_paths (List[str]): A list of file paths where the binned data is stored.
            - save_folder (str): The directory where the figures should be saved.
            - save_file_preamble (str): The prefix to use for all figure file names.
            - combine_middle_bins (bool): Whether to combine the middle bins or not.
            - save_figures_bool (bool): Whether to save the generated figures or not. If false, shows instead.
            - samples_as_dots_bool (bool): Whether to show individual samples as dots in the box plots or not.
            - show_sample_info_mode (str): The mode for showing sample information in the box plots.
            - box_plot_error_lim (float): The y-axis limit for the error box plots.
            - show_individual_target_plots (bool): Whether to generate individual plots for each target.
            - interpret (bool): Whether the results are being interpreted.
            - num_folds (int): The number of cross-validation folds to use.
            - ind_targets_to_show (List[int]): A list of target indices to show in individual plots.
            - error_scaling_factor (float, optional): Scaling factor for error. Defaults to 1.0.

        display_settings: Dictionary containing the following keys:
            - 'hatch': String representing the type of hatch pattern to use in the plots.
            - 'color': String representing the color to use for the plots.

    Returns:
        None.
    """

    # Unpack data and logging settings
    [
        uncertainty_error_pair,
        model,
        dataset,
        targets,
        all_values_q,
        cmaps,
        all_fitted_save_paths,
        save_folder,
        save_file_preamble,
        combine_middle_bins,  # cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"]
        save_figures_bool,  # cfg["OUTPUT"]["SAVE_FIGURES"]
        samples_as_dots_bool,  # cfg["BOXPLOT"]["SAMPLES_AS_DOTS"]
        show_sample_info_mode,  # cfg["BOXPLOT"]["SHOW_SAMPLE_INFO_MODE"]
        box_plot_error_lim,  # cfg["BOXPLOT"]["ERROR_LIM"]
        show_individual_target_plots,
        interpret,
        num_folds,
        ind_targets_to_show,
        error_scaling_factor,
    ] = data

    logger = logging.getLogger("qbin")

    hatch = display_settings["hatch"]
    color = display_settings["color"]

    # increse dimension of these for compatibility with future methods
    model_list = [model]
    uncertainty_error_pair_list = [uncertainty_error_pair]

    # If combining the middle bins we just have the 2 edge bins, and the combined middle ones.

    all_error_data = []
    all_error_target_sep = []
    all_bins_concat_targets_nosep_error = []
    all_bins_concat_targets_sep_foldwise_error = []
    all_bins_concat_targets_sep_all_error = []
    all_jaccard_data = []
    all_recall_data = []
    all_precision_data = []
    all_bins_concat_targets_sep_foldwise_jacc = []
    all_bins_concat_targets_sep_all_jacc = []
    all_bound_data = []
    all_bins_concat_targets_sep_foldwise_errorbound = []
    all_bins_concat_targets_sep_all_errorbound = []

    for idx, num_bins in enumerate(all_values_q):
        saved_bins_path_pre = all_fitted_save_paths[idx]

        bins_all_targets, bins_targets_sep, bounds_all_targets, bounds_targets_sep = generate_struct_for_qbin(
            model_list, targets, saved_bins_path_pre, dataset
        )

        # Get mean errors bin-wise, get all errors concatenated together bin-wise, and seperate by target.
        all_error_data_dict = get_mean_errors(
            bins_all_targets,
            uncertainty_error_pair_list,
            num_bins,
            targets,
            num_folds=num_folds,
            error_scaling_factor=error_scaling_factor,
            combine_middle_bins=combine_middle_bins,
        )
        all_error_data.append(all_error_data_dict["all mean error bins nosep"])
        all_error_target_sep.append(all_error_data_dict["all mean error bins targets sep"])

        all_bins_concat_targets_nosep_error.append(
            all_error_data_dict["all error concat bins targets nosep"]
        )  # shape is [num bins]
        all_bins_concat_targets_sep_foldwise_error.append(
            all_error_data_dict["all error concat bins targets sep foldwise"]
        )  # shape is [num targets][num bins]
        all_bins_concat_targets_sep_all_error.append(
            all_error_data_dict["all error concat bins targets sep all"]
        )  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

        all_jaccard_data_dict = evaluate_jaccard(
            bins_all_targets,
            uncertainty_error_pair_list,
            num_bins,
            targets,
            num_folds=num_folds,
            combine_middle_bins=combine_middle_bins,
        )
        all_jaccard_data.append(all_jaccard_data_dict["Jaccard All"])
        all_recall_data.append(all_jaccard_data_dict["Recall All"])
        all_precision_data.append(all_jaccard_data_dict["Precision All"])
        all_bins_concat_targets_sep_foldwise_jacc.append(
            all_jaccard_data_dict["all jacc concat bins targets sep foldwise"]
        )  # shape is [num targets][num bins]
        all_bins_concat_targets_sep_all_jacc.append(
            all_jaccard_data_dict["all jacc concat bins targets sep all"]
        )  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

        bound_return_dict = evaluate_bounds(
            bounds_all_targets,
            bins_all_targets,
            uncertainty_error_pair_list,
            num_bins,
            targets,
            num_folds,
            combine_middle_bins=combine_middle_bins,
        )

        all_bound_data.append(bound_return_dict["Error Bounds All"])
        all_bins_concat_targets_sep_foldwise_errorbound.append(
            bound_return_dict["all errorbound concat bins targets sep foldwise"]
        )  # shape is [num targets][num bins]
        all_bins_concat_targets_sep_all_errorbound.append(
            bound_return_dict["all errorbound concat bins targets sep all"]
        )  # same as all_bins_concat_targets_sep_foldwise but folds are flattened to a single list

    if interpret:
        # If we have combined the middle bins, we are only displaying 3 bins (outer edges, and combined middle bins).
        if combine_middle_bins:
            num_bins_display = 3
        else:
            num_bins_display = num_bins

        # Set x_axis labels for following plots.
        x_axis_labels = [str(x) for x in all_values_q]
        save_location = None

        # get error bounds

        if display_settings["errors"]:
            # mean error concat for each bin
            logger.info("mean error concat all L")
            if save_figures_bool:
                if samples_as_dots_bool:
                    dotted_addition = "_dotted"
                else:
                    dotted_addition = "_undotted"
                save_location = os.path.join(
                    save_folder, save_file_preamble + dotted_addition + "_error_all_targets.pdf"
                )

            box_plot_comparing_q(
                all_bins_concat_targets_nosep_error,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Localization Error (mm)",
                num_bins_display=num_bins_display,
                convert_to_percent=False,
                show_sample_info=show_sample_info_mode,
                show_individual_dots=samples_as_dots_bool,
                y_lim=box_plot_error_lim,
                to_log=True,
                save_path=save_location,
            )

            if show_individual_target_plots:
                # plot the concatentated errors for each target seperately. Must transpose the iteration.
                for target_idx in targets:
                    target_data = [x[target_idx] for x in all_bins_concat_targets_sep_all_error]

                    if target_idx in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder,
                                save_file_preamble + dotted_addition + "_error_target_" + str(target_idx) + ".pdf",
                            )

                        logger.info("individual error for T%s", target_idx)
                        box_plot_comparing_q(
                            target_data,
                            uncertainty_error_pair_list,
                            model_list,
                            hatch_type=hatch,
                            color=color,
                            x_axis_labels=x_axis_labels,
                            x_label="Q (# Bins)",
                            y_label="Localization Error (mm)",
                            num_bins_display=num_bins_display,
                            convert_to_percent=False,
                            show_sample_info=show_sample_info_mode,
                            show_individual_dots=samples_as_dots_bool,
                            y_lim=box_plot_error_lim,
                            to_log=True,
                            save_path=save_location,
                        )

            if save_figures_bool:
                save_location = os.path.join(
                    save_folder, save_file_preamble + dotted_addition + "mean_error_folds_all_targets.pdf"
                )
            box_plot_comparing_q(
                all_error_data,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Mean Error (mm)",
                num_bins_display=num_bins_display,
                convert_to_percent=False,
                show_sample_info="None",
                show_individual_dots=False,
                y_lim=box_plot_error_lim,
                to_log=True,
                save_path=save_location,
            )

        # Plot Error Bound Accuracy

        if display_settings["error_bounds"]:
            logger.info(" errorbound acc for all targets.")
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_errorbound_all_targets.pdf")

            box_plot_comparing_q(
                all_bound_data,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Error Bound Accuracy (%)",
                num_bins_display=num_bins_display,
                convert_to_percent=True,
                show_sample_info="None",
                show_individual_dots=False,
                y_lim=100,
                save_path=save_location,
            )

            if show_individual_target_plots:
                # plot the concatentated errors for each target seperately. Must transpose the iteration.
                for target_idx in targets:
                    target_data = [x[target_idx] for x in all_bins_concat_targets_sep_all_errorbound]

                    if target_idx in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder, save_file_preamble + "_errorbound_target_" + str(target_idx) + ".pdf"
                            )

                        logger.info("individual errorbound acc for T%s", target_idx)
                        box_plot_comparing_q(
                            target_data,
                            uncertainty_error_pair_list,
                            model_list,
                            hatch_type=hatch,
                            color=color,
                            x_axis_labels=x_axis_labels,
                            x_label="Q (# Bins)",
                            y_label="Error Bound Accuracy (%)",
                            num_bins_display=num_bins_display,
                            convert_to_percent=True,
                            show_individual_dots=False,
                            y_lim=100,
                            save_path=save_location,
                        )

        # Plot Jaccard Index
        if display_settings["jaccard"]:
            logger.info("Plot jaccard for all targets.")
            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_jaccard_all_targets.pdf")

            box_plot_comparing_q(
                all_jaccard_data,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Jaccard Index (%)",
                num_bins_display=num_bins_display,
                convert_to_percent=True,
                show_individual_dots=False,
                y_lim=70,
                save_path=save_location,
            )

            # mean recall for each bin
            logger.info("Plot recall for all targets.")

            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_recall_jaccard_all_targets.pdf")
            box_plot_comparing_q(
                all_recall_data,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Ground Truth Bin Recall (%)",
                num_bins_display=num_bins_display,
                convert_to_percent=True,
                show_individual_dots=False,
                y_lim=120,
                save_path=save_location,
            )

            # mean precision for each bin
            logger.info("Plot precision for all targets.")

            if save_figures_bool:
                save_location = os.path.join(save_folder, save_file_preamble + "_precision_jaccard_all_targets.pdf")
            box_plot_comparing_q(
                all_precision_data,
                uncertainty_error_pair_list,
                model_list,
                hatch_type=hatch,
                color=color,
                x_axis_labels=x_axis_labels,
                x_label="Q (# Bins)",
                y_label="Ground Truth Bin Precision (%)",
                num_bins_display=num_bins_display,
                convert_to_percent=True,
                show_individual_dots=False,
                y_lim=120,
                save_path=save_location,
            )

            if show_individual_target_plots:
                # plot the concatentated errors for each target seperately. Must transpose the iteration.
                for target_idx in targets:
                    target_data = [x[target_idx] for x in all_bins_concat_targets_sep_all_jacc]

                    if target_idx in ind_targets_to_show or ind_targets_to_show == [-1]:
                        if save_figures_bool:
                            save_location = os.path.join(
                                save_folder, save_file_preamble + "jaccard_target_" + str(target_idx) + ".pdf"
                            )

                        logger.info("individual jaccard for T%s", target_idx)
                        box_plot_comparing_q(
                            target_data,
                            uncertainty_error_pair_list,
                            model_list,
                            hatch_type=hatch,
                            color=color,
                            x_axis_labels=x_axis_labels,
                            x_label="Q (# Bins)",
                            y_label="Jaccard Index (%)",
                            num_bins_display=num_bins_display,
                            convert_to_percent=True,
                            show_individual_dots=False,
                            y_lim=70,
                            save_path=save_location,
                        )
</file>

<file path="kale/interpret/visualize.py">
# =============================================================================
# Author: Shuo Zhou, shuo.zhou@sheffield.ac.uk
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns


def _none2dict(kwarg):
    if kwarg is None:
        return {}
    else:
        return kwarg


def plot_weights(
    weight_img, background_img=None, color_marker_pos="rs", color_marker_neg="gs", im_kwargs=None, marker_kwargs=None
):
    """Visualize model weights

    Args:
        weight_img (array-like): Model weight/coefficients in 2D, could be a 2D slice of a 3D or higher order tensor.
        background_img (array-like, optional): 2D background image. Defaults to None.
        color_marker_pos (str, optional): Color and marker for weights in positive values. Defaults to red "rs".
        color_marker_neg (str, optional): Color and marker for weights in negative values. Defaults to blue "gs".
        im_kwargs (dict, optional): Keyword arguments for background images. Defaults to None.
        marker_kwargs (dict, optional): Keyword arguments for background images. Defaults to None.

    Returns:
        [matplotlib.figure.Figure]: Figure to plot.
    """
    if not isinstance(weight_img, np.ndarray):
        weight_img = np.array(weight_img)
    if len(weight_img.shape) != 2:
        raise ValueError(
            "weight_img is expected to be a 2D matrix, but got an array in shape %s" % str(weight_img.shape)
        )
    im_kwargs = _none2dict(im_kwargs)
    marker_kwargs = _none2dict(marker_kwargs)
    fig = plt.figure()
    ax = fig.add_subplot()
    if background_img is not None:
        ax.imshow(background_img, **im_kwargs)
        weight_img[np.where(background_img == 0)] = 0

    weight_pos_coords = np.where(weight_img > 0)
    weight_neg_coords = np.where(weight_img < 0)

    ax.plot(weight_pos_coords[1], weight_pos_coords[0], color_marker_pos, **marker_kwargs)
    ax.plot(weight_neg_coords[1], weight_neg_coords[0], color_marker_neg, **marker_kwargs)

    return fig


def plot_multi_images(
    images,
    n_cols=1,
    n_rows=None,
    marker_locs=None,
    image_titles=None,
    marker_titles=None,
    marker_cmap=None,
    figsize=None,
    im_kwargs=None,
    marker_kwargs=None,
    legend_kwargs=None,
    title_kwargs=None,
):
    """Plot multiple images with markers in one figure.

    Args:
        images (array-like): Images to plot, shape(n_samples, dim1, dim2)
        n_cols (int, optional): Number of columns for plotting multiple images. Defaults to 1.
        n_rows (int, optional): Number of rows for plotting multiple images. If None, n_rows = n_samples / n_cols.
        marker_locs (array-like, optional): Locations of markers, shape (n_samples, 2 * n_markers). Defaults to None.
        marker_titles (list, optional): Names of the markers, where len(marker_names) == n_markers. Defaults to None.
        marker_cmap (str, optional): Name of the color map used for plotting markers. Default to None.
        image_titles (list, optional): List of title for each image, where len(image_names) == n_samples.
            Defaults to None.
        figsize (tuple, optional): Figure size. Defaults to None.
        im_kwargs (dict, optional): Keyword arguments for plotting images. Defaults to None.
        marker_kwargs (dict, optional): Keyword arguments for markers. Defaults to None.
        legend_kwargs (dict, optional): Keyword arguments for legend. Defaults to None.
        title_kwargs (dict, optional): Keyword arguments for title. Defaults to None.

    Returns:
        [matplotlib.figure.Figure]: Figure to plot.
    """
    image_var_type = type(images)
    if image_var_type == np.ndarray:
        n_samples = images.shape[0]
    elif image_var_type == list:
        n_samples = len(images)
    else:
        raise ValueError("Unsupported variable type %s for 'images'" % image_var_type)
    if n_rows is None:
        n_rows = int(n_samples / n_cols) + 1
    im_kwargs = _none2dict(im_kwargs)
    marker_kwargs = _none2dict(marker_kwargs)
    legend_kwargs = _none2dict(legend_kwargs)
    title_kwargs = _none2dict(title_kwargs)
    if figsize is None:
        figsize = (20, 36)
    fig = plt.figure(figsize=figsize)
    if image_titles is None:
        image_titles = np.arange(n_samples) + 1
    elif not isinstance(image_titles, list) or len(image_titles) != n_samples:
        raise ValueError("Invalid type or length of 'image_names'!")
    if marker_cmap is None:
        marker_colors = None
    elif isinstance(marker_cmap, str):
        marker_colors = plt.get_cmap(marker_cmap).colors
    else:
        raise ValueError("Unsupported type %s for argument 'marker_cmap'" % type(marker_cmap))
    for i in range(n_samples):
        fig.add_subplot(n_rows, n_cols, i + 1)
        plt.axis("off")
        plt.imshow(images[i], **im_kwargs)
        if marker_locs is not None:
            coords = marker_locs[i, :].reshape((-1, 2))
            n_marker = coords.shape[0]
            if marker_titles is not None and len(marker_titles) == n_marker:
                plt_legend = True
            else:
                plt_legend = False
            for j in range(n_marker):
                ix = coords[j, 0]
                iy = coords[j, 1]
                if marker_colors is not None:
                    marker_kwargs["color"] = marker_colors[j]
                if plt_legend:
                    marker_kwargs["label"] = marker_titles[j]
                plt.scatter(ix, iy, **marker_kwargs)
            if plt_legend:
                plt.legend(**legend_kwargs)
        plt.title(image_titles[i], **title_kwargs)

    return fig


def distplot_1d(
    data,
    labels=None,
    xlabel=None,
    ylabel=None,
    title=None,
    figsize=None,
    colors=None,
    title_kwargs=None,
    hist_kwargs=None,
):
    """Plot distribution of 1D data.

    Args:
        data (array-like or list): Data to plot.
        labels (list, optional): List of labels for each data. Defaults to None.
        xlabel (str, optional): Label for x-axis. Defaults to None.
        ylabel (str, optional): Label for y-axis. Defaults to None.
        title (str, optional): Title of the plot. Defaults to None.
        figsize (tuple, optional): Figure size. Defaults to None.
        colors (str, optional): Color of the line. Defaults to None.
        title_kwargs (dict, optional): Keyword arguments for title. Defaults to None.
        hist_kwargs (dict, optional): Keyword arguments for histogram. Defaults to None.

    Returns:
        [matplotlib.figure.Figure]: Figure to plot.
    """
    hist_kwargs = _none2dict(hist_kwargs)
    title_kwargs = _none2dict(title_kwargs)

    fig = plt.figure(figsize=figsize)
    if colors is None:
        colors = plt.get_cmap("Set1").colors

    if not isinstance(data, list):
        data = [data]

    if labels is None:
        labels = np.range(len(data))

    for i in range(len(data)):
        sns.histplot(data[i], color=colors[i], label=labels[i], **hist_kwargs)

    if title is not None:
        plt.title(title, **title_kwargs)
    if xlabel is not None:
        plt.xlabel(xlabel)
    if ylabel is not None:
        plt.ylabel(ylabel)
    plt.legend()

    return fig
</file>

<file path="kale/loaddata/avmnist_datasets.py">
"""Dataset setting and data loader for AVMNIST dataset
by refactoring https://github.com/pliang279/MultiBench/blob/main/datasets/avmnist/get_data.py"""

import numpy as np
import torch
from torch.utils.data import DataLoader


class AVMNISTDataset:
    """This class loads the AVMNIST data stored in a specified directory, and prepares it for training, validation, and testing.
    This class also takes care of the pre-processing steps such as reshaping and normalizing the data based on provided
    arguments. This includes options to flatten the audio and image data, normalize the image and audio data, and
    add a dimension to the data, often used to represent the channel in image or audio data.
    Furthermore, The class handles the splitting of data into training and validation sets. It provides separate data
    loaders for the training, validation, and testing sets, which can be used to iterate over the data during model
    training and evaluation.
    This data loader class simplifies the data preparation process for multimodal learning tasks, allowing the user to
    focus on model architecture and hyperparameter tuning.

    Args:
        data_dir (str): Directory of data.
        batch_size (int, optional): Batch size. Defaults to 40.
        flatten_audio (bool, optional): Whether to flatten audio data or not. Defaults to False.
        flatten_image (bool, optional): Whether to flatten image data or not. Defaults to False.
        unsqueeze_channel (bool, optional): Whether to unsqueeze any channels or not. Defaults to True.
        normalize_image (bool, optional): Whether to normalize the images before returning. Defaults to True.
        normalize_audio (bool, optional): Whether to normalize the audio before returning. Defaults to True.
    """

    def __init__(
        self,
        data_dir,
        batch_size=40,
        flatten_audio=False,
        flatten_image=False,
        unsqueeze_channel=True,
        normalize_image=True,
        normalize_audio=True,
    ):
        self.data_dir = data_dir
        self.flatten_audio = flatten_audio
        self.flatten_image = flatten_image
        self.unsqueeze_channel = unsqueeze_channel
        self.normalize_image = normalize_image
        self.normalize_audio = normalize_audio
        self.batch_size = batch_size
        self.load_data()

    def load_data(self):
        trains = [
            np.load(self.data_dir + "/image/train_data.npy"),
            np.load(self.data_dir + "/audio/train_data.npy"),
            np.load(self.data_dir + "/train_labels.npy"),
        ]
        tests = [
            np.load(self.data_dir + "/image/test_data.npy"),
            np.load(self.data_dir + "/audio/test_data.npy"),
            np.load(self.data_dir + "/test_labels.npy"),
        ]
        train_valid_size = len(trains[0])
        test_size = len(tests[0])

        if self.flatten_audio:
            trains[1] = trains[0].reshape(train_valid_size, 112 * 112)
            tests[1] = tests[0].reshape(test_size, 112 * 112)

        if self.normalize_image:
            trains[0] = trains[0].astype("float64")
            trains[0] /= 255.0
            tests[0] = tests[0].astype("float64")
            tests[0] /= 255.0
        if self.normalize_audio:
            trains[1] = trains[1].astype("float64")
            trains[1] /= 255.0
            tests[1] = tests[1].astype("float64")
            tests[1] /= 255.0
        if not self.flatten_image:
            trains[0] = trains[0].reshape(train_valid_size, 28, 28)
            tests[0] = tests[0].reshape(test_size, 28, 28)
        if self.unsqueeze_channel:
            trains[0] = np.expand_dims(trains[0], 1)
            tests[0] = np.expand_dims(tests[0], 1)
            trains[1] = np.expand_dims(trains[1], 1)
            tests[1] = np.expand_dims(tests[1], 1)
        trains[2] = trains[2].astype(int)
        tests[2] = tests[2].astype(int)

        self.train_valid_data = [[trains[j][i] for j in range(3)] for i in range(train_valid_size)]
        self.test_data = [[tests[j][i] for j in range(3)] for i in range(test_size)]

        train_size = int(train_valid_size * 0.9)
        valid_size = train_valid_size - train_size

        self.train_data, self.valid_data = torch.utils.data.random_split(
            self.train_valid_data, [train_size, valid_size]
        )

    def get_train_loader(self, shuffle=True):
        return DataLoader(self.train_data, shuffle=shuffle, num_workers=4, batch_size=self.batch_size)

    def get_valid_loader(self, shuffle=False):
        return DataLoader(self.valid_data, shuffle=shuffle, num_workers=4, batch_size=self.batch_size)

    def get_test_loader(self, shuffle=False):
        return DataLoader(self.test_data, shuffle=shuffle, num_workers=4, batch_size=self.batch_size)
</file>

<file path="kale/loaddata/dataset_access.py">
"""
Dataset Access API adapted from https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/dataset_access.py
"""

import torch
import torch.utils.data


class DatasetAccess:
    """
    This class ensures a unique API is used to access training, validation and test splits
    of any dataset.

    Args:
        n_classes (int): the number of classes.
    """

    def __init__(self, n_classes):
        self._n_classes = n_classes

    def n_classes(self):
        return self._n_classes

    def get_train(self):
        """
        Returns: a torch.utils.data.Dataset
            Dataset: a torch.utils.data.Dataset
        """
        raise NotImplementedError()

    def get_train_valid(self, valid_ratio):
        """
        Randomly split a dataset into non-overlapping training and validation datasets.

        Args:
            valid_ratio (float): the ratio for validation set

        Returns:
            Dataset: a torch.utils.data.Dataset
        """
        train_dataset = self.get_train()

        if valid_ratio == 0:
            return train_dataset, train_dataset
        else:
            return split_by_ratios(train_dataset, [valid_ratio])

    def get_test(self):
        raise NotImplementedError()


def get_class_subset(dataset, class_ids):
    """
    Args:
        dataset: a torch.utils.data.Dataset
        class_ids (list, optional): List of chosen subset of class ids.
    Returns: a torch.utils.data.Dataset
        Dataset: a torch.utils.data.Dataset with only classes in class_ids
    """
    sub_indices = [i for i in range(0, len(dataset)) if dataset[i][1] in class_ids]
    return torch.utils.data.Subset(dataset, sub_indices)


def split_by_ratios(dataset, split_ratios):
    """Randomly split a dataset into non-overlapping new datasets of given ratios.

    Args:
        dataset (torch.utils.data.Dataset): Dataset to be split.
        split_ratios (list): Ratios of splits to be produced, where 0 < sum(split_ratios) <= 1.

    Returns:
        [List]: A list of subsets.

    Examples:
        >>> import torch
        >>> from kale.loaddata.dataset_access import split_by_ratios
        >>> subset1, subset2 = split_by_ratios(range(10), [0.3, 0.7])
        >>> len(subset1)
        3
        >>> len(subset2)
        7
        >>> subset1, subset2 = split_by_ratios(range(10), [0.3])
        >>> len(subset1)
        3
        >>> len(subset2)
        7
        >>> subset1, subset2, subset3 = split_by_ratios(range(10), [0.3, 0.3])
        >>> len(subset1)
        3
        >>> len(subset2)
        3
        >>> len(subset3)
        4
    """
    n_total = len(dataset)
    ratio_sum = sum(split_ratios)
    if ratio_sum > 1 or ratio_sum <= 0:
        raise ValueError("The sum of ratios should be in range(0, 1]")
    elif ratio_sum == 1:
        split_ratios_ = split_ratios[:-1]
    else:
        split_ratios_ = split_ratios.copy()
    split_sizes = [int(n_total * ratio_) for ratio_ in split_ratios_]
    split_sizes.append(n_total - sum(split_sizes))

    return torch.utils.data.random_split(dataset, split_sizes)
</file>

<file path="kale/loaddata/few_shot.py">
"""
Dataset class to load data for few-shot learning problems under :math:`N`-way-:math:`K`-shot settings.
Author: Wenrui Fan
Email: winslow.fan@outlook.com
"""

import os
from typing import Callable, Optional

import numpy as np
import torch
from PIL import Image
from torch.utils.data import Dataset


class NWayKShotDataset(Dataset):
    """
    This Dataset class loads data for few-shot learning problems under :math:`N`-way-:math:`K`-shot settings.

    - :math:`N`-way: The number of classes under a particular setting. The model is presented with samples from these :math:`N` classes and needs to classify them. For example, 3-way means the model has to classify 3 different classes.

    - :math:`K`-shot: The number of samples for each class in the support set. For example, in a 2-shot setting, two support samples are provided per class.

    - Support set: It is a small, labeled dataset used to train the model with a few samples of each class. The support set consists of :math:`N` classes (:math:`N`-way), with :math:`K` samples (:math:`K`-shot) for each class. For example, under a 3-way-2-shot setting, the support set has 3 classes with 2 samples per class, totaling 6 samples.

    - Query set: It evaluates the model's ability to generalize what it has learned from the support set. It contains samples from the same :math:`N` classes but not included in the support set. Continuing with the 3-way-2-shot example, the query set would include additional samples from the 3 classes, which the model must classify after learning from the support set.

    In this class, ``__getitem__()`` returns a batch of images and labels for one class. When defining the ``training/validation/testing dataloaders``, the batch size should be the number of classes (``cfg.TRAIN.NUM_CLASSES``/``cfg.VAL.NUM_CLASSES``). Therefore, ``__len__()`` returns the total number of classes in the dataset.

    Note:
        The dataset should be organized as:

        - root
            - train
                - class_name 1
                    - xxx.png
                    - yyy.png
                    - ...
                - class_name 2
                    - xxx.png
                    - yyy.png
                    - ...
                - ...
            - val
                - class_name m
                    - xxx.png
                    - yyy.png
                    - ...
                - class_name m+1
                    - xxx.png
                    - yyy.png
                    - ...
                - ...
            - test
                - class_name n
                    - xxx.png
                    - yyy.png
                    - ...
                - class_name n+1
                    - xxx.png
                    - yyy.png
                    - ...
                - ...

    Args:
        path (string): The root directory of the data.
        mode (string): The mode of the type of dataset. It can be "train", "val", or "test". Default: "train".
        num_support_samples (int): Number of samples per class in the support set. It corresponds to :math:`K` in the :math:`N`-way-:math:`K`-shot setting. Default: 5.
        num_query_samples (int): Number of samples per class in the query set. Default: 15.
        transform (callable, optional): Transform of images. Default: None.
    """

    def __init__(
        self,
        path: str,
        mode: str = "train",
        num_support_samples: int = 5,
        num_query_samples: int = 15,
        transform: Optional[Callable] = None,
    ):
        super(NWayKShotDataset, self).__init__()

        self.root = path
        self.num_support_samples = num_support_samples
        self.num_query_samples = num_query_samples
        self.mode = mode
        self.transform = transform
        self.images = []  # type: list
        self.labels = []  # type: list
        self._load_data()

    def __len__(self):
        # returning the number of classes in the datasets
        return len(self.classes)

    def __getitem__(self, idx):
        # sampling images and labels from one class
        image_idx = self._get_idx(idx)
        images = self._sample_data(image_idx)
        assert isinstance(images, list)
        images = torch.stack(images)
        return images, idx

    def _get_idx(self, idx):
        # getting the indices of images for one class
        image_idx = np.random.choice(
            [i for (i, item) in enumerate(self.labels) if item == idx],
            self.num_support_samples + self.num_query_samples,
            replace=False,
        )
        return image_idx

    def _sample_data(self, image_idx):
        # loading sampled images and applying transform
        images = [self.transform(self.images[index]) if self.transform else self.images[index] for index in image_idx]
        return images

    def _load_data(self):
        # loading image list from the root directory
        data_path = os.path.join(self.root, self.mode)
        classes = os.listdir(data_path)
        classes.sort()
        self.classes = classes
        for i, c in enumerate(classes):
            c_dir = os.path.join(data_path, c)
            imgs = os.listdir(c_dir)
            for img in imgs:
                img_path = os.path.join(c_dir, img)
                self.images.append(Image.open(img_path).convert("RGB"))
                self.labels.append(i)
</file>

<file path="kale/loaddata/image_access.py">
import glob
import logging
import os
from enum import Enum

import numpy as np
import pydicom
import torch
from torchvision import datasets, transforms

from kale.loaddata.dataset_access import DatasetAccess
from kale.loaddata.mnistm import MNISTM
from kale.loaddata.multi_domain import MultiDomainAccess, MultiDomainImageFolder
from kale.loaddata.usps import USPS
from kale.prepdata.image_transform import get_transform
from kale.utils.download import download_file_by_url


class DigitDataset(Enum):
    MNIST = "MNIST"
    MNIST_RGB = "MNIST_RGB"
    MNISTM = "MNISTM"
    USPS = "USPS"
    USPS_RGB = "USPS_RGB"
    SVHN = "SVHN"

    @staticmethod
    def get_channel_numbers(dataset: "DigitDataset"):
        channel_numbers = {
            DigitDataset.MNIST: 1,
            DigitDataset.MNIST_RGB: 3,
            DigitDataset.MNISTM: 3,
            DigitDataset.USPS: 1,
            DigitDataset.USPS_RGB: 3,
            DigitDataset.SVHN: 3,
        }
        return channel_numbers[dataset]

    @staticmethod
    def get_digit_transform(dataset: "DigitDataset", n_channels):
        transform_names = {
            (DigitDataset.MNIST, 1): "mnist32",
            (DigitDataset.MNIST, 3): "mnist32rgb",
            (DigitDataset.MNIST_RGB, 3): "mnist32rgb",
            (DigitDataset.MNISTM, 3): "mnistm",
            (DigitDataset.USPS, 1): "usps32",
            (DigitDataset.USPS, 3): "usps32rgb",
            (DigitDataset.USPS_RGB, 3): "usps32rgb",
            (DigitDataset.SVHN, 3): "svhn",
        }

        return transform_names[(dataset, n_channels)]

    @staticmethod
    def get_access(dataset: "DigitDataset", data_path, num_channels=None):
        """Gets data loaders for digit datasets

        Args:
            dataset (DigitDataset): dataset name
            data_path (string): root directory of dataset
            num_channels (int): number of channels, defaults to None

        Examples::
            >>> data_access, num_channel = DigitDataset.get_access(dataset, data_path)
        """

        factories = {
            DigitDataset.MNIST: MNISTDatasetAccess,
            DigitDataset.MNIST_RGB: MNISTDatasetAccess,
            DigitDataset.MNISTM: MNISTMDatasetAccess,
            DigitDataset.USPS: USPSDatasetAccess,
            DigitDataset.USPS_RGB: USPSDatasetAccess,
            DigitDataset.SVHN: SVHNDatasetAccess,
        }
        if num_channels is None:
            num_channels = DigitDataset.get_channel_numbers(dataset)
        tf = DigitDataset.get_digit_transform(dataset, num_channels)

        return factories[dataset](data_path, tf), num_channels

    # Originally get_access
    @staticmethod
    def get_source_target(source: "DigitDataset", target: "DigitDataset", data_path):
        """Gets data loaders for source and target datasets

        Args:
            source (DigitDataset): source dataset name
            target (DigitDataset): target dataset name
            data_path (string): root directory of dataset

        Examples::
            >>> source_access, target_access, num_channel = DigitDataset.get_source_target(source, target, data_path)
        """
        src_n_channels = DigitDataset.get_channel_numbers(source)
        tgt_n_channels = DigitDataset.get_channel_numbers(target)
        num_channels = max(src_n_channels, tgt_n_channels)
        src_access, src_n_channels = DigitDataset.get_access(source, data_path, num_channels)
        tgt_access, tgt_n_channels = DigitDataset.get_access(target, data_path, num_channels)

        return src_access, tgt_access, num_channels


class DigitDatasetAccess(DatasetAccess):
    """Common API for digit dataset access

    Args:
        data_path (string): root directory of dataset
        transform_kind (string): types of image transforms
    """

    def __init__(self, data_path, transform_kind):
        super().__init__(n_classes=10)
        self._data_path = data_path
        self._transform = get_transform(transform_kind)


class MNISTDatasetAccess(DigitDatasetAccess):
    """
    MNIST data loader
    """

    def get_train(self):
        return datasets.MNIST(self._data_path, train=True, transform=self._transform, download=True)

    def get_test(self):
        return datasets.MNIST(self._data_path, train=False, transform=self._transform, download=True)


class MNISTMDatasetAccess(DigitDatasetAccess):
    """
    Modified MNIST (MNISTM) data loader
    """

    def get_train(self):
        return MNISTM(self._data_path, train=True, transform=self._transform, download=True)

    def get_test(self):
        return MNISTM(self._data_path, train=False, transform=self._transform, download=True)


class USPSDatasetAccess(DigitDatasetAccess):
    """
    USPS data loader
    """

    def get_train(self):
        return USPS(self._data_path, train=True, transform=self._transform, download=True)

    def get_test(self):
        return USPS(self._data_path, train=False, transform=self._transform, download=True)


class SVHNDatasetAccess(DigitDatasetAccess):
    """
    SVHN data loader
    """

    def get_train(self):
        return datasets.SVHN(self._data_path, split="train", transform=self._transform, download=True)

    def get_test(self):
        return datasets.SVHN(self._data_path, split="test", transform=self._transform, download=True)


OFFICE_DOMAINS = ["amazon", "caltech", "dslr", "webcam"]
office_transform = get_transform("office")


class OfficeAccess(MultiDomainImageFolder, DatasetAccess):
    """Common API for office dataset access

    Args:
        root (string): root directory of dataset
        transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed
            version. Defaults to office_transform.
        download (bool, optional): Whether to allow downloading the data if not found on disk. Defaults to False.

    References:
        [1] Saenko, K., Kulis, B., Fritz, M. and Darrell, T., 2010, September. Adapting visual category models to
        new domains. In European Conference on Computer Vision (pp. 213-226). Springer, Berlin, Heidelberg.
        [2] Griffin, Gregory and Holub, Alex and Perona, Pietro, 2007. Caltech-256 Object Category Dataset.
        California Institute of Technology. (Unpublished).
        https://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001.
        [3] Gong, B., Shi, Y., Sha, F. and Grauman, K., 2012, June. Geodesic flow kernel for unsupervised
        domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (pp. 2066-2073).
    """

    def __init__(self, root, transform=office_transform, download=False, **kwargs):
        if download:
            self.download(root)
        super(OfficeAccess, self).__init__(root, transform=transform, **kwargs)

    @staticmethod
    def download(path):
        """Download dataset.
        Office-31 source: https://www.cc.gatech.edu/~judy/domainadapt/#datasets_code
        Caltech-256 source: http://www.vision.caltech.edu/Image_Datasets/Caltech256/
        Data with this library is adapted from: http://www.stat.ucla.edu/~jxie/iFRAME/code/imageClassification.rar
        """
        url = "https://github.com/pykale/data/raw/main/images/office"

        if not os.path.exists(path):
            os.makedirs(path)
        for domain_ in OFFICE_DOMAINS:
            filename = "%s.zip" % domain_
            data_path = os.path.join(path, filename)
            if os.path.exists(data_path):
                logging.info(f"Data file {filename} already exists.")
                continue
            else:
                data_url = "%s/%s" % (url, filename)
                download_file_by_url(data_url, path, filename, "zip")
                logging.info(f"Download {data_url} to {data_path}")

        logging.info("[DONE]")
        return


class Office31(OfficeAccess):
    def __init__(self, root, **kwargs):
        """Office-31 Dataset. Consists of three domains: 'amazon', 'dslr', and 'webcam', with 31 image classes.

        Args:
            root (string): path to directory where the office folder will be created (or exists).

        Reference:
            Saenko, K., Kulis, B., Fritz, M. and Darrell, T., 2010, September. Adapting visual category models to new
            domains. In European Conference on Computer Vision (pp. 213-226). Springer, Berlin, Heidelberg.
        """
        sub_domain_set = ["amazon", "dslr", "webcam"]
        super(Office31, self).__init__(root, sub_domain_set=sub_domain_set, **kwargs)


class OfficeCaltech(OfficeAccess):
    def __init__(self, root, **kwargs):
        """Office-Caltech-10 Dataset. This dataset consists of four domains: 'amazon', 'caltech', 'dslr', and 'webcam',
            which are samples with overlapped 10 classes between Office-31 and Caltech-256.

        Args:
            root (string): path to directory where the office folder will be created (or exists).

        References:
            [1] Saenko, K., Kulis, B., Fritz, M. and Darrell, T., 2010, September. Adapting visual category models to
            new domains. In European Conference on Computer Vision (pp. 213-226). Springer, Berlin, Heidelberg.
            [2] Griffin, Gregory and Holub, Alex and Perona, Pietro, 2007. Caltech-256 Object Category Dataset.
            California Institute of Technology. (Unpublished).
            https://resolver.caltech.edu/CaltechAUTHORS:CNS-TR-2007-001.
            [3] Gong, B., Shi, Y., Sha, F. and Grauman, K., 2012, June. Geodesic flow kernel for unsupervised
            domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (pp. 2066-2073).
        """
        sub_class_set = [
            "mouse",
            "calculator",
            "back_pack",
            "keyboard",
            "monitor",
            "projector",
            "headphones",
            "bike",
            "laptop_computer",
            "mug",
        ]
        super(OfficeCaltech, self).__init__(root, sub_class_set=sub_class_set, **kwargs)


class ImageAccess:
    @staticmethod
    def get_multi_domain_images(image_set_name: str, data_path: str, sub_domain_set=None, **kwargs):
        """Get multi-domain images as a dataset from the given data path.

        Args:
            image_set_name (str): name of image dataset
            data_path (str): path to the image dataset
            sub_domain_set (list, optional): A list of domain names, which should be a subset of domains under the
                directory of data path. If None, all available domains will be used. Defaults to None.

        Returns:
            [MultiDomainImageFolder, or MultiDomainAccess]: Multi-domain image dataset
        """
        image_set_name = image_set_name.upper()
        if image_set_name == "OFFICE_CALTECH":
            return OfficeCaltech(data_path, **kwargs)
        elif image_set_name == "OFFICE31":
            return Office31(data_path, **kwargs)
        elif image_set_name == "OFFICE":
            return OfficeAccess(data_path, sub_domain_set=sub_domain_set, **kwargs)
        elif image_set_name == "DIGITS":
            data_dict = dict()
            if sub_domain_set is None:
                sub_domain_set = ["SVHN", "USPS_RGB", "MNIST_RGB", "MNISTM"]
            for domain in sub_domain_set:
                data_dict[domain] = DigitDataset.get_access(DigitDataset(domain), data_path)[0]
            return MultiDomainAccess(data_dict, 10, **kwargs)
        else:
            # default image transform
            transform = transforms.Compose(
                [transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
            )
            return MultiDomainImageFolder(data_path, transform=transform, sub_domain_set=sub_domain_set, **kwargs)


def get_cifar(cfg):
    """Gets training and validation data loaders for the CIFAR datasets

    Args:
        cfg (CfgNode): hyperparameters from configure file

    Examples:
        >>> train_loader, valid_loader = get_cifar(cfg)
    """
    logging.info("==> Preparing to load data " + cfg.DATASET.NAME + " at " + cfg.DATASET.ROOT)
    cifar_train_transform = get_transform("cifar", augment=True)
    cifar_test_transform = get_transform("cifar", augment=False)

    if cfg.DATASET.NAME == "CIFAR10":
        train_set = datasets.CIFAR10(
            cfg.DATASET.ROOT, train=True, download=cfg.DATASET.DOWNLOAD, transform=cifar_train_transform
        )
        valid_set = datasets.CIFAR10(
            cfg.DATASET.ROOT, train=False, download=cfg.DATASET.DOWNLOAD, transform=cifar_test_transform
        )
    elif cfg.DATASET.NAME == "CIFAR100":
        train_set = datasets.CIFAR100(
            cfg.DATASET.ROOT, train=True, download=cfg.DATASET.DOWNLOAD, transform=cifar_train_transform
        )
        valid_set = datasets.CIFAR100(
            cfg.DATASET.ROOT, train=False, download=cfg.DATASET.DOWNLOAD, transform=cifar_test_transform
        )
    else:
        raise NotImplementedError

    train_loader = torch.utils.data.DataLoader(
        train_set,
        batch_size=cfg.SOLVER.TRAIN_BATCH_SIZE,
        shuffle=True,
        num_workers=cfg.DATASET.NUM_WORKERS,
        pin_memory=True,
        drop_last=True,
    )
    valid_loader = torch.utils.data.DataLoader(
        valid_set,
        batch_size=cfg.SOLVER.TEST_BATCH_SIZE,
        shuffle=False,
        num_workers=cfg.DATASET.NUM_WORKERS,
        pin_memory=True,
    )

    return train_loader, valid_loader


def read_dicom_phases(dicom_path, sort_instance=True):
    """Read dicom images of multiple instances/phases for one patient.

    Args:
        dicom_path (str): Path to DICOM images.
        sort_instance (bool, optional): Whether sort images by InstanceNumber (i.e. phase number). Defaults to True.

    Returns:
        [list]: List of dicom dataset objects
    """
    dcm_phases = []  # list of dicom dataset objects (phases)
    # get all dicom files under the directory
    phase_files = glob.glob(dicom_path + "/**/*.dcm", recursive=True)
    for phase_file in phase_files:
        dataset = pydicom.dcmread(phase_file)
        setattr(dataset, "FilePath", phase_file)
        dcm_phases.append(dataset)
    if sort_instance:
        dcm_phases.sort(key=lambda x: x.InstanceNumber, reverse=False)

    return dcm_phases


def check_dicom_series_uid(dcm_phases, sort_instance=True):
    """Check if all dicom images have the same series UID.

    Args:
        dcm_phases (list): List of dicom dataset objects (phases)
        sort_instance (bool, optional): Whether sort images by InstanceNumber (i.e. phase number). Defaults to True.

    Returns:
        list: List of list(s) dicom phases.
    """
    series_ids = [dcm.SeriesInstanceUID for dcm in dcm_phases]
    unique_ids = np.unique(series_ids)
    if unique_ids.shape[0] > 1:
        dcms = {uid: [] for uid in unique_ids}
        for dcm in dcm_phases:
            dcms[dcm.SeriesInstanceUID].append(dcm)
        if sort_instance:
            for uid in dcms:
                dcms[uid].sort(key=lambda x: x.InstanceNumber, reverse=False)

        dcms_out = []
        for uid in dcms:
            dcms_out.append(dcms[uid])
    else:
        dcms_out = [dcm_phases]

    return dcms_out


def read_dicom_dir(dicom_path, sort_instance=True, sort_patient=False, check_series_uid=False):
    """Read dicom files for multiple patients and multiple instances / phases from a given directory arranged in the
        following structure:

            root/patient_a/.../phase_1.dcm
            root/patient_a/.../phase_2.dcm
            root/patient_a/.../phase_3.dcm

            root/patient_b/.../phase_1.dcm
            root/patient_b/.../phase_2.dcm
            root/patient_b/.../phase_3.dcm

            root/patient_m/.../phase_1.dcm
            root/patient_m/.../phase_2.dcm
            root/patient_m/.../phase_3.dcm

    Args:
        dicom_path (str): Directory of DICOM files.
        sort_instance (bool, optional): Whether sort images by InstanceNumber (i.e. phase number) for each subject.
            Defaults to True.
        sort_patient (bool, optional): Whether sort subjects' images by PatientID. Defaults to False.
        check_series_uid (bool, optional): Whether check if all series UIDs are the same. Defaults to False.

    Returns:
        [list[list]]: [a list of dicom dataset lists]
    """
    patient_dirs = [os.path.join(dicom_path, patient_dir) for patient_dir in os.listdir(dicom_path)]
    patient_dirs = filter(os.path.isdir, patient_dirs)
    dcm_patients = []  # list of dicom dataset patient lists

    for patient_dir in patient_dirs:
        patient_dcm_list = read_dicom_phases(patient_dir, sort_instance)
        if check_series_uid:
            patient_dcm_list = check_dicom_series_uid(patient_dcm_list, sort_instance)
            for dcm_series_instance in patient_dcm_list:
                dcm_patients.append(dcm_series_instance)
        else:
            dcm_patients.append(patient_dcm_list)

    if sort_patient:
        dcm_patients.sort(key=lambda x: x[0].PatientID, reverse=False)

    return dcm_patients


def dicom2arraylist(dicom_patient_list, return_patient_id=False):
    """Convert dicom datasets to arrays

    Args:
        dicom_patient_list (list): List of dicom patient lists.
        return_patient_id (bool, optional): Whether return PatientID. Defaults to False.

    Returns:
        list: list of array-like tensors.
        list (optional): list of PatientIDs.
    """
    n_samples = len(dicom_patient_list)
    image_list = []  # number of phases can be different across patients, using list to avoid the phase dimension issue
    patient_ids = []
    for i in range(n_samples):
        patient_ids.append(dicom_patient_list[i][0].PatientID)
        n_phases = len(dicom_patient_list[i])
        img_size = dicom_patient_list[i][0].pixel_array.shape
        img = np.zeros((n_phases,) + img_size)
        for j in range(n_phases):
            img[j, ...] = dicom_patient_list[i][j].pixel_array
        image_list.append(img)
    if return_patient_id:
        return image_list, patient_ids
    else:
        return image_list
</file>

<file path="kale/loaddata/mnistm.py">
"""
Dataset setting and data loader for MNIST-M, from
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/dataset_mnistm.py
(based on https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py)
CREDIT: https://github.com/corenel
"""

from __future__ import print_function

import errno
import logging
import os

import torch
import torch.utils.data as data
from PIL import Image


class MNISTM(data.Dataset):
    """
    MNIST-M Dataset.
    Auto-downloads the dataset and provide the torch Dataset API.

    Args:
        root (str): path to directory where the MNISTM folder will be created (or exists.)
        train (bool, optional): defaults to True.
            If True, loads the training data. Otherwise, loads the test data.
        transform (callable, optional): defaults to None.
            A function/transform that takes in
            an PIL image and returns a transformed version.
            E.g., ``transforms.RandomCrop``
            This preprocessing function applied to all images (whether source or target)

        target_transform (callable, optional): default to None, similar to transform.
            This preprocessing function applied to all target images, after `transform`

        download (bool optional): defaults to False.
            Whether to allow downloading the data if not found on disk.
    """

    url = "https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz"

    raw_folder = "raw"
    processed_folder = "processed"
    training_file = "mnist_m_train.pt"
    test_file = "mnist_m_test.pt"

    def __init__(
        self,
        root,
        train=True,
        transform=None,
        target_transform=None,
        download=False,
    ):
        """Init MNIST-M dataset."""
        super(MNISTM, self).__init__()
        self.root = os.path.join(root, "MNISTM")
        self.mnist_root = root
        self.transform = transform
        self.target_transform = target_transform
        self.train = train  # training set or test set

        if download:
            self.download()

        if not self._check_exists():
            raise RuntimeError("Dataset not found." + " You can use download=True to download it")

        if self.train:
            self.data, self.targets = torch.load(os.path.join(self.root, self.processed_folder, self.training_file))
        else:
            self.data, self.targets = torch.load(os.path.join(self.root, self.processed_folder, self.test_file))

    def __getitem__(self, index):
        """Get images and target for data loader.
        Args:
            index (int): Index
        Returns:
            tuple: (image, target) where target is index of the target class.
        """
        img, target = self.data[index], self.targets[index]

        # doing this so that it is consistent with all other datasets
        # to return a PIL Image
        img = Image.fromarray(img.squeeze().numpy(), mode="RGB")

        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target

    def __len__(self):
        """Return size of dataset."""
        return len(self.data)

    def _check_exists(self):
        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and os.path.exists(
            os.path.join(self.root, self.processed_folder, self.test_file)
        )

    def download(self):
        """Download the MNISTM data."""
        # import essential packages
        import gzip
        import pickle

        # Ignore comment is used here because type stubs for 'six.moves' are not available. This suppresses mypy errors.
        from six.moves import urllib  # type: ignore
        from torchvision import datasets

        # check if dataset already exists
        if self._check_exists():
            return

        # make data dirs
        try:
            os.makedirs(os.path.join(self.root, self.raw_folder))
            os.makedirs(os.path.join(self.root, self.processed_folder))
        except OSError as e:
            if e.errno == errno.EEXIST:
                pass
            else:
                raise

        # download pkl files
        logging.info("Downloading " + self.url)
        filename = self.url.rpartition("/")[2]
        file_path = os.path.join(self.root, self.raw_folder, filename)
        if not os.path.exists(file_path.replace(".gz", "")):
            data = urllib.request.urlopen(self.url)
            with open(file_path, "wb") as f:
                f.write(data.read())
            with open(file_path.replace(".gz", ""), "wb") as out_f, gzip.GzipFile(file_path) as zip_f:
                out_f.write(zip_f.read())
            os.unlink(file_path)

        # process and save as torch files
        logging.info("Processing...")

        # load MNIST-M images from pkl file
        with open(file_path.replace(".gz", ""), "rb") as f:
            mnist_m_data = pickle.load(f, encoding="bytes")
        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b"train"])
        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b"test"])

        # get MNIST labels
        mnist_train_labels = datasets.MNIST(root=self.mnist_root, train=True, download=True).targets
        mnist_test_labels = datasets.MNIST(root=self.mnist_root, train=False, download=True).targets

        # save MNIST-M dataset
        training_set = (mnist_m_train_data, mnist_train_labels)
        test_set = (mnist_m_test_data, mnist_test_labels)
        with open(os.path.join(self.root, self.processed_folder, self.training_file), "wb") as f:
            torch.save(training_set, f)
        with open(os.path.join(self.root, self.processed_folder, self.test_file), "wb") as f:
            torch.save(test_set, f)

        logging.info("[DONE]")
</file>

<file path="kale/loaddata/multi_domain.py">
"""
Construct a dataset with (multiple) source and target domains, adapted from
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/multisource.py
"""

import logging
import os
from enum import Enum
from typing import Any, Callable, cast, Dict, List, Optional, Tuple

import numpy as np
import torch.utils.data
from sklearn.utils import check_random_state
from torchvision.datasets import VisionDataset
from torchvision.datasets.folder import default_loader, has_file_allowed_extension, IMG_EXTENSIONS

from kale.loaddata.dataset_access import DatasetAccess, get_class_subset, split_by_ratios
from kale.loaddata.sampler import FixedSeedSamplingConfig, get_labels, MultiDataLoader, SamplingConfig


class WeightingType(Enum):
    NATURAL = "natural"
    BALANCED = "balanced"
    PRESET0 = "preset0"


class DatasetSizeType(Enum):
    Max = "max"  # size of the biggest dataset
    Source = "source"  # size of the source dataset

    @staticmethod
    def get_size(size_type, source_dataset, *other_datasets):
        if size_type is DatasetSizeType.Max:
            return max(list(map(len, other_datasets)) + [len(source_dataset)])
        elif size_type is DatasetSizeType.Source:
            return len(source_dataset)
        else:
            raise ValueError(f"Size type size must be 'max' or 'source', had '{size_type}'")


class DomainsDatasetBase:
    def prepare_data_loaders(self):
        """
        handles train/validation/test split to have 3 datasets each with data from all domains
        """
        raise NotImplementedError()

    def get_domain_loaders(self, split="train", batch_size=32):
        """
        handles the sampling of a dataset containing multiple domains

        Args:
            split (string, optional): ["train"|"valid"|"test"]. Which dataset to iterate on. Defaults to "train".
            batch_size (int, optional): Defaults to 32.

        Returns:
            MultiDataLoader: A dataloader with API similar to the torch.dataloader, but returning
            batches from several domains at each iteration.
        """
        raise NotImplementedError()


class MultiDomainDatasets(DomainsDatasetBase):
    def __init__(
        self,
        source_access: DatasetAccess,
        target_access: DatasetAccess,
        config_weight_type="natural",
        config_size_type=DatasetSizeType.Max,
        valid_split_ratio=0.1,
        source_sampling_config=None,
        target_sampling_config=None,
        n_fewshot=None,
        random_state=None,
        class_ids=None,
    ):
        """The class controlling how the source and target domains are
            iterated over.

        Args:
            source_access (DatasetAccess): accessor for the source dataset
            target_access (DatasetAccess): accessor for the target dataset
            config_weight_type (WeightingType, optional): The weight type for sampling. Defaults to 'natural'.
            config_size_type (DatasetSizeType, optional): Which dataset size to use to define the number of epochs vs
                batch_size. Defaults to DatasetSizeType.Max.
            valid_split_ratio (float, optional): ratio for the validation part of the train dataset. Defaults to 0.1.
            source_sampling_config (SamplingConfig, optional): How to sample from the source. Defaults to None
                (=> RandomSampler).
            target_sampling_config (SamplingConfig, optional): How to sample from the target. Defaults to None
                (=> RandomSampler).
            n_fewshot (int, optional): Number of target samples for which the label may be used,
                to define the few-shot, semi-supervised setting. Defaults to None.
            random_state ([int|np.random.RandomState], optional): Used for deterministic sampling/few-shot label
                selection. Defaults to None.
            class_ids (list, optional): List of chosen subset of class ids. Defaults to None (=> All Classes).
        Examples::
            >>> dataset = MultiDomainDatasets(source_access, target_access)
        """
        weight_type = WeightingType(config_weight_type)
        size_type = DatasetSizeType(config_size_type)

        if weight_type is WeightingType.PRESET0:
            self._source_sampling_config = SamplingConfig(class_weights=np.arange(source_access.n_classes(), 0, -1))
            self._target_sampling_config = SamplingConfig(
                class_weights=np.random.randint(1, 4, size=target_access.n_classes())
            )
        elif weight_type is WeightingType.BALANCED:
            self._source_sampling_config = SamplingConfig(balance=True)
            self._target_sampling_config = SamplingConfig(balance=True)
        elif weight_type not in WeightingType:
            raise ValueError(f"Unknown weighting method {weight_type}.")
        else:
            self._source_sampling_config = SamplingConfig()
            self._target_sampling_config = SamplingConfig()

        self._source_access = source_access
        self._target_access = target_access
        self._valid_split_ratio = valid_split_ratio
        # self._source_sampling_config = (
        #     source_sampling_config
        #     if source_sampling_config is not None
        #     else SamplingConfig()
        # )
        # self._target_sampling_config = (
        #     target_sampling_config
        #     if target_sampling_config is not None
        #     else SamplingConfig()
        # )
        self._size_type = size_type
        self._n_fewshot = n_fewshot
        self._random_state = check_random_state(random_state)
        self._source_by_split: Dict[str, torch.utils.data.Subset] = {}
        self._labeled_target_by_split = None
        self._target_by_split: Dict[str, torch.utils.data.Subset] = {}
        self.class_ids = class_ids

    def is_semi_supervised(self):
        return self._n_fewshot is not None and self._n_fewshot > 0

    def prepare_data_loaders(self):
        logging.debug("Load source")
        (
            self._source_by_split["train"],
            self._source_by_split["valid"],
        ) = self._source_access.get_train_valid(self._valid_split_ratio)
        if self.class_ids is not None:
            self._source_by_split["train"] = get_class_subset(self._source_by_split["train"], self.class_ids)
            self._source_by_split["valid"] = get_class_subset(self._source_by_split["valid"], self.class_ids)

        logging.debug("Load target")
        (
            self._target_by_split["train"],
            self._target_by_split["valid"],
        ) = self._target_access.get_train_valid(self._valid_split_ratio)
        if self.class_ids is not None:
            self._target_by_split["train"] = get_class_subset(self._target_by_split["train"], self.class_ids)
            self._target_by_split["valid"] = get_class_subset(self._target_by_split["valid"], self.class_ids)

        logging.debug("Load source Test")
        self._source_by_split["test"] = self._source_access.get_test()
        if self.class_ids is not None:
            self._source_by_split["test"] = get_class_subset(self._source_by_split["test"], self.class_ids)
        logging.debug("Load target Test")
        self._target_by_split["test"] = self._target_access.get_test()
        if self.class_ids is not None:
            self._target_by_split["test"] = get_class_subset(self._target_by_split["test"], self.class_ids)

        if self._n_fewshot is not None and self._n_fewshot > 0:
            # semi-supervised target domain
            self._labeled_target_by_split = {}
            for part in ["train", "valid", "test"]:
                (
                    self._labeled_target_by_split[part],
                    self._target_by_split[part],
                ) = _split_dataset_few_shot(self._target_by_split[part], self._n_fewshot)

    def get_domain_loaders(self, split="train", batch_size=32):
        source_ds = self._source_by_split[split]
        source_loader = self._source_sampling_config.create_loader(source_ds, batch_size)
        target_ds = self._target_by_split[split]

        if self._labeled_target_by_split is None:
            # unsupervised target domain
            target_loader = self._target_sampling_config.create_loader(target_ds, batch_size)
            n_dataset = DatasetSizeType.get_size(self._size_type, source_ds, target_ds)
            return MultiDataLoader(
                dataloaders=[source_loader, target_loader],
                n_batches=max(n_dataset // batch_size, 1),
            )
        else:
            # semi-supervised target domain
            target_labeled_ds = self._labeled_target_by_split[split]
            target_unlabeled_ds = target_ds
            # label domain: always balanced
            target_labeled_loader = SamplingConfig(balance=True, class_weights=None).create_loader(
                target_labeled_ds, batch_size=min(len(target_labeled_ds), batch_size)
            )
            target_unlabeled_loader = self._target_sampling_config.create_loader(target_unlabeled_ds, batch_size)
            n_dataset = DatasetSizeType.get_size(self._size_type, source_ds, target_labeled_ds, target_unlabeled_ds)
            return MultiDataLoader(
                dataloaders=[source_loader, target_labeled_loader, target_unlabeled_loader],
                n_batches=max(n_dataset // batch_size, 1),
            )

    def __len__(self):
        source_ds = self._source_by_split["train"]
        target_ds = self._target_by_split["train"]
        if self._labeled_target_by_split is None:
            return DatasetSizeType.get_size(self._size_type, source_ds, target_ds)
        else:
            labeled_target_ds = self._labeled_target_by_split["train"]
            return DatasetSizeType.get_size(self._size_type, source_ds, labeled_target_ds, target_ds)


def _split_dataset_few_shot(dataset, n_fewshot, random_state=None):
    if n_fewshot <= 0:
        raise ValueError(f"n_fewshot should be > 0, not '{n_fewshot}'")
    assert n_fewshot > 0
    labels = get_labels(dataset)
    classes = sorted(set(labels))
    if n_fewshot < 1:
        max_few = len(dataset) // len(classes)
        n_fewshot = round(max_few * n_fewshot)
    n_fewshot = int(round(n_fewshot))

    random_state = check_random_state(random_state)
    # sample n_fewshot items per class from last dataset
    tindices = []
    uindices = []
    for class_ in classes:
        indices = np.where(labels == class_)[0]
        random_state.shuffle(indices)
        head, tail = np.split(indices, [n_fewshot])
        assert len(head) == n_fewshot
        tindices.append(head)
        uindices.append(tail)
    tindices = np.concatenate(tindices)
    uindices = np.concatenate(uindices)
    assert len(tindices) == len(classes) * n_fewshot
    labeled_dataset = torch.utils.data.Subset(dataset, tindices)
    unlabeled_dataset = torch.utils.data.Subset(dataset, uindices)
    return labeled_dataset, unlabeled_dataset


def _domain_stratified_split(domain_labels, n_partitions, split_ratios):
    """Get domain stratified indices of random split. Samples with the same domain label will be split based on the
        given ratios. Then the indices of different domains within the same split will be concatenated.

    Args:
        domain_labels (array-like): Labels to indicate which domains the samples are from.
        n_partitions (int): Number of partitions to split, 2 <= n_partitions <= len(split_ratios) + 1.
        split_ratios (list): Ratios of splits to be produced, where 0 < sum(split_ratios) <= 1.

    Returns:
        [list]: Indices for different splits.
    """
    domains = np.unique(domain_labels)
    subset_idx = [[] for i in range(n_partitions)]
    for domain_label_ in domains:
        domain_idx = np.where(domain_labels == domain_label_)[0]
        subsets = split_by_ratios(torch.from_numpy(domain_idx), split_ratios)
        for i in range(n_partitions):
            subset_idx[i].append(domain_idx[subsets[i].indices])

    stratified_idx = []
    for i in range(n_partitions):
        stratified_idx.append(np.concatenate(subset_idx[i]))

    return stratified_idx


class MultiDomainImageFolder(VisionDataset):
    """A generic data loader where the samples are arranged in this way: ::

        root/domain_a/class_1/xxx.ext
        root/domain_a/class_1/xxy.ext
        root/domain_a/class_2/xxz.ext

        root/domain_b/class_1/efg.ext
        root/domain_b/class_2/pqr.ext
        root/domain_b/class_2/lmn.ext

        root/domain_k/class_2/123.ext
        root/domain_k/class_1/abc3.ext
        root/domain_k/class_1/asd932_.ext

    Args:
        root (string): Root directory path.
        loader (callable): A function to load a sample given its path.
        extensions (tuple[string]): A list of allowed extensions. Either extensions or is_valid_file should be
            passed.
        transform (callable, optional): A function/transform that takes in a sample and returns a transformed
            version.  E.g, ``transforms.RandomCrop`` for images.
        target_transform (callable, optional): A function/transform that takes in the target and transforms it.
        sub_domain_set (list): A list of domain names, which should be a subset of domains (folders) under the root
            directory. If None, all available domains will be used. Defaults to None.
        sub_class_set (list): A list of class names, which should be a subset of classes (folders) under each
            domain's directory. If None, all available classes will be used. Defaults to None.
        is_valid_file (callable, optional): A function that takes path of a file and check if the file is a valid
            file (to check corrupt files). Either extensions or is_valid_file should be passed.
     Attributes:
        classes (list): List of the class names sorted alphabetically.
        class_to_idx (dict): Dict with items (class_name, class_index).
        samples (list): List of (sample path, class_index) tuples
        targets (list): The class_index value for each image in the dataset
        domains (list): List of the domain names sorted alphabetically.
        domain_to_idx (dict): Dict with items (domain_name, domain_index).
        domain_labels (list): The domain_index value for each image in the dataset
    """

    def __init__(
        self,
        root: str,
        loader: Callable[[str], Any] = default_loader,
        extensions: Optional[Tuple[str, ...]] = IMG_EXTENSIONS,
        transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        sub_domain_set=None,
        sub_class_set=None,
        is_valid_file: Optional[Callable[[str], bool]] = None,
        return_domain_label: Optional[bool] = False,
        split_train_test: Optional[bool] = False,
        split_ratio: float = 0.8,
    ) -> None:
        super(MultiDomainImageFolder, self).__init__(root, transform=transform, target_transform=target_transform)
        domains, domain_to_idx = self._find_classes(self.root)
        if isinstance(sub_domain_set, list):
            for domain_name in sub_domain_set:
                if domain_name not in domains:
                    raise ValueError("Domain %s not in the image directory" % domain_name)
            domains = sub_domain_set
            domain_to_idx = {domain_name: i for i, domain_name in enumerate(sub_domain_set)}

        classes, class_to_idx = self._find_classes(os.path.join(self.root, domains[0]))
        if isinstance(sub_class_set, list):
            for class_name in sub_class_set:
                if class_name not in classes:
                    raise ValueError("Class %s not in the image directory" % class_name)
            classes = sub_class_set
            class_to_idx = {class_name: i for i, class_name in enumerate(sub_class_set)}
        samples = make_multi_domain_set(self.root, class_to_idx, domain_to_idx, extensions, is_valid_file)
        if len(samples) == 0:
            msg = "Found 0 files in sub-folders of: {}\n".format(self.root)
            if extensions is not None:
                msg += "Supported extensions are: {}".format(",".join(extensions))
            raise RuntimeError(msg)

        self.loader = loader
        self.extensions = extensions

        self.classes = classes
        self.class_to_idx = class_to_idx
        self.samples = samples
        self.targets = [s[1] for s in samples]
        self.domains = domains
        self.domain_to_idx = domain_to_idx
        self.domain_labels = [s[2] for s in samples]
        self.return_domain_label = return_domain_label
        self.split_train_test = split_train_test
        self.split_ratio = split_ratio
        if split_train_test and 0 < split_ratio < 1:
            self.train_idx, self.test_idx = _domain_stratified_split(self.domain_labels, 2, [split_ratio])
        else:
            self.train_idx = None
            self.test_idx = None

    @staticmethod
    def _find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:
        """
        Finds the class folders in a dataset.
        Args:
            directory (string): Directory path.
        Returns:
            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.
        Ensures:
            No class is a subdirectory of another.
        """
        classes = [d.name for d in os.scandir(directory) if d.is_dir()]
        classes.sort()
        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        return classes, class_to_idx

    def __getitem__(self, index: int) -> Tuple:
        """
        Args:
            index (int): Index
        Returns:
            tuple: (sample, target, domain) where target is class_index of the target class.
        """
        path, target, domain = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)
        if self.return_domain_label:
            return sample, target, domain
        else:
            return sample, target

    def __len__(self) -> int:
        return len(self.samples)

    def get_train(self):
        if self.split_train_test:
            if 0 < self.split_ratio < 1:
                return torch.utils.data.Subset(self, self.train_idx)
            else:
                return self
        else:
            return None

    def get_test(self):
        if self.split_train_test:
            if 0 < self.split_ratio < 1:
                return torch.utils.data.Subset(self, self.test_idx)
            else:
                return self
        else:
            return None


def make_multi_domain_set(
    directory: str,
    class_to_idx: Dict[str, int],
    domain_to_idx: Dict[str, int],
    extensions: Optional[Tuple[str, ...]] = None,
    is_valid_file: Optional[Callable[[str], bool]] = None,
) -> List[Tuple[str, int, int]]:
    """Generates a list of samples of a form (path_to_sample, class, domain).
    Args:
        directory (str): root dataset directory
        class_to_idx (Dict[str, int]): dictionary mapping class name to class index
        domain_to_idx (Dict[str, int]): dictionary mapping d name to class index
        extensions (optional): A list of allowed extensions. Either extensions or is_valid_file should be passed.
            Defaults to None.
        is_valid_file (optional): A function that takes path of a file and checks if the file is a valid file
            (to check corrupt files) both extensions and is_valid_file should not be passed. Defaults to None.
    Raises:
        ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.
    Returns:
        List[Tuple[str, int, int]]: samples of a form (path_to_sample, class, domain)
    """
    instances = []
    directory = os.path.expanduser(directory)
    both_none = extensions is None and is_valid_file is None
    both_something = extensions is not None and is_valid_file is not None
    if both_none or both_something:
        raise ValueError("Both extensions and is_valid_file cannot be None or not None at the same time")
    if extensions is not None:

        def is_valid_file(x: str) -> bool:
            return has_file_allowed_extension(x, cast(Tuple[str, ...], extensions))

    is_valid_file = cast(Callable[[str], bool], is_valid_file)
    for target_domain in sorted(domain_to_idx.keys()):
        domain_index = domain_to_idx[target_domain]
        domain_dir = os.path.join(directory, target_domain)
        for target_class in sorted(class_to_idx.keys()):
            class_index = class_to_idx[target_class]
            target_dir = os.path.join(domain_dir, target_class)
            if not os.path.isdir(target_dir):
                continue
            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):
                for fname in sorted(fnames):
                    path = os.path.join(root, fname)
                    if is_valid_file(path):
                        item = path, class_index, domain_index
                        instances.append(item)
    return instances


class ConcatMultiDomainAccess(torch.utils.data.Dataset):
    """Concatenate multiple datasets as a single dataset with domain labels

    Args:
        data_access (dict): Dictionary of domain datasets, e.g. {"Domain1_name": domain1_set,
            "Domain2_name": domain2_set}
        domain_to_idx (dict): Dictionary of domain name to domain labels, e.g. {"Domain1_name": 0, "Domain2_name": 1}
        return_domain_label (Optional[bool], optional): Whether return domain labels in each batch. Defaults to False.
    """

    def __init__(
        self,
        data_access: dict,
        domain_to_idx: dict,
        return_domain_label: Optional[bool] = False,
    ):
        self.domain_to_idx = domain_to_idx
        self.data = []
        self.labels = []
        self.domain_labels = []
        for domain_ in domain_to_idx:
            n_samples = data_access[domain_].data.shape[0]
            for idx in range(n_samples):
                x, y = data_access[domain_][idx]
                self.data.append(x)
                self.labels.append(y)
                self.domain_labels.append(domain_to_idx[domain_])

        self.data = torch.stack(self.data)
        self.labels = torch.tensor(self.labels)
        self.domain_labels = torch.tensor(self.domain_labels)
        self.return_domain_label = return_domain_label

    def __getitem__(self, index: int) -> Tuple:
        if self.return_domain_label:
            return self.data[index], self.labels[index], self.domain_labels[index]
        else:
            return self.data[index], self.labels[index]

    def __len__(self):
        return len(self.labels)


class MultiDomainAccess(DatasetAccess):
    """Convert multiple digits-like data accesses to a single data access.
    Args:
        data_access (dict): Dictionary of data accesses, e.g. {"Domain1_name": domain1_access,
            "Domain2_name": domain2_access}
        n_classes (int): number of classes.
        return_domain_label (Optional[bool], optional): Whether return domain labels in each batch.
            Defaults to False.
    """

    def __init__(self, data_access: dict, n_classes: int, return_domain_label: Optional[bool] = False):
        super().__init__(n_classes)
        self.data_access = data_access
        self.domain_to_idx = {list(data_access.keys())[i]: i for i in range(len(data_access))}
        self.return_domain_label = return_domain_label

    def get_train(self):
        train_access = {domain_: self.data_access[domain_].get_train() for domain_ in self.domain_to_idx}
        return ConcatMultiDomainAccess(train_access, self.domain_to_idx, self.return_domain_label)

    def get_test(self):
        test_access = {domain_: self.data_access[domain_].get_test() for domain_ in self.domain_to_idx}
        return ConcatMultiDomainAccess(test_access, self.domain_to_idx, self.return_domain_label)

    def __len__(self):
        return len(self.get_train()) + len(self.get_test())


class MultiDomainAdapDataset(DomainsDatasetBase):
    """The class controlling how the multiple domains are iterated over.

    Args:
        data_access (MultiDomainImageFolder, or MultiDomainAccess): Multi-domain data access.
        valid_split_ratio (float, optional): Split ratio for validation set. Defaults to 0.1.
        test_split_ratio (float, optional): Split ratio for test set. Defaults to 0.2.
        random_state (int, optional): Random state for generator. Defaults to 1.
        test_on_all (bool, optional): Whether test model on all target. Defaults to False.
    """

    def __init__(
        self, data_access, valid_split_ratio=0.1, test_split_ratio=0.2, random_state: int = 1, test_on_all=False
    ):
        self.domain_to_idx = data_access.domain_to_idx
        self.n_domains = len(data_access.domain_to_idx)
        self.data_access = data_access
        self._valid_split_ratio = valid_split_ratio
        self._test_split_ratio = test_split_ratio
        self._sample_by_split: Dict[str, torch.utils.data.Subset] = {}
        self._sampling_config = FixedSeedSamplingConfig(seed=random_state, balance_domain=True)
        self._loader = MultiDataLoader
        self._random_state = random_state
        self.test_on_all = test_on_all

    def prepare_data_loaders(self):
        splits = ["test", "valid", "train"]
        self._sample_by_split["test"] = self.data_access.get_test()
        if self._sample_by_split["test"] is None:
            # split test, valid, and train set if the data access no train test splits
            if self.test_on_all:
                subset_idx = _domain_stratified_split(self.data_access.domain_labels, 2, [self._valid_split_ratio])
                self._sample_by_split["valid"] = torch.utils.data.Subset(self.data_access, subset_idx[0])
                self._sample_by_split["train"] = torch.utils.data.Subset(self.data_access, subset_idx[1])
                self._sample_by_split["test"] = torch.utils.data.Subset(self.data_access, np.concatenate(subset_idx))
            else:
                subset_idx = _domain_stratified_split(
                    self.data_access.domain_labels, 3, [self._test_split_ratio, self._valid_split_ratio]
                )
                for i in range(len(splits)):
                    self._sample_by_split[splits[i]] = torch.utils.data.Subset(self.data_access, subset_idx[i])
        else:
            # use original data split if get_test() is not none
            self._sample_by_split["valid"], self._sample_by_split["train"] = self.data_access.get_train_valid(
                self._valid_split_ratio
            )

    def get_domain_loaders(self, split="train", batch_size=32):
        return self._sampling_config.create_loader(self._sample_by_split[split], batch_size)

    def __len__(self):
        return len(self.data_access)
</file>

<file path="kale/loaddata/multiomics_datasets.py">
# =============================================================================
# Author: Sina Tabakhi, sina.tabakhi@gmail.com
# =============================================================================

"""
Construct a dataset with multiple omics modalities based on PyTorch Geometric.

This code is written by refactoring the MOGONET dataset code (https://github.com/txWang/MOGONET/blob/main/train_test.py)
within the 'Dataset' class provided in the PyTorch Geometric.

Reference:
Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021). MOGONET integrates multi-omics data
using graph convolutional networks allowing patient classification and biomarker identification. Nature communications.
https://www.nature.com/articles/s41467-021-23774-w
"""

import os.path as osp
from typing import Callable, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn.functional as F
from torch_geometric.data import Data, Dataset, download_url, extract_zip
from torch_sparse import SparseTensor

from kale.evaluate.metrics import calculate_distance, DistanceMetric


class MultiomicsDataset(Dataset):
    r"""The multiomics data for creating graph dataset.
    See `here <https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html>`__ in PyTorch Geometric
    for the accompanying tutorial.

    Args:
        root (string): Root directory where the dataset should be saved.
        num_modalities (int): The total number of modalities in the dataset.
        num_classes (int): The total number of classes in the dataset.
        url (string, optional): The url to download the dataset from.
        raw_file_names (list[callable], optional): The name of the files in the ``self.raw_dir`` folder that must be
            present in order to skip downloading.
        random_split (bool, optional): Whether to split the dataset into random train and test subsets. (default:
            ``False``)
        train_size (float, optional): The proportion of the dataset to include in the train split that should be between
            0.0 and 1.0. This parameter is used when ``random_split`` is ``True``.
        transform (callable, optional): A function/transform that takes in an array_like data and returns a transformed
            version. The data object will be transformed before every access. (default: ``None``)
        pre_transform (callable, optional): A function/transform that takes in an array_like data and returns a
            transformed version. The data object will be transformed before being saved to disk. (default: ``None``)
        target_pre_transform (callable, optional): A function/transform that takes in an array_like of labels and
            returns a transformed version. The label object will be transformed before being saved to disk. (default:
            ``None``)
    """

    def __init__(
        self,
        root: str,
        num_modalities: int,
        num_classes: int,
        url: Optional[str] = None,
        raw_file_names: Optional[List[str]] = None,
        random_split: bool = False,
        train_size: float = 0.7,
        transform: Optional[Callable] = None,
        pre_transform: Optional[Callable] = None,
        target_pre_transform: Optional[Callable] = None,
    ) -> None:
        self._url = url
        self._raw_file_names = raw_file_names
        self._num_modalities = num_modalities
        self._num_classes = num_classes
        self._random_split = random_split
        self._train_size = train_size
        self._target_pre_transform = target_pre_transform
        self._processed_file_names = "data.pt"
        super().__init__(root, transform, pre_transform)

    @property
    def raw_file_names(self) -> Optional[List[str]]:
        r"""The name of the files in the ``self.raw_dir`` folder that must be present in order to skip
        downloading."""
        return self._raw_file_names

    @property
    def processed_file_names(self) -> Union[str, List[str], Tuple]:
        r"""The name of the files in the ``self.processed_dir`` folder that must be present in order to skip
        processing."""
        return self._processed_file_names

    def download(self) -> None:
        r"""Downloads the dataset to the ``self.raw_dir`` folder."""
        path = download_url(self._url, self.raw_dir)
        extract_zip(path, self.raw_dir)

    def process(self) -> None:
        r"""Processes the dataset to the ``self.processed_dir`` folder. This function reads input files, creates a
        ''Data'' object, and saves it into the ''processed_dir''."""
        data_list = []

        for modality in range(self.num_modalities):
            if self._random_split:
                full_data = np.loadtxt(self.raw_paths[modality * 2], delimiter=",")
                full_labels = np.loadtxt(self.raw_paths[(modality * 2) + 1], delimiter=",")
                full_labels = full_labels.astype(int)

                train_idx, test_idx = self.get_random_split(full_labels, self._num_classes, self._train_size)
                num_train = len(train_idx)
                num_tests = len(test_idx)
            else:
                # The datasets provided here have already been pre-split into training and test sets.
                train_data = np.loadtxt(self.raw_paths[modality * 4], delimiter=",")
                train_labels = np.loadtxt(self.raw_paths[(modality * 4) + 1], delimiter=",")
                train_labels = train_labels.astype(int)
                num_train = len(train_labels)
                train_idx = torch.tensor(list(range(num_train)), dtype=torch.long)

                test_data = np.loadtxt(self.raw_paths[(modality * 4) + 2], delimiter=",")
                test_labels = np.loadtxt(self.raw_paths[(modality * 4) + 3], delimiter=",")
                test_labels = test_labels.astype(int)
                num_tests = len(test_labels)
                test_idx = torch.tensor(list(range(num_train, num_train + num_tests)), dtype=torch.long)

                full_data = np.concatenate((train_data, test_data), axis=0)
                full_labels = np.concatenate((train_labels, test_labels))
                full_labels = full_labels.astype(int)

            full_data = full_data if self.pre_transform is None else self.pre_transform(full_data)
            full_labels = full_labels if self._target_pre_transform is None else self._target_pre_transform(full_labels)

            edge_index, edge_weight = self.get_adjacency_info(full_data)
            adj_t = SparseTensor(row=edge_index[0], col=edge_index[1], value=edge_weight)

            data = Data(
                x=full_data,
                edge_index=edge_index,
                edge_weight=edge_weight,
                adj_t=adj_t,
                y=full_labels,
                train_idx=train_idx,
                test_idx=test_idx,
                num_train=num_train,
                num_test=num_tests,
            )

            data = self.extend_data(data)
            data_list.append(data)

        torch.save(data_list, osp.join(self.processed_dir, "data.pt"))

    @staticmethod
    def get_random_split(labels, num_classes: int, train_size: float = 0.7) -> Tuple:
        """Split arrays into random train and test indices.

        Args:
            labels (array-like): Array-like object that represents the labels of the dataset.
            num_classes (int): The total number of classes in the dataset.
            train_size (float, optional): The proportion of the dataset to include in the train split that should be
                between 0.0 and 1.0. (default: 0.7)

        Returns:
            A tuple of two arrays containing the indices for the train and test sets.
        """
        train_idx = []
        test_idx = []
        for c in range(num_classes):
            idx = (labels == c).nonzero()[0]
            idx = idx[torch.randperm(len(idx))]
            num_train = int(len(idx) * train_size)
            train_idx.append(idx[:num_train])
            test_idx.append(idx[num_train:])

        train_idx = np.concatenate(train_idx)
        test_idx = np.concatenate(test_idx)
        train_idx = np.sort(train_idx)
        test_idx = np.sort(test_idx)

        return train_idx, test_idx

    @staticmethod
    def get_adjacency_info(data: torch.Tensor) -> Tuple:
        """Calculate a sparse adjacency matrix of the input dataset defined by edge indices and edge attributes.

        Args:
            data (torch.Tensor): The input data.

        Returns:
            A tuple of edge indices and edge attributes.
        """
        adj = torch.ones(data.shape[0], data.shape[0], dtype=torch.long)
        adj.fill_diagonal_(0)
        edge_index = (adj > 0).nonzero().t()

        return edge_index, None

    def extend_data(self, data: Data) -> Data:
        """Extend data object by adding additional attributes.

        Args:
            data (Data): An input data object.

        Returns:
            Extended data object with additional attributes.
        """
        return data

    def len(self) -> int:
        r"""Returns the number of graphs stored in the dataset."""
        return self.num_modalities

    def get(self, modality_idx) -> Data:
        r"""Gets the data object at index ``idx``."""
        data_list = torch.load(osp.join(self.processed_dir, "data.pt"))
        return data_list[modality_idx]

    def __len__(self) -> int:
        return 1

    def __getitem__(self, index) -> Union["Dataset", Data]:
        data_list = torch.load(osp.join(self.processed_dir, "data.pt"))
        return data_list

    @property
    def num_modalities(self) -> int:
        r"""Returns the number of modalities in the dataset."""
        return self._num_modalities

    @property
    def num_classes(self) -> int:
        r"""Returns the number of classes in the dataset."""
        return self._num_classes


class SparseMultiomicsDataset(MultiomicsDataset):
    r"""The multiomics data for creating sparse graph dataset based on the settings in the MOGONET paper.

    Args:
        root (string): Root directory where the dataset should be saved.
        raw_file_names (list[callable], optional): The name of the files in the ``self.raw_dir`` folder that must be
            present in order to skip downloading.
        num_modalities (int): The total number of modalities in the dataset.
        num_classes (int): The total number of classes in the dataset.
        edge_per_node (int): Predefined number of edges per nodes in computing adjacency matrix.
        url (string, optional): The url to download the dataset from.
        random_split (bool, optional): Whether to split the dataset into random train and test subsets. (default:
            ``False``)
        train_size (float, optional): The proportion of the dataset to include in the train split that should be between
            0.0 and 1.0. This parameter is used when ``random_split`` is ``True``.
        equal_weight (bool, optional): Whether to use equal weights for all samples. (default: ``False``)
        transform (callable, optional): A function/transform that takes in an array_like data and returns a transformed
            version. The data object will be transformed before every access. (default: ``None``)
        pre_transform (callable, optional): A function/transform that takes in an array_like data and returns a
            transformed version. The data object will be transformed before being saved to disk. (default: ``None``)
        target_pre_transform (callable, optional): A function/transform that takes in an array_like of labels and
            returns a transformed version. The label object will be transformed before being saved to disk. (default:
            ``None``)
    """

    def __init__(
        self,
        root: str,
        raw_file_names: List[str],
        num_modalities: int,
        num_classes: int,
        edge_per_node: int,
        url: Optional[str] = None,
        random_split: bool = False,
        train_size: float = 0.7,
        equal_weight: bool = False,
        transform: Optional[Callable] = None,
        pre_transform: Optional[Callable] = None,
        target_pre_transform: Optional[Callable] = None,
    ):
        self.edge_per_node = edge_per_node
        self.equal_weight = equal_weight
        self.sim_threshold = None  # similarity threshold to keep the desired number of edges in the adjacency matrix.
        super().__init__(
            root,
            num_modalities,
            num_classes,
            url,
            raw_file_names,
            random_split,
            train_size,
            transform,
            pre_transform,
            target_pre_transform,
        )

    def extend_data(self, data: Data) -> Data:
        """Extend data object by adding additional attributes.

        Args:
            data (Data): An input data object.

        Returns:
            Extended data object with additional attributes.
        """
        # Add train sample weights to the data object
        train_labels = torch.argmax(data.y[data.train_idx], dim=1)
        train_sample_weight = self._get_sample_weight(train_labels)
        data.train_sample_weight = train_sample_weight

        # Add adjacency matrices to the data object
        edge_index_train, edge_weight_train = self._get_adjacency_info(data.x[data.train_idx], train=True)
        adj_t_train = SparseTensor(row=edge_index_train[0], col=edge_index_train[1], value=edge_weight_train)

        edge_index, edge_weight = self._get_adjacency_info(
            data.x[data.train_idx], test_data=data.x[data.test_idx], train=False
        )
        adj_t = SparseTensor(row=edge_index[0], col=edge_index[1], value=edge_weight)

        data.edge_index = edge_index
        data.edge_weight = edge_weight
        data.adj_t = adj_t
        data.edge_index_train = edge_index_train
        data.edge_weight_train = edge_weight_train
        data.adj_t_train = adj_t_train

        return data

    def _get_adjacency_info(
        self,
        train_data: torch.Tensor,
        test_data: torch.Tensor = None,
        train: bool = True,
        eps: float = 1e-8,
        metric: DistanceMetric = DistanceMetric.COSINE,
    ) -> Tuple:
        """Calculate a sparse adjacency matrix of the input dataset defined by edge indices and edge attributes.

        Args:
            train_data (torch.Tensor): The training data.
            test_data (torch.Tensor, optional): The test data. If ''None'' then the adjacency matrix is only calculated
                on the train data. (default: ``None``)
            train (bool, optional): Whether to use only train data to calculate the adjacency matrix. If ''False'' then
                the entire dataset (training and test sets) is used. (default: ``True``)
            eps (float, optional): Small value to avoid division by zero. (default: 1e-8)
            metric (DistanceMetric, optional): The metric to compute distance between input matrices. (default:
                ``DistanceMetric.COSINE``)

        Returns:
            A tuple of edge indices and edge attributes.
        """
        num_train = train_data.shape[0]
        num_test = 0 if test_data is None else test_data.shape[0]
        if train:
            adj_matrix = calculate_distance(train_data, eps=eps, metric=metric)
            self._find_sim_threshold(adj_matrix, num_train)
            non_zero_entries = self._generate_sparse_adj(adj_matrix, self_loop=train)
            adj_matrix = torch.mul(adj_matrix, non_zero_entries)
        else:
            adj_matrix = torch.zeros((num_train + num_test, num_train + num_test))
            dist = calculate_distance(train_data, test_data, eps=eps, metric=metric)
            non_zero_entries = self._generate_sparse_adj(dist, self_loop=train)
            adj_matrix[:num_train, num_train:] = torch.mul(dist, non_zero_entries)

            dist = calculate_distance(test_data, train_data, eps=eps, metric=metric)
            non_zero_entries = self._generate_sparse_adj(dist, self_loop=train)
            adj_matrix[num_train:, :num_train] = torch.mul(dist, non_zero_entries)

        adj_matrix_trans = adj_matrix.transpose(0, 1)
        adj_matrix = (
            adj_matrix
            + adj_matrix_trans * (adj_matrix_trans > adj_matrix).float()
            - adj_matrix * (adj_matrix_trans > adj_matrix).float()
        )
        identity_mat = torch.eye(adj_matrix.shape[0], device=adj_matrix.device)
        adj_matrix = F.normalize(adj_matrix + identity_mat, p=1)
        adj_matrix = adj_matrix.to_sparse()

        return adj_matrix.indices(), adj_matrix.values()

    def _find_sim_threshold(self, adj_mat: torch.Tensor, num_train: int) -> None:
        r"""Finds a similarity threshold for the adjacency matrix in order to keep the predefined number of edges per
        nodes.

        Args:
            adj_mat (torch.Tensor): The dense adjacency matrix.
            num_train (int): The number of samples in training data.
        """
        sorted_adj_mat = torch.sort(
            adj_mat.reshape(
                -1,
            ),
            descending=True,
        ).values[self.edge_per_node * num_train]
        self.sim_threshold = sorted_adj_mat.item()

    def _generate_sparse_adj(self, adj_mat: torch.Tensor, self_loop: bool = True) -> torch.Tensor:
        r"""Returns a sparse adjacency matrix by setting entries below the ``sim_threshold`` to 0.

        Args:
            adj_mat (torch.Tensor): The dense adjacency matrix.
            self_loop (bool, optional): Whether to fill the main diagonal with zero. (default: ``True``)

        Returns:
            torch.Tensor: Computed sparse adjacency matrix.
        """
        non_zero_entries = (adj_mat >= self.sim_threshold).float()
        if self_loop:
            non_zero_entries.fill_diagonal_(0)

        return non_zero_entries

    def _get_sample_weight(self, labels: np.ndarray) -> torch.Tensor:
        r"""Get sample weights based on the class distribution.

        Args:
            labels (np.ndarray): A list of ground truth labels of samples.

        Returns:
            torch.Tensor: A list of label weights calculated for each sample.
        """
        if self.equal_weight:
            sample_weight = np.ones(len(labels)) / len(labels)
        else:
            count = np.bincount(labels, minlength=self.num_classes)
            sample_weight = count[labels] / np.sum(count)

        sample_weight = torch.tensor(sample_weight, dtype=torch.float)

        return sample_weight

    def __str__(self) -> str:
        r"""Returns a string representation of the dataset object.

        Returns:
            str: The string representation of the dataset object.
        """
        modalities_str = [
            "\nDataset info:",
            f"\n   number of modalities: {self.num_modalities}",
            f"\n   number of classes: {self.num_classes}",
            "\n\n   modality | total samples | num train | num test  | num features",
            f"\n   {'-' * 65}",
        ]
        for modality in range(self.num_modalities):
            modality_data = self.get(modality)
            modalities_str.append(
                f"\n   {modality + 1:<8} | "
                f"{len(modality_data.x):<13} | "
                f"{len(modality_data.x[modality_data.train_idx]):<9} | "
                f"{len(modality_data.x[modality_data.test_idx]):<9} | "
                f"{modality_data.num_features:<12}"
            )

        modalities_str.append(f"\n   {'-' * 65}\n\n")
        return "".join(modalities_str)
</file>

<file path="kale/loaddata/polypharmacy_datasets.py">
import os

import torch
from torch.utils.data import Dataset
from torch_geometric.data.data import Data

from kale.utils.download import download_file_by_url


class PolypharmacyDataset(Dataset):
    r"""Polypharmacy side effect prediction dataset. Only for full-batch training.

    Args:
        url (string): The url to download the dataset from.
        root (string): The root directory containing the dataset file.
        name (string): Name of the dataset.
        mode (string): "train", "valid" or "test". Defaults to "train".
    """

    def __init__(self, url: str, root: str, name: str, mode: str = "train"):
        super(PolypharmacyDataset, self).__init__()

        self.url = url
        self.root = root
        self.name = name
        data = self.load_data()

        self.edge_index = data.__getitem__(f"{mode}_idx")
        self.edge_type = data.__getitem__(f"{mode}_et")
        self.edge_type_range = data.__getitem__(f"{mode}_range")

        if mode == "train":
            self.protein_feat = data.g_feat
            self.protein_edge_index = data.gg_edge_index
            self.drug_feat = data.d_feat
            self.protein_drug_edge_index = data.gd_edge_index

        self.len = self.edge_type_range.shape[0]

    def load_data(self) -> Data:
        """Setup dataset: download if need and load it."""

        # download data if not exist
        download_file_by_url(self.url, self.root, f"{self.name}.pt")
        data_path = os.path.join(self.root, f"{self.name}.pt")

        # load data
        return torch.load(data_path)

    def __len__(self):
        return 1

    def __getitem__(self, idx):
        return self.edge_index, self.edge_type, self.edge_type_range
</file>

<file path="kale/loaddata/README.md">
# Data loading modules

Data loading functions to get the input data from somewhere.
</file>

<file path="kale/loaddata/sampler.py">
"""Various sampling strategies for datasets to construct dataloader,
from https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/sampler.py
"""

import logging

import numpy as np
import torch.utils.data
import torchvision
from torch.utils.data.sampler import BatchSampler, RandomSampler


class SamplingConfig:
    def __init__(self, balance=False, class_weights=None, balance_domain=False):
        """Sampler configuration

        Args:
            balance (bool, optional): If True, samples equal number of training samples for each class per batch.
                Defaults to False.
            class_weights (list, optional): Weights of classes if the classes are not equally weighted.
                Defaults to None.
            balance_domain (bool, optional): If True, samples equal number of training samples for each domain per
                batch. Defaults to False.
        """
        if balance and class_weights is not None:
            raise ValueError("Params 'balance' and 'weights' are incompatible")
        self._balance = balance
        self._balance_domain = balance_domain
        self._class_weights = class_weights
        self._balance_domain = balance_domain

    def create_loader(self, dataset, batch_size):
        """Create the data loader

        Reference: https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler

        Args:
            dataset (Dataset): dataset from which to load the data.
            batch_size (int): how many samples per batch to load
        """
        if self._balance:
            sampler = BalancedBatchSampler(dataset, batch_size=batch_size)
        elif self._class_weights is not None:
            sampler = ReweightedBatchSampler(dataset, batch_size=batch_size, class_weights=self._class_weights)
        elif self._balance_domain:
            sampler = DomainBalancedBatchSampler(dataset, batch_size=batch_size)
        else:
            if len(dataset) < batch_size:
                sub_sampler = RandomSampler(dataset, replacement=True, num_samples=batch_size)
            else:
                sub_sampler = RandomSampler(dataset)
            sampler = BatchSampler(sub_sampler, batch_size=batch_size, drop_last=True)
        return torch.utils.data.DataLoader(dataset=dataset, batch_sampler=sampler)


class FixedSeedSamplingConfig(SamplingConfig):
    def __init__(self, seed=1, balance=False, class_weights=None, balance_domain=False):
        """Sampling with fixed seed."""
        super(FixedSeedSamplingConfig, self).__init__(balance, class_weights, balance_domain)
        self._seed = seed

    def create_loader(self, dataset, batch_size):
        """Create the data loader with fixed seed."""
        if self._balance:
            sampler = BalancedBatchSampler(dataset, batch_size=batch_size)
        elif self._class_weights is not None:
            sampler = ReweightedBatchSampler(dataset, batch_size=batch_size, class_weights=self._class_weights)
        elif self._balance_domain:
            sampler = DomainBalancedBatchSampler(dataset, batch_size=batch_size)
        else:
            if len(dataset) < batch_size:
                sub_sampler = RandomSampler(
                    dataset,
                    replacement=True,
                    num_samples=batch_size,
                    generator=torch.Generator().manual_seed(self._seed),
                )
            else:
                sub_sampler = RandomSampler(dataset, generator=torch.Generator().manual_seed(self._seed))
            sampler = BatchSampler(sub_sampler, batch_size=batch_size, drop_last=True)
        return torch.utils.data.DataLoader(dataset=dataset, batch_sampler=sampler)


# TODO: deterministic shuffle?
class MultiDataLoader:
    """
    Batch Sampler for a MultiDataset. Iterates in parallel over different batch samplers for each dataset.
    Yields batches [(x_1, y_1), ..., (x_s, y_s)] for s datasets.
    """

    def __init__(self, dataloaders, n_batches):
        if n_batches <= 0:
            raise ValueError("n_batches should be > 0")
        self._dataloaders = dataloaders
        self._n_batches = np.maximum(1, n_batches)
        self._init_iterators()

    def _init_iterators(self):
        self._iterators = [iter(dl) for dl in self._dataloaders]

    def _get_nexts(self):
        def _get_next_dl_batch(di, dl):
            try:
                batch = next(dl)
            except StopIteration:
                logging.debug(f"reinit loader {di} of type {type(dl)}")
                new_dl = iter(self._dataloaders[di])
                self._iterators[di] = new_dl
                batch = next(new_dl)
            return batch

        return [_get_next_dl_batch(di, dl) for di, dl in enumerate(self._iterators)]

    def __iter__(self):
        for _ in range(self._n_batches):
            yield self._get_nexts()
        self._init_iterators()

    def __len__(self):
        return self._n_batches


class BalancedBatchSampler(torch.utils.data.sampler.BatchSampler):
    """
    BatchSampler - from a MNIST-like dataset, samples n_samples for each of the n_classes.
    Returns batches of size n_classes * (batch_size // n_classes)
    adapted from https://github.com/adambielski/siamese-triplet/blob/master/datasets.py
    """

    def __init__(self, dataset, batch_size):
        labels = get_labels(dataset)
        classes = sorted(set(labels))

        n_classes = len(classes)
        self._n_samples = batch_size // n_classes
        if self._n_samples == 0:
            raise ValueError(f"batch_size should be bigger than the number of classes, got {batch_size}")

        self._class_iters = [InfiniteSliceIterator(np.where(labels == class_)[0], class_=class_) for class_ in classes]

        batch_size = self._n_samples * n_classes
        self._n_batches = len(labels) // batch_size
        if self._n_batches == 0:
            raise ValueError(f"Dataset is not big enough to generate batches with size {batch_size}")
        logging.debug("K=", n_classes, "nk=", self._n_samples)
        logging.debug("Batch size = ", batch_size)

    def __iter__(self):
        for _ in range(self._n_batches):
            indices = []
            for class_iter in self._class_iters:
                indices.extend(class_iter.get(self._n_samples))
            np.random.shuffle(indices)
            yield indices

        for class_iter in self._class_iters:
            class_iter.reset()

    def __len__(self):
        return self._n_batches


class ReweightedBatchSampler(torch.utils.data.sampler.BatchSampler):
    """
    BatchSampler - from a MNIST-like dataset, samples batch_size according to given input distribution
    assuming multi-class labels
    adapted from https://github.com/adambielski/siamese-triplet/blob/master/datasets.py
    """

    # /!\ 'class_weights' should be provided in the "natural order" of the classes (i.e. sorted(classes)) /!\
    def __init__(self, dataset, batch_size, class_weights):
        labels = get_labels(dataset)
        self._classes = sorted(set(labels))

        n_classes = len(self._classes)
        if n_classes > len(class_weights):
            k = len(class_weights)
            sum_w = np.sum(class_weights)
            if sum_w >= 1:
                # normalize attributing equal weight to weighted part and remaining part
                class_weights /= sum_w * k / n_classes + (n_classes - k) / n_classes
            krem = k - n_classes
            wrem = 1 - sum_w
            logging.warning(f"will assume uniform distribution for labels > {len(class_weights)}")
            self._class_weights = np.ones(n_classes, dtype=np.float64)
            self._class_weights[:k] = class_weights
            self._class_weights[k:] = wrem / krem
        else:
            self._class_weights = class_weights[:n_classes]

        if np.sum(self._class_weights) != 1:
            self._class_weights = self._class_weights / np.sum(self._class_weights)

        logging.debug("Using weights=", self._class_weights)
        if batch_size == 0:
            raise ValueError(f"batch_size should be bigger than the number of classes, got {batch_size}")

        self._class_to_iter = {
            class_: InfiniteSliceIterator(np.where(labels == class_)[0], class_=class_) for class_ in self._classes
        }

        self.n_dataset = len(labels)
        self._batch_size = batch_size
        self._n_batches = self.n_dataset // self._batch_size
        if self._n_batches == 0:
            raise ValueError(f"Dataset is not big enough to generate batches with size {self._batch_size}")
        logging.debug("K=", n_classes, "nk=", self._batch_size)
        logging.debug("Batch size = ", self._batch_size)

    def __iter__(self):
        for _ in range(self._n_batches):
            # sample batch_size classes
            class_idx = np.random.choice(
                self._classes,
                p=self._class_weights,
                replace=True,
                size=self._batch_size,
            )
            indices = []
            for class_, num in zip(*np.unique(class_idx, return_counts=True)):
                indices.extend(self._class_to_iter[class_].get(num))
            np.random.shuffle(indices)
            yield indices

        for class_iter in self._class_to_iter.values():
            class_iter.reset()

    def __len__(self):
        return self._n_batches


def get_labels(dataset):
    """
    Get class labels for dataset
    """

    dataset_type = type(dataset)
    if dataset_type is torchvision.datasets.SVHN:
        return dataset.labels
    if dataset_type is torchvision.datasets.ImageFolder:
        return np.array(dataset.targets)

    # Handle subset, recurses into non-subset version
    if dataset_type is torch.utils.data.Subset:
        indices = dataset.indices
        all_labels = get_labels(dataset.dataset)
        logging.debug(f"data subset of len {len(indices)} from {len(all_labels)}")
        labels = all_labels[indices]
        if isinstance(labels, torch.Tensor):
            return labels.numpy()
        return labels

    try:
        logging.debug(dataset.targets.shape, type(dataset.targets))
        if isinstance(dataset.targets, torch.Tensor):
            return dataset.targets.numpy()
        return dataset.targets
    except AttributeError:
        logging.error(type(dataset))


class InfiniteSliceIterator:
    def __init__(self, array, class_):
        assert type(array) is np.ndarray
        self.array = array
        self.i = 0
        self.class_ = class_

    def reset(self):
        self.i = 0

    def get(self, n):
        len_ = len(self.array)
        # not enough element in 'array'
        if len_ < n:
            logging.debug(f"there are really few items in class {self.class_}")
            self.reset()
            np.random.shuffle(self.array)
            mul = n // len_
            rest = n - mul * len_
            return np.concatenate((np.tile(self.array, mul), self.array[:rest]))

        # not enough element in array's tail
        if len_ - self.i < n:
            self.reset()

        if self.i == 0:
            np.random.shuffle(self.array)
        i = self.i
        self.i += n
        return self.array[i : self.i]


class DomainBalancedBatchSampler(BalancedBatchSampler):
    """BatchSampler - samples n_samples for each of the n_domains.
        Returns batches of size n_domains * (batch_size / n_domains)

    Args:
        dataset (.multi_domain.MultiDomainImageFolder or torch.utils.data.Subset): Multi-domain data access.
        batch_size (int): Batch size

    """

    def __init__(self, dataset, batch_size):
        # call to __init__ of super class will generate class balanced sampler, do not do it here
        dataset_type = type(dataset)
        if dataset_type is torch.utils.data.Subset:
            domain_labels = np.asarray(dataset.dataset.domain_labels)[dataset.indices]
            domains = list(dataset.dataset.domain_to_idx.values())
        else:
            domain_labels = np.asarray(dataset.domain_labels)
            domains = list(dataset.domain_to_idx.values())

        n_domains = len(domains)

        self._n_samples = batch_size // n_domains
        if self._n_samples == 0:
            raise ValueError(f"batch_size should be bigger than the number of classes, got {batch_size}")

        self._class_iters = [
            InfiniteSliceIterator(np.where(domain_labels == domain_)[0], class_=domain_) for domain_ in domains
        ]
        batch_size = self._n_samples * n_domains

        self._n_batches = len(domain_labels) // batch_size
        if self._n_batches == 0:
            raise ValueError(f"Dataset is not big enough to generate batches with size {batch_size}")
        logging.debug("K=", n_domains, "nk=", self._n_samples)
        logging.debug("Batch size = ", batch_size)
</file>

<file path="kale/loaddata/tabular_access.py">
"""Authors: Lawrence Schobs, lawrenceschobs@gmail.com

Functions for accessing tabular data.

"""

from typing import List, Union

import numpy as np
import pandas as pd


def load_csv_columns(
    datapath: str, split: str, fold: Union[int, List[int]], cols_to_return: Union[str, List[str]] = "All"
) -> pd.DataFrame:
    """
    Reads a CSV file of data and returns samples where the value of the specified
    `split` column is contained in the `fold` variable. The columns specified in `cols_to_return` are returned.

    Args:
        datapath: The path to the CSV file of data.
        split: The column name for the split (e.g. "Validation", "Testing").
        fold: The fold/s contained in the split column to return. Can be a single integer or a list of integers.
        cols_to_return: Which columns to return. If set to "All", returns all columns.

    Returns:
        A tuple of two pandas DataFrames: the first is the full DataFrame selected, and the second is the DataFrame
        with only the columns specified in `cols_to_return`.
    """
    # Load the uncertainty & error results
    datafame = pd.read_csv(datapath + ".csv", header=0)

    if cols_to_return == "All":
        cols_to_return = datafame.columns
    elif not isinstance(cols_to_return, (list, pd.core.series.Series, np.ndarray)):
        cols_to_return = [cols_to_return]

    # Test if a single fold or list of folds
    if isinstance(fold, (list, pd.core.series.Series, np.ndarray)):
        return_data = (datafame.loc[datafame[split].isin(fold)]).loc[:, cols_to_return]
    else:
        return_data = (datafame.loc[datafame[split] == fold]).loc[:, cols_to_return]

    return return_data
</file>

<file path="kale/loaddata/tdc_datasets.py">
import torch
from tdc.multi_pred import DTI
from torch.utils import data

from kale.prepdata.chem_transform import integer_label_protein, integer_label_smiles


class BindingDBDataset(data.Dataset):
    """
    A custom dataset for loading and processing original TDC data, which is used as input data in DeepDTA model.

    Args:
         name (str): TDC dataset name.
         split (str): Data split type (train, valid or test).
         path (str): dataset download/local load path (default: "./data")
         mode (str): encoding mode (default: cnn_cnn)
         drug_transform: Transform operation (default: None)
         protein_transform: Transform operation (default: None)
         y_log (bool): Whether convert y values to log space. (default: True)
    """

    def __init__(
        self,
        name: str,
        split="train",
        path="./data",
        mode="cnn_cnn",
        y_log=True,
        drug_transform=None,
        protein_transform=None,
    ):
        self.data = DTI(name=name, path=path)
        self.mode = mode.lower()
        if y_log:
            self.data.convert_to_log()
        self.data = self.data.get_split()[split]
        self.drug_transform = drug_transform
        self.protein_transform = protein_transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        drug, protein, label = self.data["Drug"][idx], self.data["Target"][idx], self.data["Y"][idx]
        mode_drug, mode_protein = self.mode.split("_")
        if mode_drug == "cnn":
            drug = torch.LongTensor(integer_label_smiles(drug))
        if mode_protein == "cnn":
            protein = torch.LongTensor(integer_label_protein(protein))
        label = torch.Tensor([label])
        if self.drug_transform is not None:
            self.drug_transform(drug)
        if self.protein_transform is not None:
            self.protein_transform(protein)
        return drug, protein, label
</file>

<file path="kale/loaddata/usps.py">
"""
Dataset setting and data loader for USPS, from
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/dataset_usps.py
(based on https://github.com/mingyuliutw/CoGAN/blob/master/cogan_pytorch/src/dataset_usps.py)
"""
import gzip
import logging
import os
import pickle
import urllib

import numpy as np
import torch
import torch.utils.data as data


class USPS(data.Dataset):
    """USPS Dataset.

    Args:
        root (string): Root directory of dataset where dataset file exist.
        train (bool, optional): If True, resample from dataset randomly.
        download (bool, optional): If true, downloads the dataset
            from the internet and puts it in root directory.
            If dataset is already downloaded, it is not downloaded again.
        transform (callable, optional): A function/transform that takes in
            an PIL image and returns a transformed version.
            E.g, ``transforms.RandomCrop``
    """

    url = "https://raw.githubusercontent.com/mingyuliutw/CoGAN/master/cogan_pytorch/data/uspssample/usps_28x28.pkl"

    def __init__(self, root, train=True, transform=None, download=False):
        """Init USPS dataset."""
        # init params
        self.root = os.path.expanduser(root)
        self.filename = "usps_28x28.pkl"
        self.train = train
        # Num of Train = 7438, Num ot Test 1860
        self.transform = transform
        self.dataset_size = None

        # download dataset.
        if download:
            self.download()
        if not self._check_exists():
            raise RuntimeError("Dataset not found." + " You can use download=True to download it")

        self.data, self.targets = self.load_samples()
        self.targets = torch.LongTensor(self.targets)
        if self.train:
            total_num_samples = self.data.shape[0]
            indices = np.arange(total_num_samples)
            np.random.shuffle(indices)
            self.data = self.data[indices[0 : self.dataset_size], ::]
            self.targets = self.targets[indices[0 : self.dataset_size]]
        # self.train_data *= 255.0  # TODO check bug
        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC

    def __getitem__(self, index):
        """Get images and target for data loader.
        Args:
            index (int): Index
        Returns:
            tuple: (image, target) where target is index of the target class.
        """
        img, label = self.data[index, ::], self.targets[index]
        if self.transform is not None:
            img = self.transform(img)
        label = torch.LongTensor([np.int64(label).item()])
        # label = torch.FloatTensor([label.item()])
        return img, label

    def __len__(self):
        """Return size of dataset."""
        return self.dataset_size

    def _check_exists(self):
        """Check if dataset is download and in right place."""
        return os.path.exists(os.path.join(self.root, self.filename))

    def download(self):
        """Download dataset."""
        filename = os.path.join(self.root, self.filename)
        dirname = os.path.dirname(filename)
        if not os.path.isdir(dirname):
            os.makedirs(dirname)
        if os.path.isfile(filename):
            return
        logging.info(f"Download {self.url} to {os.path.abspath(filename)}")
        urllib.request.urlretrieve(self.url, filename)
        logging.info("[DONE]")
        return

    def load_samples(self):
        """Load sample images from dataset."""
        filename = os.path.join(self.root, self.filename)
        f = gzip.open(filename, "rb")
        data_set = pickle.load(f, encoding="bytes")
        f.close()
        if self.train:
            images = data_set[0][0]
            labels = data_set[0][1]
            self.dataset_size = labels.shape[0]
        else:
            images = data_set[1][0]
            labels = data_set[1][1]
            self.dataset_size = labels.shape[0]
        return images, labels
</file>

<file path="kale/loaddata/video_access.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""
Action video dataset loading for EPIC-Kitchen, ADL, GTEA, KITCHEN. The code is based on
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/digits_dataset_access.py
"""

import os
from copy import deepcopy
from enum import Enum

import torch

import kale.prepdata.video_transform as video_transform
from kale.loaddata.dataset_access import DatasetAccess
from kale.loaddata.video_datasets import BasicVideoDataset, EPIC


def get_image_modality(image_modality):
    """Change image_modality (string) to rgb (bool) and flow (bool) for efficiency"""

    if image_modality == "joint":
        rgb = flow = True
    elif image_modality == "rgb" or image_modality == "flow":
        rgb = image_modality == "rgb"
        flow = image_modality == "flow"
    else:
        raise Exception("Invalid modality option: {}".format(image_modality))
    return rgb, flow


def get_videodata_config(cfg):
    """Get the configure parameters for video data from the cfg files"""

    config_params = {
        "data_params": {
            "dataset_root": cfg.DATASET.ROOT,
            "dataset_src_name": cfg.DATASET.SOURCE,
            "dataset_src_trainlist": cfg.DATASET.SRC_TRAINLIST,
            "dataset_src_testlist": cfg.DATASET.SRC_TESTLIST,
            "dataset_tgt_name": cfg.DATASET.TARGET,
            "dataset_tgt_trainlist": cfg.DATASET.TGT_TRAINLIST,
            "dataset_tgt_testlist": cfg.DATASET.TGT_TESTLIST,
            "dataset_image_modality": cfg.DATASET.IMAGE_MODALITY,
            "frames_per_segment": cfg.DATASET.FRAMES_PER_SEGMENT,
        }
    }
    return config_params


def generate_list(data_name, data_params_local, domain):
    """

    Args:
        data_name (string): name of dataset
        data_params_local (dict): hyperparameters from configure file
        domain (string): domain type (source or target)

    Returns:
        data_path (string): image directory of dataset
        train_listpath (string): training list file directory of dataset
        test_listpath (string): test list file directory of dataset
    """

    if data_name == "EPIC":
        dataset_path = os.path.join(data_params_local["dataset_root"], data_name, "EPIC_KITCHENS_2018")
        data_path = os.path.join(dataset_path, "frames_rgb_flow")
    elif data_name in ["ADL", "GTEA", "KITCHEN"]:
        dataset_path = os.path.join(data_params_local["dataset_root"], data_name)
        data_path = os.path.join(dataset_path, "frames_rgb_flow")
    else:
        raise ValueError("Wrong dataset name. Select from [EPIC, ADL, GTEA, KITCHEN]")

    train_listpath = os.path.join(
        dataset_path, "annotations", "labels_train_test", data_params_local["dataset_{}_trainlist".format(domain)]
    )
    test_listpath = os.path.join(
        dataset_path, "annotations", "labels_train_test", data_params_local["dataset_{}_testlist".format(domain)]
    )

    return data_path, train_listpath, test_listpath


class VideoDataset(Enum):
    EPIC = "EPIC"
    ADL = "ADL"
    GTEA = "GTEA"
    KITCHEN = "KITCHEN"

    @staticmethod
    def get_source_target(source: "VideoDataset", target: "VideoDataset", seed, params):
        """
        Gets data loaders for source and target datasets
        Sets channel_number as 3 for RGB, 2 for flow.
        Sets class_number as 8 for EPIC, 7 for ADL, 6 for both GTEA and KITCHEN.

        Args:
            source (VideoDataset): source dataset name
            target (VideoDataset): target dataset name
            seed (int): seed value set manually
            params (CfgNode): hyperparameters from configure file

        Examples:
            >>> source, target, num_classes = get_source_target(source, target, seed, params)
        """
        config_params = get_videodata_config(params)
        data_params = config_params["data_params"]
        data_params_local = deepcopy(data_params)
        data_src_name = data_params_local["dataset_src_name"].upper()
        src_data_path, src_tr_listpath, src_te_listpath = generate_list(data_src_name, data_params_local, domain="src")
        data_tgt_name = data_params_local["dataset_tgt_name"].upper()
        tgt_data_path, tgt_tr_listpath, tgt_te_listpath = generate_list(data_tgt_name, data_params_local, domain="tgt")
        image_modality = data_params_local["dataset_image_modality"]
        frames_per_segment = data_params_local["frames_per_segment"]

        rgb, flow = get_image_modality(image_modality)

        transform_names = {
            VideoDataset.EPIC: "epic",
            VideoDataset.GTEA: "gtea",
            VideoDataset.ADL: "adl",
            VideoDataset.KITCHEN: "kitchen",
        }

        class_numbers = {
            VideoDataset.EPIC: 8,
            VideoDataset.GTEA: 6,
            VideoDataset.ADL: 7,
            VideoDataset.KITCHEN: 6,
        }

        factories = {
            VideoDataset.EPIC: EPICDatasetAccess,
            VideoDataset.GTEA: GTEADatasetAccess,
            VideoDataset.ADL: ADLDatasetAccess,
            VideoDataset.KITCHEN: KITCHENDatasetAccess,
        }

        # handle color/nb classes
        num_classes = min(class_numbers[source], class_numbers[target])
        source_tf = transform_names[source]
        target_tf = transform_names[target]

        rgb_source, rgb_target, flow_source, flow_target = [None] * 4

        if rgb:
            rgb_source = factories[source](
                src_data_path,
                src_tr_listpath,
                src_te_listpath,
                "rgb",
                frames_per_segment,
                num_classes,
                source_tf,
                seed,
            )
            rgb_target = factories[target](
                tgt_data_path,
                tgt_tr_listpath,
                tgt_te_listpath,
                "rgb",
                frames_per_segment,
                num_classes,
                target_tf,
                seed,
            )

        if flow:
            flow_source = factories[source](
                src_data_path,
                src_tr_listpath,
                src_te_listpath,
                "flow",
                frames_per_segment,
                num_classes,
                source_tf,
                seed,
            )
            flow_target = factories[target](
                tgt_data_path,
                tgt_tr_listpath,
                tgt_te_listpath,
                "flow",
                frames_per_segment,
                num_classes,
                target_tf,
                seed,
            )

        return (
            {"rgb": rgb_source, "flow": flow_source},
            {"rgb": rgb_target, "flow": flow_target},
            num_classes,
        )


class VideoDatasetAccess(DatasetAccess):
    """
    Common API for video dataset access

    Args:
        data_path (string): image directory of dataset
        train_list (string): training list file directory of dataset
        test_list (string): test list file directory of dataset
        image_modality (string): image type (RGB or Optical Flow)
        frames_per_segment (int): length of each action sample (the unit is number of frame)
        n_classes (int): number of class
        transform_kind (string): types of video transforms
        seed (int): seed value set manually
    """

    def __init__(
        self, data_path, train_list, test_list, image_modality, frames_per_segment, n_classes, transform_kind, seed
    ):
        super().__init__(n_classes)
        self._data_path = data_path
        self._train_list = train_list
        self._test_list = test_list
        self._image_modality = image_modality
        self._frames_per_segment = frames_per_segment
        self._transform = video_transform.get_transform(transform_kind, self._image_modality)
        self._seed = seed

    def get_train_valid(self, valid_ratio):
        """Get the train and validation dataset with the fixed random split. This is used for joint input like RGB and
        optical flow, which will call `get_train_valid` twice. Fixing the random seed here can keep the seeds for twice
        the same."""
        train_dataset = self.get_train()
        ntotal = len(train_dataset)
        ntrain = int((1 - valid_ratio) * ntotal)
        return torch.utils.data.random_split(
            train_dataset, [ntrain, ntotal - ntrain], generator=torch.Generator().manual_seed(self._seed)
        )


class EPICDatasetAccess(VideoDatasetAccess):
    """EPIC data loader"""

    def get_train(self):
        return EPIC(
            root_path=self._data_path,
            annotationfile_path=self._train_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg",
            transform=self._transform["train"],
            random_shift=True,
            test_mode=False,
            image_modality=self._image_modality,
            dataset_split="train",
            n_classes=self._n_classes,
        )

    def get_test(self):
        return EPIC(
            root_path=self._data_path,
            annotationfile_path=self._test_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg",
            transform=self._transform["test"],
            random_shift=False,
            test_mode=True,
            image_modality=self._image_modality,
            dataset_split="test",
            n_classes=self._n_classes,
        )


class GTEADatasetAccess(VideoDatasetAccess):
    """GTEA data loader"""

    def get_train(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._train_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["train"],
            random_shift=False,
            test_mode=False,
            image_modality=self._image_modality,
            dataset_split="train",
            n_classes=self._n_classes,
        )

    def get_test(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._test_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["test"],
            random_shift=False,
            test_mode=True,
            image_modality=self._image_modality,
            dataset_split="test",
            n_classes=self._n_classes,
        )


class ADLDatasetAccess(VideoDatasetAccess):
    """ADL data loader"""

    def get_train(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._train_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["train"],
            random_shift=False,
            test_mode=False,
            image_modality=self._image_modality,
            dataset_split="train",
            n_classes=self._n_classes,
        )

    def get_test(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._test_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["test"],
            random_shift=False,
            test_mode=True,
            image_modality=self._image_modality,
            dataset_split="test",
            n_classes=self._n_classes,
        )


class KITCHENDatasetAccess(VideoDatasetAccess):
    """KITCHEN data loader"""

    def get_train(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._train_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["train"],
            random_shift=False,
            test_mode=False,
            image_modality=self._image_modality,
            dataset_split="train",
            n_classes=self._n_classes,
        )

    def get_test(self):
        return BasicVideoDataset(
            root_path=self._data_path,
            annotationfile_path=self._test_list,
            num_segments=1,
            frames_per_segment=self._frames_per_segment,
            imagefile_template="frame_{:010d}.jpg" if self._image_modality in ["rgb"] else "flow_{}_{:010d}.jpg",
            transform=self._transform["test"],
            random_shift=False,
            test_mode=True,
            image_modality=self._image_modality,
            dataset_split="test",
            n_classes=self._n_classes,
        )
</file>

<file path="kale/loaddata/video_datasets.py">
import logging
import math
import os
from pathlib import Path

import pandas as pd
from PIL import Image

from kale.loaddata.videos import VideoFrameDataset, VideoRecord


class BasicVideoDataset(VideoFrameDataset):
    """
    Dataset for GTEA, ADL and KITCHEN.

    Args:
        root_path (string): The root path in which video folders lie.
        annotationfile_path (string): The annotation file containing one row per video sample.
        dataset_split (string): Split type (train or test)
        image_modality (string): Image modality (RGB or Optical Flow)
        num_segments (int): The number of segments the video should be divided into to sample frames from.
        frames_per_segment (int): The number of frames that should be loaded per segment.
        imagefile_template (string): The image filename template.
        transform (Compose): Video transform.
        random_shift (bool): Whether the frames from each segment should be taken consecutively starting from
                        the center(False) of the segment, or consecutively starting from
                        a random(True) location inside the segment range.
        test_mode (bool): Whether this is a test dataset. If so, chooses frames from segments with random_shift=False.
        n_classes (int): The number of classes.
    """

    def __init__(
        self,
        root_path: str,
        annotationfile_path: str,
        dataset_split: str,
        image_modality: str,
        num_segments: int = 1,
        frames_per_segment: int = 16,
        imagefile_template: str = "img_{:010d}.jpg",
        transform=None,
        random_shift: bool = True,
        test_mode: bool = False,
        n_classes: int = 8,
    ):
        self.root_path = Path(root_path)
        self.image_modality = image_modality
        self.dataset = dataset_split
        self.n_classes = n_classes
        self.img_path = self.root_path.joinpath(self.image_modality)
        super(BasicVideoDataset, self).__init__(
            root_path,
            annotationfile_path,
            image_modality,
            num_segments,
            frames_per_segment,
            imagefile_template,
            transform,
            random_shift,
            test_mode,
        )

    def _parse_list(self):
        self.video_list = [VideoRecord(x, self.img_path) for x in list(self.make_dataset())]

    def make_dataset(self):
        """
        Load data from the EPIC-Kitchen list file and make them into the united format.
        Different datasets correspond to a different number of classes.

        Returns:
            data (list): list of (video_name, start_frame, end_frame, label)
        """

        data = []
        i = 0
        input_file = pd.read_pickle(self.annotationfile_path)
        for line in input_file.values:
            if 0 <= eval(line[5]) < self.n_classes:
                data.append((line[0], eval(line[1]), eval(line[2]), eval(line[5])))
                i = i + 1
        logging.info("Number of {:5} action segments: {}".format(self.dataset, i))
        return data


class EPIC(VideoFrameDataset):
    """
    Dataset for EPIC-Kitchen.
    """

    def __init__(
        self,
        root_path: str,
        annotationfile_path: str,
        dataset_split: str,
        image_modality: str,
        num_segments: int = 1,
        frames_per_segment: int = 16,
        imagefile_template: str = "img_{:010d}.jpg",
        transform=None,
        random_shift: bool = True,
        test_mode: bool = False,
        n_classes: int = 8,
    ):
        self.root_path = Path(root_path)
        self.image_modality = image_modality
        self.dataset = dataset_split
        self.n_classes = n_classes
        self.img_path = self.root_path.joinpath(self.image_modality, self.dataset)
        super(EPIC, self).__init__(
            root_path,
            annotationfile_path,
            image_modality,
            num_segments,
            frames_per_segment,
            imagefile_template,
            transform,
            random_shift,
            test_mode,
        )

    def _parse_list(self):
        self.video_list = [VideoRecord(x, self.img_path) for x in list(self.make_dataset())]

    def _load_image(self, directory, idx):
        if self.image_modality == "rgb":
            return [Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert("RGB")]
        elif self.image_modality == "flow":
            idx = math.ceil(idx / 2) - 1 if idx > 2 else 1
            u_img = Image.open(os.path.join(directory, "u", self.imagefile_template.format(idx))).convert("L")
            v_img = Image.open(os.path.join(directory, "v", self.imagefile_template.format(idx))).convert("L")
            return [u_img, v_img]
        else:
            raise RuntimeError("Input modality is not in [rgb, flow, joint]. Current is {}".format(self.image_modality))

    def make_dataset(self):
        """
        Load data from the EPIC-Kitchen list file and make them into the united format.
        Because the original list files are not the same, inherit from class BasicVideoDataset and be modified.
        """

        data = []
        i = 0
        input_file = pd.read_pickle(self.annotationfile_path)
        for line in input_file.values:
            if line[1] in ["P01", "P08", "P22"]:
                if 0 <= line[9] < self.n_classes:
                    if line[7] - line[6] + 1 >= self.frames_per_segment:
                        label = line[9]
                        data.append((os.path.join(line[1], line[2]), line[6], line[7], label))
                        i = i + 1
        logging.info("Number of {:5} action segments: {}".format(self.dataset, i))
        return data
</file>

<file path="kale/loaddata/video_multi_domain.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""Construct a dataset for videos with (multiple) source and target domains"""

import logging

import numpy as np
from sklearn.utils import check_random_state

from kale.loaddata.dataset_access import get_class_subset
from kale.loaddata.multi_domain import DatasetSizeType, MultiDomainDatasets, WeightingType
from kale.loaddata.sampler import FixedSeedSamplingConfig, MultiDataLoader
from kale.loaddata.video_access import get_image_modality


class VideoMultiDomainDatasets(MultiDomainDatasets):
    def __init__(
        self,
        source_access_dict,
        target_access_dict,
        image_modality,
        seed,
        config_weight_type="natural",
        config_size_type=DatasetSizeType.Max,
        valid_split_ratio=0.1,
        source_sampling_config=None,
        target_sampling_config=None,
        n_fewshot=None,
        random_state=None,
        class_ids=None,
    ):
        """The class controlling how the source and target domains are iterated over when the input is joint.
            Inherited from MultiDomainDatasets.
        Args:
            source_access_dict (dictionary): dictionary of source RGB and flow dataset accessors
            target_access_dict (dictionary): dictionary of target RGB and flow dataset accessors
            image_modality (string): image type (RGB or Optical Flow)
            seed (int): seed value set manually.
            class_ids (list, optional): List of chosen subset of class ids. Defaults to None (=> All Classes).
        """

        self._image_modality = image_modality
        self.rgb, self.flow = get_image_modality(self._image_modality)
        self._seed = seed

        if self.rgb:
            source_access = source_access_dict["rgb"]
            target_access = target_access_dict["rgb"]
        if self.flow:
            source_access = source_access_dict["flow"]
            target_access = target_access_dict["flow"]

        weight_type = WeightingType(config_weight_type)
        size_type = DatasetSizeType(config_size_type)

        if weight_type is WeightingType.PRESET0:
            self._source_sampling_config = FixedSeedSamplingConfig(
                class_weights=np.arange(source_access.n_classes(), 0, -1)
            )
            self._target_sampling_config = FixedSeedSamplingConfig(
                class_weights=np.random.randint(1, 4, size=target_access.n_classes())
            )
        elif weight_type is WeightingType.BALANCED:
            self._source_sampling_config = FixedSeedSamplingConfig(balance=True)
            self._target_sampling_config = FixedSeedSamplingConfig(balance=True)
        elif weight_type not in WeightingType:
            raise ValueError(f"Unknown weighting method {weight_type}.")
        else:
            self._source_sampling_config = FixedSeedSamplingConfig(seed=self._seed)
            self._target_sampling_config = FixedSeedSamplingConfig(seed=self._seed)

        self._source_access_dict = source_access_dict
        self._target_access_dict = target_access_dict
        self._valid_split_ratio = valid_split_ratio
        self._rgb_source_by_split = {}
        self._flow_source_by_split = {}
        self._rgb_target_by_split = {}
        self._flow_target_by_split = {}
        self._size_type = size_type
        self._n_fewshot = n_fewshot
        self._random_state = check_random_state(random_state)
        self._source_by_split = {}
        self._labeled_target_by_split = None
        self._target_by_split = {}
        self.class_ids = class_ids

    def prepare_data_loaders(self):
        if self.rgb:
            logging.debug("Load RGB train and valid")
            (self._rgb_source_by_split["train"], self._rgb_source_by_split["valid"]) = self._source_access_dict[
                "rgb"
            ].get_train_valid(self._valid_split_ratio)
            if self.class_ids is not None:
                self._rgb_source_by_split["train"] = get_class_subset(
                    self._rgb_source_by_split["train"], self.class_ids
                )
                self._rgb_source_by_split["valid"] = get_class_subset(
                    self._rgb_source_by_split["valid"], self.class_ids
                )

            (self._rgb_target_by_split["train"], self._rgb_target_by_split["valid"]) = self._target_access_dict[
                "rgb"
            ].get_train_valid(self._valid_split_ratio)
            if self.class_ids is not None:
                self._rgb_target_by_split["train"] = get_class_subset(
                    self._rgb_target_by_split["train"], self.class_ids
                )
                self._rgb_target_by_split["valid"] = get_class_subset(
                    self._rgb_target_by_split["valid"], self.class_ids
                )

            logging.debug("Load RGB Test")
            self._rgb_source_by_split["test"] = self._source_access_dict["rgb"].get_test()
            self._rgb_target_by_split["test"] = self._target_access_dict["rgb"].get_test()
            if self.class_ids is not None:
                self._rgb_source_by_split["test"] = get_class_subset(self._rgb_source_by_split["test"], self.class_ids)
                self._rgb_target_by_split["test"] = get_class_subset(self._rgb_target_by_split["test"], self.class_ids)

        if self.flow:
            logging.debug("Load flow train and valid")
            (self._flow_source_by_split["train"], self._flow_source_by_split["valid"]) = self._source_access_dict[
                "flow"
            ].get_train_valid(self._valid_split_ratio)
            if self.class_ids is not None:
                self._flow_source_by_split["train"] = get_class_subset(
                    self._flow_source_by_split["train"], self.class_ids
                )
                self._flow_source_by_split["valid"] = get_class_subset(
                    self._flow_source_by_split["valid"], self.class_ids
                )

            (self._flow_target_by_split["train"], self._flow_target_by_split["valid"]) = self._target_access_dict[
                "flow"
            ].get_train_valid(self._valid_split_ratio)
            if self.class_ids is not None:
                self._flow_target_by_split["train"] = get_class_subset(
                    self._flow_target_by_split["train"], self.class_ids
                )
                self._flow_target_by_split["valid"] = get_class_subset(
                    self._flow_target_by_split["valid"], self.class_ids
                )

            logging.debug("Load flow Test")
            self._flow_source_by_split["test"] = self._source_access_dict["flow"].get_test()
            self._flow_target_by_split["test"] = self._target_access_dict["flow"].get_test()
            if self.class_ids is not None:
                self._flow_source_by_split["test"] = get_class_subset(
                    self._flow_source_by_split["test"], self.class_ids
                )
                self._flow_target_by_split["test"] = get_class_subset(
                    self._flow_target_by_split["test"], self.class_ids
                )

    def get_domain_loaders(self, split="train", batch_size=32):
        rgb_source_ds = rgb_target_ds = flow_source_ds = flow_target_ds = None
        rgb_source_loader = rgb_target_loader = flow_source_loader = flow_target_loader = None
        rgb_target_labeled_loader = flow_target_labeled_loader = None
        rgb_target_unlabeled_loader = flow_target_unlabeled_loader = n_dataset = None

        if self.rgb:
            rgb_source_ds = self._rgb_source_by_split[split]
            rgb_source_loader = self._source_sampling_config.create_loader(rgb_source_ds, batch_size)
            rgb_target_ds = self._rgb_target_by_split[split]

        if self.flow:
            flow_source_ds = self._flow_source_by_split[split]
            flow_source_loader = self._source_sampling_config.create_loader(flow_source_ds, batch_size)
            flow_target_ds = self._flow_target_by_split[split]

        if self._labeled_target_by_split is None:
            # unsupervised target domain
            if self.rgb:
                rgb_target_loader = self._target_sampling_config.create_loader(rgb_target_ds, batch_size)
                n_dataset = DatasetSizeType.get_size(self._size_type, rgb_source_ds, rgb_target_ds)
            if self.flow:
                flow_target_loader = self._target_sampling_config.create_loader(flow_target_ds, batch_size)
                n_dataset = DatasetSizeType.get_size(self._size_type, flow_source_ds, flow_target_ds)

            dataloaders = [rgb_source_loader, flow_source_loader, rgb_target_loader, flow_target_loader]
            dataloaders = [x for x in dataloaders if x is not None]

            return MultiDataLoader(
                dataloaders=dataloaders,
                n_batches=max(n_dataset // batch_size, 1),
            )
        else:
            # semi-supervised target domain
            if self.rgb:
                rgb_target_labeled_ds = self._labeled_target_by_split[split]
                rgb_target_unlabeled_ds = rgb_target_ds
                # label domain: always balanced
                rgb_target_labeled_loader = FixedSeedSamplingConfig(balance=True, class_weights=None).create_loader(
                    rgb_target_labeled_ds, batch_size=min(len(rgb_target_labeled_ds), batch_size)
                )

                rgb_target_unlabeled_loader = self._target_sampling_config.create_loader(
                    rgb_target_unlabeled_ds, batch_size
                )
                n_dataset = DatasetSizeType.get_size(
                    self._size_type, rgb_source_ds, rgb_target_labeled_ds, rgb_target_unlabeled_ds
                )
            if self.flow:
                flow_target_labeled_ds = self._labeled_target_by_split[split]
                flow_target_unlabeled_ds = flow_target_ds
                flow_target_labeled_loader = FixedSeedSamplingConfig(balance=True, class_weights=None).create_loader(
                    flow_target_labeled_ds, batch_size=min(len(flow_target_labeled_ds), batch_size)
                )
                flow_target_unlabeled_loader = self._target_sampling_config.create_loader(
                    flow_target_unlabeled_ds, batch_size
                )
                n_dataset = DatasetSizeType.get_size(
                    self._size_type, rgb_source_ds, flow_target_labeled_ds, flow_target_unlabeled_ds
                )

            # combine loaders into a list and remove the loader which is NONE.
            dataloaders = [
                rgb_source_loader,
                flow_source_loader,
                rgb_target_labeled_loader,
                flow_target_labeled_loader,
                rgb_target_unlabeled_loader,
                flow_target_unlabeled_loader,
            ]
            dataloaders = [x for x in dataloaders if x is not None]

            return MultiDataLoader(dataloaders=dataloaders, n_batches=max(n_dataset // batch_size, 1))

    def __len__(self):
        if self.rgb:
            source_ds = self._rgb_source_by_split["train"]
            target_ds = self._rgb_target_by_split["train"]
        if self.flow:
            source_ds = self._flow_source_by_split["train"]
            target_ds = self._flow_target_by_split["train"]

        if self._labeled_target_by_split is None:
            return DatasetSizeType.get_size(self._size_type, source_ds, target_ds)
        else:
            labeled_target_ds = self._labeled_target_by_split["train"]
            return DatasetSizeType.get_size(self._size_type, source_ds, labeled_target_ds, target_ds)
</file>

<file path="kale/loaddata/videos.py">
import math
import os
import os.path
import random
from pathlib import Path

import numpy as np
import torch
from PIL import Image


class VideoRecord(object):
    """
    Helper class for class VideoFrameDataset. This class
    represents a video sample's metadata.

    Args:
        root_datapath: the system path to the root folder
                       of the videos.
        row: A list with four or more elements where 1) The first
             element is the path to the video sample's frames excluding
             the root_datapath prefix 2) The  second element is the starting frame id of the video
             3) The third element is the inclusive ending frame id of the video
             4) The fourth element is the label index.
             5) any following elements are labels in the case of multi-label classification
    """

    def __init__(self, row, root_datapath):
        self._data = row
        self._path = os.path.join(root_datapath, row[0])

    @property
    def path(self):
        return self._path

    @property
    def num_frames(self):
        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive

    @property
    def start_frame(self):
        return int(self._data[1])

    @property
    def end_frame(self):
        return int(self._data[2])

    @property
    def label(self):
        # just one label_id
        if len(self._data) == 4:
            return int(self._data[3])
        # sample associated with multiple labels
        else:
            return [int(label_id) for label_id in self._data[3:]]


class VideoFrameDataset(torch.utils.data.Dataset):
    r"""
    A highly efficient and adaptable dataset class for videos.
    Instead of loading every frame of a video,
    loads x RGB frames of a video (sparse temporal sampling) and evenly
    chooses those frames from start to end of the video, returning
    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``
    tensors where FRAMES=x if the ``kale.prepdata.video_transform.ImglistToTensor()``
    transform is used.

    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS
    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.

    Note:
        A demonstration of using this class can be seen
        in ``PyKale/examples/video_loading``
        https://github.com/pykale/pykale/tree/master/examples/video_loading

    Note:
        This dataset broadly corresponds to the frame sampling technique
        introduced in ``Temporal Segment Networks`` at ECCV2016
        https://arxiv.org/abs/1608.00859.


    Note:
        This class relies on receiving video data in a structure where
        inside a ``ROOT_DATA`` folder, each video lies in its own folder,
        where each video folder contains the frames of the video as
        individual files with a naming convention such as
        img_001.jpg ... img_059.jpg.
        For enumeration and annotations, this class expects to receive
        the path to a .txt file where each video sample has a row with four
        (or more in the case of multi-label, see example README on Github)
        space separated values:
        ``VIDEO_FOLDER_PATH     START_FRAME     END_FRAME     LABEL_INDEX``.
        ``VIDEO_FOLDER_PATH`` is expected to be the path of a video folder
        excluding the ``ROOT_DATA`` prefix. For example, ``ROOT_DATA`` might
        be ``home\data\datasetxyz\videos\``, inside of which a ``VIDEO_FOLDER_PATH``
        might be ``jumping\0052\`` or ``sample1\`` or ``00053\``.

    Args:
        root_path: The root path in which video folders lie.
                   this is ROOT_DATA from the description above.
        annotationfile_path: The .txt annotation file containing
                             one row per video sample as described above.
        image_modality: Image modality (RGB or Optical Flow).
        num_segments: The number of segments the video should
                      be divided into to sample frames from.
        frames_per_segment: The number of frames that should
                            be loaded per segment. For each segment's
                            frame-range, a random start index or the
                            center is chosen, from which frames_per_segment
                            consecutive frames are loaded.
        imagefile_template: The image filename template that video frame files
                            have inside of their video folders as described above.
        transform: Transform pipeline that receives a list of PIL images/frames.
        random_shift: Whether the frames from each segment should be taken
                      consecutively starting from the center of the segment, or
                      consecutively starting from a random location inside the
                      segment range.
        test_mode: Whether this is a test dataset. If so, chooses
                   frames from segments with random_shift=False.

    """

    def __init__(
        self,
        root_path: str,
        annotationfile_path: str,
        image_modality: str = "rgb",
        num_segments: int = 3,
        frames_per_segment: int = 1,
        imagefile_template: str = "img_{:05d}.jpg",
        transform=None,
        random_shift: bool = True,
        test_mode: bool = False,
    ):
        super(VideoFrameDataset, self).__init__()

        self.root_path = Path(root_path)
        self.annotationfile_path = Path(annotationfile_path)
        self.image_modality = image_modality
        self.num_segments = num_segments
        self.frames_per_segment = frames_per_segment
        self.imagefile_template = imagefile_template
        self.transform = transform
        self.random_shift = random_shift
        self.test_mode = test_mode
        if self.image_modality == "flow" and self.frames_per_segment > 1:
            self.frames_per_segment //= 2

        self._parse_list()

    def _load_image(self, directory, idx):
        if self.image_modality == "rgb":
            return [Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert("RGB")]
        elif self.image_modality == "flow":
            idx = math.ceil(idx / 2) - 1 if idx > 2 else 1
            x_img = Image.open(os.path.join(directory, self.imagefile_template.format("x", idx))).convert("L")
            y_img = Image.open(os.path.join(directory, self.imagefile_template.format("y", idx))).convert("L")
            return [x_img, y_img]
        else:
            raise ValueError("Input modality is not in [rgb, flow, joint]. Current is {}".format(self.image_modality))

    def _parse_list(self):
        self.video_list = [VideoRecord(x.strip().split(" "), self.root_path) for x in open(self.annotationfile_path)]

    def _get_random_indices(self, record):
        """
        For each segment, randomly chooses the start frame indexes.

        Args:
            record: VideoRecord denoting a video sample.
        Returns:
            List of indices of segment start frames.
        """

        if record.num_frames > self.num_segments * self.frames_per_segment - 1:
            segment_duration = (record.num_frames - self.frames_per_segment + 1) // self.num_segments
            offsets = np.multiply(list(range(self.num_segments)), segment_duration) + np.random.randint(
                segment_duration, size=self.num_segments
            )
        else:
            offsets = np.sort(random.sample(range(record.num_frames - self.frames_per_segment), self.num_segments))
        return offsets

    def _get_symmetric_indices(self, record):
        """
        For each segment, finds the start frame indexes which are symmetrical.

        Args:
            record: VideoRecord denoting a video sample
        Returns:
            List of indices of segment start frames.
        """

        tick = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)

        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])

        return offsets

    def __getitem__(self, index):
        """
        For video with id index, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT
        frames from evenly chosen locations.

        Args:
            index: Video sample index.
        Returns:
            a list of PIL images or the result
            of applying self.transform on this list if
            self.transform is not None.
        """

        record = self.video_list[index]

        if record.num_frames < self.frames_per_segment:
            raise RuntimeError(
                "Path:{}, start:{}, end:{}.\n Video_length is {}, which should be larger than "
                "frame_per_segment {}.".format(
                    record.path, record.start_frame, record.end_frame, record.num_frames, self.frames_per_segment
                )
            )
        elif record.num_frames < self.num_segments:
            raise RuntimeError(
                "Path:{}, start:{}, end:{}.\n Video_length is {}, which should be larger than "
                "num_segments {}.".format(
                    record.path, record.start_frame, record.end_frame, record.num_frames, self.num_segments
                )
            )
        elif record.num_frames < self.num_segments * self.frames_per_segment:
            if self.num_segments > record.num_frames - self.frames_per_segment + 1:
                raise RuntimeError(
                    "Path:{}, start:{}, end:{}.\n Video_length is {}, num_segments is {} and "
                    "frame_per_segment is {}. Please make num_segments<frame_length-frames_per_segment "
                    "to avoid getting too many same segments.".format(
                        record.path,
                        record.start_frame,
                        record.end_frame,
                        record.num_frames,
                        self.num_segments,
                        self.frames_per_segment,
                    )
                )

        if not self.test_mode:
            segment_indices = (
                self._get_random_indices(record) if self.random_shift else self._get_symmetric_indices(record)
            )
        else:
            segment_indices = self._get_symmetric_indices(record)

        return self._get(record, segment_indices)

    def _get(self, record, indices):
        """
        Loads the frames of a video at the corresponding indices.

        Args:
            record: VideoRecord denoting a video sample.
            indices: Indices at which to load video frames from.
        Returns:
            1) A list of PIL images or the result of applying self.transform on this list if self.transform is not None.
            2) An integer denoting the video label.
        """

        indices = indices + record.start_frame
        images = list()
        image_indices = list()
        for seg_ind in indices:
            frame_index = int(seg_ind)
            for i in range(self.frames_per_segment):
                seg_img = self._load_image(record.path, frame_index)
                images.extend(seg_img)
                image_indices.append(frame_index)
                if frame_index < record.end_frame:
                    frame_index += 1

        if self.transform is not None:
            images = self.transform(images)

        return images, record.label

    def __len__(self):
        return len(self.video_list)
</file>

<file path="kale/pipeline/base_nn_trainer.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haolin Wang, LWang0101@outlook.com
#         Mohammod Naimul Islam Suvon, m.suvon@sheffield.ac.uk
# =============================================================================


"""Classification systems (pipelines)

This module provides neural network (nn) trainers for developing classification task models. The BaseNNTrainer
defines the required fundamental functions and structures, such as the optimizer, learning rate scheduler,
training/validation/testing procedure, workflow, etc. The BaseNNTrainer is inherited to construct specialized trainers.

The structure and workflow of BaseNNTrainer is consistent with `kale.pipeline.domain_adapter.BaseAdaptTrainer`

This module uses `PyTorch Lightning <https://github.com/Lightning-AI/lightning>`_ to standardize the flow.

This module also provides a Multimodal Neural Network Trainer (MultimodalNNTrainer) where this trainer uses separate
encoders for each modality, a fusion technique to combine the modalities, and a classifier head for final prediction.
MultimodalNNTrainer is also designed to handle training, validation, and testing steps for multimodal data using
specified models, optimization algorithms, and loss functions.
Adapted from: https://github.com/pliang279/MultiBench/blob/main/training_structures/Supervised_Learning.py
"""


import pytorch_lightning as pl
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score

import kale.evaluate.metrics as losses


class BaseNNTrainer(pl.LightningModule):
    """Base class for classification models using neural network, based on PyTorch Lightning wrapper. The forward pass
    and loss computation must be implemented if new trainers inherit from this class. The basic workflow is defined
    in this class as follows. Every training/validation/testing procedure will call `compute_loss()` to compute the
    loss and log the output metrics. The `compute_loss()` function will call `forward()` to generate the output feature
    using the neural networks.

    Args:
        optimizer (dict, None): optimizer parameters.
        max_epochs (int): maximum number of epochs.
        init_lr (float): initial learning rate. Defaults to 0.001.
        adapt_lr (bool): whether to use the schedule for the learning rate. Defaults to False.
    """

    def __init__(self, optimizer, max_epochs, init_lr=0.001, adapt_lr=False):
        super(BaseNNTrainer, self).__init__()
        self._optimizer_params = optimizer
        self._max_epochs = max_epochs
        self._init_lr = init_lr
        self._adapt_lr = adapt_lr

    def forward(self, x):
        """Override this function to define the forward pass. Normally includes feature extraction and classification
        and be called in `compute_loss()`.
        """
        raise NotImplementedError("Forward pass needs to be defined.")

    def compute_loss(self, batch, split_name="valid"):
        """Compute loss for a given batch.

        Args:
            batch (tuple): batches returned by dataloader.
            split_name (str, optional): learning stage (one of ["train", "valid", "test"]).
                Defaults to "valid" for validation. "train" is for training and "test" for testing. This is currently
                used only for naming the metrics used for logging.

        Returns:
            loss (torch.Tensor): loss value.
            log_metrics (dict): dictionary of metrics to be logged. This is needed when using PyKale logging, but not
                mandatory when using PyTorch Lightning logging.
        """
        raise NotImplementedError("Loss function needs to be defined.")

    def configure_optimizers(self):
        """Default optimizer configuration. Set Adam to the default and provide SGD with cosine annealing.
        If other optimizers are needed, please override this function.
        """
        if self._optimizer_params is None:
            optimizer = torch.optim.Adam(self.parameters(), lr=self._init_lr)
            return [optimizer]
        if self._optimizer_params["type"] == "Adam":
            optimizer = torch.optim.Adam(
                self.parameters(),
                lr=self._init_lr,
                **self._optimizer_params["optim_params"],
            )
            return [optimizer]
        if self._optimizer_params["type"] == "SGD":
            optimizer = torch.optim.SGD(
                self.parameters(),
                lr=self._init_lr,
                **self._optimizer_params["optim_params"],
            )

            if self._adapt_lr:
                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self._max_epochs)
                return [optimizer], [scheduler]
            return [optimizer]
        raise NotImplementedError(f"Unknown optimizer type {self._optimizer_params['type']}.")

    def training_step(self, train_batch, batch_idx) -> torch.Tensor:
        """Compute and return the training loss and metrics on one step. loss is to store the loss value. log_metrics
        is to store the metrics to be logged, including loss, top1 and/or top5 accuracies.

        Use self.log_dict(log_metrics, on_step, on_epoch, logger) to log the metrics on each step and each epoch. For
        training, log on each step and each epoch. For validation and testing, only log on each epoch. This way can
        avoid using on_training_epoch_end() and on_validation_epoch_end().
        """
        loss, log_metrics = self.compute_loss(train_batch, split_name="train")
        self.log_dict(log_metrics, on_step=True, on_epoch=True, logger=True)

        return loss

    def validation_step(self, valid_batch, batch_idx) -> None:
        """Compute and return the validation loss and metrics on one step."""
        loss, log_metrics = self.compute_loss(valid_batch, split_name="valid")
        self.log_dict(log_metrics, on_step=False, on_epoch=True, logger=True)

    def test_step(self, test_batch, batch_idx) -> None:
        """Compute and return the testing loss and metrics on one step."""
        loss, log_metrics = self.compute_loss(test_batch, split_name="test")
        self.log_dict(log_metrics, on_step=False, on_epoch=True, logger=True)


class CNNTransformerTrainer(BaseNNTrainer):
    """PyTorch Lightning trainer for cnntransformer.

    Args:
        feature_extractor (torch.nn.Sequential, optional): the feature extractor network.
        optimizer (dict): optimizer parameters.
        lr_milestones (list): list of epoch indices. Must be increasing.
        lr_gamma (float): multiplicative factor of learning rate decay.
    """

    def __init__(self, feature_extractor, task_classifier, lr_milestones, lr_gamma, **kwargs):
        super().__init__(**kwargs)
        self.feat = feature_extractor
        self.classifier = task_classifier
        self.lr_milestones = lr_milestones
        self.lr_gamma = lr_gamma

    def forward(self, x):
        """Forward pass for the model with a feature extractor and a classifier."""
        x = self.feat(x)
        output = self.classifier(x)
        return output

    def compute_loss(self, batch, split_name="valid"):
        """Compute loss, top1 and top5 accuracy for a given batch."""
        x, y = batch
        y_hat = self.forward(x)

        loss, _ = losses.cross_entropy_logits(y_hat, y)
        top1, top5 = losses.topk_accuracy(y_hat, y, topk=(1, 5))
        top1 = top1.double().mean()
        top5 = top5.double().mean()

        log_metrics = {
            f"{split_name}_loss": loss,
            f"{split_name}_top1_acc": top1,
            f"{split_name}_top5_acc": top5,
        }

        return loss, log_metrics

    def configure_optimizers(self):
        """Set up an SGD optimizer and multistep learning rate scheduler. When self._adapt_lr is True, the learning
        rate will be decayed by self.lr_gamma every step in milestones.
        """
        optimizer = torch.optim.SGD(
            self.parameters(),
            lr=self._init_lr,
            **self._optimizer_params["optim_params"],
        )
        if self._adapt_lr:
            scheduler = torch.optim.lr_scheduler.MultiStepLR(
                optimizer, milestones=self.lr_milestones, gamma=self.lr_gamma
            )
            return [optimizer], [scheduler]
        return [optimizer]


class MultimodalNNTrainer(pl.LightningModule):
    """MultimodalNNTrainer, serves as a PyTorch Lightning trainer for multimodal models. It is designed to handle
    training, validation, and testing steps for multimodal data using specified models, optimization algorithms, and loss functions.
       For each training, validation, and test step, the trainer class computes the model's loss and accuracy and logs
       these metrics. This trainer simplifies the process of training complex multimodal models, allowing the user to
       focus on model architecture and hyperparameter tuning.
       This trainer is flexible and can be used with various models, optimizers, and loss functions, enabling its use
       across a wide range of multimodal learning tasks.
    Args:
        encoders (List[nn.Module]): A list of PyTorch `nn.Module` encoders, with one encoder per modality. Each encoder
        is responsible for transforming the raw input of a single modality into a high-level representation.
        fusion (nn.Module): A PyTorch `nn.Module` that merges the high-level representations from each modality into a
        single representation.
        head (nn.Module): A PyTorch `nn.Module` that takes the fused representation and outputs a class prediction.
        is_packed (bool, optional):  whether the input modalities are packed in one list or not (default is False, which
        means we expect input of [tensor(20xmodal1_size),(20xmodal2_size),(20xlabel_size)] for batch size 20 and 2 input
        modalities)
        optim (torch.optim, optional): The optimization algorithm to use. Defaults to torch.optim.SGD.
        lr (float, optional): Learning rate for the optimizer. Defaults to 0.001.
        weight_decay (float, optional): Weight decay for the optimizer. Defaults to 0.0.
        objective (torch.nn.Module, optional): Loss function. Defaults to torch.nn.CrossEntropyLoss.
    """

    def __init__(
        self,
        encoders,
        fusion,
        head,
        variable_length_sequences=False,
        optim=torch.optim.SGD,
        lr=0.001,
        weight_decay=0.0,
        objective=nn.CrossEntropyLoss(),
    ):
        super(MultimodalNNTrainer, self).__init__()

        self.encoders = nn.ModuleList(encoders)
        self.fusion_module = fusion
        self.classifier = head
        self.modalities_reps = []
        self.fusion_output = None

        self.variable_length_sequences = variable_length_sequences
        self.optim = optim
        self.lr = lr
        self.weight_decay = weight_decay
        self.objective = objective

    def forward(self, inputs):
        if self.variable_length_sequences:
            with torch.backends.cudnn.flags(enabled=False):
                inputs = [[i.float() for i in inputs[0]], inputs[1]]
        else:
            inputs = [i.float() for i in inputs[:-1]]

        modality_outputs = []
        for i in range(len(inputs)):
            modality_outputs.append(self.encoders[i](inputs[i]))
        self.modalities_reps = modality_outputs
        fused_output = self.fusion_module(modality_outputs)
        self.fusion_output = fused_output
        if type(fused_output) is tuple:
            fused_output = fused_output[0]
        return self.classifier(fused_output)

    def compute_loss(self, batch, split_name="valid"):
        out = self.forward(batch)
        if len(batch[-1].size()) == len(out.size()):
            truth1 = batch[-1].squeeze(len(out.size()) - 1)
        else:
            truth1 = batch[-1]
        loss = self.objective(out, truth1.long())
        pred = torch.argmax(out, dim=1)
        accuracy = accuracy_score(truth1.cpu().numpy(), pred.cpu().numpy())

        return loss, {"loss": loss.item(), "accuracy": accuracy}

    def configure_optimizers(self):
        optimizer = self.optim(
            [p for p in self.parameters() if p.requires_grad], lr=self.lr, weight_decay=self.weight_decay
        )
        return optimizer

    def training_step(self, train_batch, batch_idx):
        loss, metrics = self.compute_loss(train_batch, split_name="train")
        self.log_dict(metrics, on_step=True, on_epoch=True, logger=True)
        return loss

    def validation_step(self, valid_batch, batch_idx):
        loss, metrics = self.compute_loss(valid_batch, split_name="valid")
        self.log_dict(metrics, on_step=False, on_epoch=True, logger=True)

    def test_step(self, test_batch, batch_idx):
        loss, metrics = self.compute_loss(test_batch, split_name="test")
        self.log_dict(metrics, on_step=False, on_epoch=True, logger=True)
</file>

<file path="kale/pipeline/deepdta.py">
import pytorch_lightning as pl
import torch
from torch.nn import functional as F

from kale.evaluate.metrics import concord_index


class BaseDTATrainer(pl.LightningModule):
    """
    Base class for all drug target encoder-decoder architecture models, which is based on pytorch lightning wrapper,
    for more details about pytorch lightning, please check https://github.com/PyTorchLightning/pytorch-lightning.
    If you inherit from this class, a forward pass function must be implemented.

    Args:
        drug_encoder: drug information encoder.
        target_encoder: target information encoder.
        decoder: drug-target representations decoder.
        lr: learning rate. (default: 0.001)
        ci_metric: calculate the Concordance Index (CI) metric, and the operation is time-consuming for large-scale
        dataset. (default: :obj:`False`)
    """

    def __init__(self, drug_encoder, target_encoder, decoder, lr=0.001, ci_metric=False, **kwargs):
        super(BaseDTATrainer, self).__init__()
        self.drug_encoder = drug_encoder
        self.target_encoder = target_encoder
        self.decoder = decoder
        self.lr = lr
        self.ci_metric = ci_metric
        if kwargs:
            self.save_hyperparameters(kwargs)

    def configure_optimizers(self):
        """
        Config adam as default optimizer.
        """
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

    def forward(self, x_drug, x_target):
        """
        Same as :meth:`torch.nn.Module.forward()`
        """
        raise NotImplementedError("Forward pass needs to be defined.")

    def training_step(self, train_batch, batch_idx):
        """
        Compute and return the training loss on one step
        """
        x_drug, x_target, y = train_batch
        y_pred = self(x_drug, x_target)
        loss = F.mse_loss(y_pred, y.view(-1, 1))
        if self.ci_metric:
            ci = concord_index(y, y_pred)
            self.logger.log_metrics({"train_step_ci": ci}, self.global_step)
        self.logger.log_metrics({"train_step_loss": loss}, self.global_step)
        return loss

    def validation_step(self, valid_batch, batch_idx):
        """
        Compute and return the validation loss on one step
        """
        x_drug, x_target, y = valid_batch
        y_pred = self(x_drug, x_target)
        loss = F.mse_loss(y_pred, y.view(-1, 1))
        return loss

    def test_step(self, test_batch, batch_idx):
        """
        Compute and return the test loss on one step
        """
        x_drug, x_target, y = test_batch
        y_pred = self(x_drug, x_target)
        loss = F.mse_loss(y_pred, y.view(-1, 1))
        if self.ci_metric:
            ci = concord_index(y, y_pred)
            self.log("test_ci", ci, on_epoch=True, on_step=False)
        self.log("test_loss", loss, on_epoch=True, on_step=False)
        return loss


class DeepDTATrainer(BaseDTATrainer):
    """
    An implementation of DeepDTA model based on BaseDTATrainer.
    Args:
        drug_encoder: drug CNN encoder.
        target_encoder: target CNN encoder.
        decoder: drug-target MLP decoder.
        lr: learning rate.
    """

    def __init__(self, drug_encoder, target_encoder, decoder, lr=0.001, ci_metric=False, **kwargs):
        super().__init__(drug_encoder, target_encoder, decoder, lr, ci_metric, **kwargs)

    def forward(self, x_drug, x_target):
        """
        Forward propagation in DeepDTA architecture.

        Args:
            x_drug: drug sequence encoding.
            x_target: target protein sequence encoding.
        """
        drug_emb = self.drug_encoder(x_drug)
        target_emb = self.target_encoder(x_target)
        comb_emb = torch.cat((drug_emb, target_emb), dim=1)
        output = self.decoder(comb_emb)
        return output

    def validation_step(self, valid_batch, batch_idx):
        x_drug, x_target, y = valid_batch
        y_pred = self(x_drug, x_target)
        loss = F.mse_loss(y_pred, y.view(-1, 1))
        if self.ci_metric:
            ci = concord_index(y, y_pred)
            self.log("valid_ci", ci, on_epoch=True, on_step=False)
        self.log("valid_loss", loss, on_epoch=True, on_step=False)
        return loss
</file>

<file path="kale/pipeline/domain_adapter.py">
"""Domain adaptation systems (pipelines) with three types of architectures

This module takes individual modules as input and organises them into an architecture. This is taken directly from
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/architectures.py with minor changes.

This module uses `PyTorch Lightning <https://github.com/Lightning-AI/lightning>`_ to standardize the flow.
"""

from enum import Enum
from typing import Optional

import numpy as np
import pytorch_lightning as pl
import torch
from torch.autograd import Function

import kale.evaluate.metrics as losses


class GradReverse(Function):
    """The gradient reversal layer (GRL)

    This is defined in the DANN paper http://jmlr.org/papers/volume17/15-239/15-239.pdf

    Forward pass: identity transformation.
    Backward propagation: flip the sign of the gradient.

    From https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/layers.py
    """

    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha

        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None


def set_requires_grad(model, requires_grad=True):
    """
    Configure whether gradients are required for a model
    """
    for param in model.parameters():
        param.requires_grad = requires_grad


class Method(Enum):
    """
    Lists the available methods.
    Provides a few methods that group the methods by type.
    """

    Source = "Source"
    DANN = "DANN"
    CDAN = "CDAN"
    CDAN_E = "CDAN-E"
    FSDANN = "FSDANN"
    MME = "MME"
    WDGRL = "WDGRL"  # Wasserstein Distance Guided Representation Learning
    WDGRLMod = "WDGRLMod"
    DAN = "DAN"  # Deep Adaptation Networks
    JAN = "JAN"  # Joint Adaptation Networks

    def is_mmd_method(self):
        return self in (Method.DAN, Method.JAN)

    def is_dann_method(self):
        return self in (Method.DANN, Method.Source)

    def is_cdan_method(self):
        return self in (Method.CDAN, Method.CDAN_E)

    def is_fewshot_method(self):
        return self in (Method.FSDANN, Method.MME, Method.Source)

    def allow_supervised(self):
        return self.is_fewshot_method()


def create_mmd_based(method: Method, dataset, feature_extractor, task_classifier, **train_params):
    """MMD-based deep learning methods for domain adaptation: DAN and JAN"""
    if not method.is_mmd_method():
        raise ValueError(f"Unsupported MMD method: {method}")
    if method is Method.DAN:
        return DANTrainer(dataset, feature_extractor, task_classifier, method=method, **train_params)
    if method is Method.JAN:
        return JANTrainer(
            dataset,
            feature_extractor,
            task_classifier,
            method=method,
            kernel_mul=[2.0, 2.0],
            kernel_num=[5, 1],
            **train_params,
        )


def create_dann_like(method: Method, dataset, feature_extractor, task_classifier, critic, **train_params):
    """DANN-based deep learning methods for domain adaptation: DANN, CDAN, CDAN+E"""
    if dataset.is_semi_supervised():
        return create_fewshot_trainer(method, dataset, feature_extractor, task_classifier, critic, **train_params)

    if method.is_dann_method():
        alpha = 0.0 if method is Method.Source else 1.0
        return DANNTrainer(
            alpha=alpha,
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    elif method.is_cdan_method():
        return CDANTrainer(
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            use_entropy=method is Method.CDAN_E,
            **train_params,
        )
    elif method is Method.WDGRL:
        return WDGRLTrainer(
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    elif method is Method.WDGRLMod:
        return WDGRLTrainerMod(
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    else:
        raise ValueError(f"Unsupported method: {method}")


def create_fewshot_trainer(method: Method, dataset, feature_extractor, task_classifier, critic, **train_params):
    """DANN-based few-shot deep learning methods for domain adaptation: FSDANN, MME"""
    if not dataset.is_semi_supervised():
        raise ValueError("Dataset must be semi-supervised for few-shot methods.")

    if method.is_fewshot_method():
        alpha = 0 if method is Method.Source else 1
        return FewShotDANNTrainer(
            alpha=alpha,
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    else:
        raise ValueError(f"Unsupported semi-supervised method: {method}")


class BaseAdaptTrainer(pl.LightningModule):
    r"""Base class for all domain adaptation architectures.

    This class implements the classic building blocks used in all the derived architectures
    for domain adaptation.
    If you inherit from this class, you will have to implement only:
        - a forward pass
        - a `compute_loss` function that returns the task loss :math:`\mathcal{L}_c` and adaptation loss
        :math:`\mathcal{L}_a`, as well as a dictionary for summary statistics and other metrics you may want to have
        access to.

    The default training step uses only the task loss :math:`\mathcal{L}_c` during warmup,
    then uses the loss defined as:

    :math:`\mathcal{L} = \mathcal{L}_c + \lambda \mathcal{L}_a`,

    where :math:`\lambda` will follow the schedule defined by the DANN paper:

    :math:`\lambda_p = \frac{2}{1 + \exp{(-\gamma \cdot p)}} - 1` where :math:`p` the learning progress
    changes linearly from 0 to 1.

    Args:
        dataset (kale.loaddata.multi_domain): the multi-domain datasets to be used for train, validation, and tests.
        feature_extractor (torch.nn.Module): the feature extractor network (mapping inputs :math:`x\in\mathcal{X}`
            to a latent space :math:`\mathcal{Z}`,).
        task_classifier (torch.nn.Module): the task classifier network that learns to predict labels
            :math:`y \in \mathcal{Y}` from latent vectors.
        method (Method, optional): the method implemented by the class. Defaults to None.
            Mostly useful when several methods may be implemented using the same class.
        lambda_init (float, optional): weight attributed to the adaptation part of the loss. Defaults to 1.0.
        adapt_lambda (bool, optional): whether to make lambda grow from 0 to 1 following the schedule from
            the DANN paper. Defaults to True.
        adapt_lr (bool, optional): whether to use the schedule for the learning rate as defined
            in the DANN paper. Defaults to True.
        nb_init_epochs (int, optional): number of warmup epochs (during which lambda=0, training only on the source).
            Defaults to 10.
        nb_adapt_epochs (int, optional): number of training epochs. Defaults to 50.
        batch_size (int, optional): defaults to 32.
        init_lr (float, optional): initial learning rate. Defaults to 1e-3.
        optimizer (dict, optional): optimizer parameters, a dictionary with 2 keys:
            "type": a string in ("SGD", "Adam", "AdamW")
            "optim_params": kwargs for the above PyTorch optimizer.
            Defaults to None.
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        method: Optional[str] = None,
        lambda_init: float = 1.0,
        adapt_lambda: bool = True,
        adapt_lr: bool = True,
        nb_init_epochs: int = 10,
        nb_adapt_epochs: int = 50,
        batch_size: int = 32,
        init_lr: float = 1e-3,
        optimizer: Optional[dict] = None,
    ):
        super().__init__()
        self._method = method

        self._init_lambda = lambda_init
        self.lamb_da = lambda_init
        self._adapt_lambda = adapt_lambda
        self._adapt_lr = adapt_lr

        self._init_epochs = nb_init_epochs
        self._non_init_epochs = nb_adapt_epochs - self._init_epochs
        assert self._non_init_epochs > 0
        self._batch_size = batch_size
        self._init_lr = init_lr
        self._lr_fact = 1.0
        self._grow_fact = 0.0
        self._dataset = dataset
        self.feat = feature_extractor
        self.classifier = task_classifier
        self._dataset.prepare_data_loaders()
        self._nb_training_batches = None  # to be set by method train_dataloader
        self._optimizer_params = optimizer

    @property
    def method(self):
        return self._method

    def _update_batch_epoch_factors(self, batch_id):
        if self.current_epoch >= self._init_epochs:
            delta_epoch = self.current_epoch - self._init_epochs
            p = (batch_id + delta_epoch * self._nb_training_batches) / (
                self._non_init_epochs * self._nb_training_batches
            )
            self._grow_fact = 2.0 / (1.0 + np.exp(-10 * p)) - 1

            if self._adapt_lr:
                self._lr_fact = 1.0 / ((1.0 + 10 * p) ** 0.75)

        if self._adapt_lambda:
            self.lamb_da = self._init_lambda * self._grow_fact

    def forward(self, x):
        raise NotImplementedError("Forward pass needs to be defined.")

    def compute_loss(self, batch, split_name="valid"):
        """Define the loss of the model

        Args:
            batch (tuple): batches returned by the MultiDomainLoader.
            split_name (str, optional): learning stage (one of ["train", "valid", "test"]).
                Defaults to "valid" for validation. "train" is for training and "test" for testing.
                This is currently used only for naming the metrics used for logging.

        Returns:
            a 3-element tuple with task_loss, adv_loss, log_metrics.
            log_metrics should be a dictionary.

        Raises:
            NotImplementedError: children of this classes should implement this method.
        """
        raise NotImplementedError("Loss needs to be defined.")

    #########################################
    # @profile  # For getting active GPU peak memory. Ignore this when training.
    #########################################
    def training_step(self, batch, batch_nb):
        """The most generic of training steps

        Args:
            batch (tuple): the batch as returned by the MultiDomainLoader dataloader iterator:
                2 tuples: (x_source, y_source), (x_target, y_target) in the unsupervised setting
                3 tuples: (x_source, y_source), (x_target_labeled, y_target_labeled), (x_target_unlabeled, y_target_unlabeled) in the semi-supervised setting
            batch_nb (int): id of the current batch.

        Returns:
            dict: must contain a "loss" key with the loss to be used for back-propagation.
                see pytorch-lightning for more details.
        """
        self._update_batch_epoch_factors(batch_nb)

        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="train")
        if self.current_epoch < self._init_epochs:
            # init phase doesn't use few-shot learning
            # ad-hoc decision but makes models more comparable between each other
            loss = task_loss
        else:
            loss = task_loss + self.lamb_da * adv_loss

        log_metrics["train_total_loss"] = loss
        log_metrics["train_adv_loss"] = adv_loss
        log_metrics["train_task_loss"] = task_loss

        self.log_dict(log_metrics, on_step=True, on_epoch=False)

        # logging alpha and lambda when they exist (they exist for DANN and CDAN but not for DAN and JAN)
        self.log("alpha", self.alpha, on_step=False, on_epoch=True) if hasattr(self, "alpha") else None
        self.log("lambda", self.lamb_da, on_step=False, on_epoch=True) if hasattr(self, "lamb_da") else None

        return loss  # required, for backward pass

    def validation_step(self, batch, batch_nb):
        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="valid")
        loss = task_loss + self.lamb_da * adv_loss
        log_metrics["valid_loss"] = loss
        log_metrics["valid_task_loss"] = task_loss
        log_metrics["valid_adv_loss"] = adv_loss
        self.log_dict(log_metrics, on_step=False, on_epoch=True)

    def test_step(self, batch, batch_nb):
        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="test")
        loss = task_loss + self.lamb_da * adv_loss
        log_metrics["test_loss"] = loss
        log_metrics["test_task_loss"] = task_loss
        log_metrics["test_adv_loss"] = adv_loss
        self.log_dict(log_metrics, on_step=False, on_epoch=True)

    def _configure_optimizer(self, parameters):
        if self._optimizer_params is None:
            optimizer = torch.optim.Adam(
                parameters,
                lr=self._init_lr,
                betas=(0.8, 0.999),
                weight_decay=1e-5,
            )
            return [optimizer]
        if self._optimizer_params["type"] == "Adam":
            optimizer = torch.optim.Adam(
                parameters,
                lr=self._init_lr,
                **self._optimizer_params["optim_params"],
            )
            return [optimizer]
        if self._optimizer_params["type"] == "SGD":
            optimizer = torch.optim.SGD(
                parameters,
                lr=self._init_lr,
                **self._optimizer_params["optim_params"],
            )

            if self._adapt_lr:
                feature_sched = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: self._lr_fact)
                return [optimizer], [feature_sched]
            return [optimizer]
        raise NotImplementedError(f"Unknown optimizer type {self._optimizer_params['type']}")

    def configure_optimizers(self):
        return self._configure_optimizer(self.parameters())

    def train_dataloader(self):
        dataloader = self._dataset.get_domain_loaders(split="train", batch_size=self._batch_size)
        self._nb_training_batches = len(dataloader)
        return dataloader

    def val_dataloader(self):
        return self._dataset.get_domain_loaders(split="valid", batch_size=self._batch_size)

    def test_dataloader(self):
        return self._dataset.get_domain_loaders(split="test", batch_size=self._batch_size)


class BaseDANNLike(BaseAdaptTrainer):
    """Common API for DANN-based methods: DANN, CDAN, CDAN+E, WDGRL, MME, FSDANN"""

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        critic,
        alpha=1.0,
        entropy_reg=0.0,  # not used
        adapt_reg=True,  # not used
        batch_reweighting=False,  # not used
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, **base_params)

        self.alpha = alpha

        self._entropy_reg_init = entropy_reg  # not used
        self._entropy_reg = entropy_reg  # not used
        self._adapt_reg = adapt_reg  # not used

        self._reweight_beta = 4  # not used
        self._do_dynamic_batch_weight = batch_reweighting  # not used

        self.domain_classifier = critic

    def _update_batch_epoch_factors(self, batch_id):
        super()._update_batch_epoch_factors(batch_id)
        if self._adapt_reg:
            self._entropy_reg = self._entropy_reg_init * self._grow_fact

    def compute_loss(self, batch, split_name="valid"):
        if len(batch) == 3:
            raise NotImplementedError("DANN does not support semi-supervised setting.")
        (x_s, y_s), (x_tu, y_tu) = batch
        batch_size = len(y_s)

        _, y_hat, d_hat = self.forward(x_s)
        _, y_t_hat, d_t_hat = self.forward(x_tu)

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        loss_dmn_src, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size))
        loss_dmn_tgt, dok_tgt = losses.cross_entropy_logits(d_t_hat, torch.ones(batch_size))

        dok = torch.cat((dok_src, dok_tgt)).double().mean()
        dok_src = dok_src.double().mean()
        dok_tgt = dok_tgt.double().mean()

        adv_loss = loss_dmn_src + loss_dmn_tgt
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
        }
        return task_loss, adv_loss, log_metrics


class DANNTrainer(BaseDANNLike):
    """
    This class implements the DANN architecture from
    Ganin, Yaroslav, et al.
    "Domain-adversarial training of neural networks."
    The Journal of Machine Learning Research¬†(2016)
    https://arxiv.org/abs/1505.07818

    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        critic,
        method=None,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, critic, **base_params)

        if method is None:
            self._method = Method.DANN
        else:
            self._method = Method(method)
            assert self._method.is_dann_method()

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)
        feature = x.view(x.size(0), -1)

        reverse_feature = GradReverse.apply(feature, self.alpha)
        class_output = self.classifier(feature)
        adversarial_output = self.domain_classifier(reverse_feature)
        return feature, class_output, adversarial_output


class CDANTrainer(BaseDANNLike):
    """
    Implements CDAN: Long, Mingsheng, et al. "Conditional adversarial domain adaptation."
    Advances in Neural Information Processing Systems. 2018.
    https://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation.pdf
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        critic,
        use_entropy=False,
        use_random=False,
        random_dim=1024,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, critic, **base_params)
        self.random_layer = None
        self.random_dim = random_dim
        self.entropy = use_entropy
        if use_random:
            nb_inputs = self.feat.output_size() * self.classifier.n_classes()
            self.random_layer = torch.nn.Linear(in_features=nb_inputs, out_features=self.random_dim, bias=False)
            torch.nn.init.normal_(self.random_layer.weight, mean=0, std=1)
            for param in self.random_layer.parameters():
                param.requires_grad = False

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)
        x = x.view(x.size(0), -1)

        class_output = self.classifier(x)

        # The GRL hook is applied to all inputs to the adversary
        reverse_feature = GradReverse.apply(x, self.alpha)

        softmax_output = torch.nn.Softmax(dim=1)(class_output)
        reverse_out = GradReverse.apply(softmax_output, self.alpha)

        feature = torch.bmm(reverse_out.unsqueeze(2), reverse_feature.unsqueeze(1))
        feature = feature.view(-1, reverse_out.size(1) * reverse_feature.size(1))
        if self.random_layer:
            random_out = self.random_layer.forward(feature)
            adversarial_output = self.domain_classifier(random_out.view(-1, random_out.size(1)))
        else:
            adversarial_output = self.domain_classifier(feature)

        return x, class_output, adversarial_output

    def _compute_entropy_weights(self, logits):
        entropy = losses.entropy_logits(logits)
        entropy = GradReverse.apply(entropy, self.alpha)
        entropy_w = 1.0 + torch.exp(-entropy)
        return entropy_w

    def compute_loss(self, batch, split_name="valid"):
        if len(batch) == 3:
            raise NotImplementedError("CDAN does not support semi-supervised setting.")
        (x_s, y_s), (x_tu, y_tu) = batch
        batch_size = len(y_s)

        _, y_hat, d_hat = self.forward(x_s)
        _, y_t_hat, d_t_hat = self.forward(x_tu)

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        if self.entropy:
            e_s = self._compute_entropy_weights(y_hat)
            e_t = self._compute_entropy_weights(y_t_hat)
            source_weight = e_s / torch.sum(e_s)
            target_weight = e_t / torch.sum(e_t)
        else:
            source_weight = None
            target_weight = None

        loss_dmn_src, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size), source_weight)
        loss_dmn_tgt, dok_tgt = losses.cross_entropy_logits(d_t_hat, torch.ones(len(d_t_hat)), target_weight)

        dok = torch.cat((dok_src, dok_tgt)).double().mean()
        dok_src = dok_src.double().mean()
        dok_tgt = dok_tgt.double().mean()

        adv_loss = loss_dmn_src + loss_dmn_tgt
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
        }
        return task_loss, adv_loss, log_metrics


class WDGRLTrainer(BaseDANNLike):
    """
    Implements WDGRL as described in
    Shen, Jian, et al.
    "Wasserstein distance guided representation learning for domain adaptation."
    Thirty-Second AAAI Conference on Artificial Intelligence. 2018.
    https://arxiv.org/pdf/1707.01217.pdf

    This class also implements the asymmetric (:math:`\\beta`) variant described in:
    Wu, Yifan, et al.
    "Domain adaptation with asymmetrically-relaxed distribution alignment."
    ICML (2019)
    https://arxiv.org/pdf/1903.01689.pdf
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        critic,
        k_critic=5,
        gamma=10,
        beta_ratio=0,
        **base_params,
    ):
        """
        parameters:

            k_critic: number of steps to train critic (called n in Algorithm 1 of the paper)
        """
        super().__init__(dataset, feature_extractor, task_classifier, critic, **base_params)
        self._k_critic = k_critic
        self._beta_ratio = beta_ratio
        self._gamma = gamma

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)
        x = x.view(x.size(0), -1)

        class_output = self.classifier(x)
        adversarial_output = self.domain_classifier(x)
        return x, class_output, adversarial_output

    def compute_loss(self, batch, split_name="valid"):
        if len(batch) == 3:
            raise NotImplementedError("WDGRL does not support semi-supervised setting.")
        (x_s, y_s), (x_tu, y_tu) = batch
        batch_size = len(y_s)

        _, y_hat, d_hat = self.forward(x_s)
        _, y_t_hat, d_t_hat = self.forward(x_tu)

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        _, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size))
        _, dok_tgt = losses.cross_entropy_logits(d_t_hat, torch.ones(len(d_t_hat)))

        dok = torch.cat((dok_src, dok_tgt)).double().mean()
        dok_src = dok_src.double().mean()
        dok_tgt = dok_tgt.double().mean()

        wasserstein_distance = d_hat.mean() - (1 + self._beta_ratio) * d_t_hat.mean()
        adv_loss = wasserstein_distance
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
            f"{split_name}_wasserstein_dist": wasserstein_distance,
        }
        return task_loss, adv_loss, log_metrics

    def critic_update_steps(self, batch):
        if self.current_epoch < self._init_epochs:
            return

        set_requires_grad(self.feat, requires_grad=False)
        set_requires_grad(self.domain_classifier, requires_grad=True)

        (x_s, y_s), (x_tu, _) = batch
        with torch.no_grad():
            h_s = self.feat(x_s).data.view(x_s.shape[0], -1)
            h_t = self.feat(x_tu).data.view(x_tu.shape[0], -1)
        for _ in range(self._k_critic):
            gp = losses.gradient_penalty(self.domain_classifier, h_s, h_t)

            critic_s = self.domain_classifier(h_s)
            critic_t = self.domain_classifier(h_t)
            wasserstein_distance = critic_s.mean() - (1 + self._beta_ratio) * critic_t.mean()

            critic_cost = -wasserstein_distance + self._gamma * gp

            self.critic_opt.zero_grad()
            critic_cost.backward()
            self.critic_opt.step()
            if self.critic_sched:
                self.critic_sched.step()

        set_requires_grad(self.feat, requires_grad=True)
        set_requires_grad(self.domain_classifier, requires_grad=False)

    def training_step(self, batch, batch_id):
        self._update_batch_epoch_factors(batch_id)
        self.critic_update_steps(batch)

        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="train")
        if self.current_epoch < self._init_epochs:
            # init phase doesn't use few-shot learning
            # ad-hoc decision but makes models more comparable between each other
            loss = task_loss
        else:
            loss = task_loss + self.lamb_da * adv_loss

        log_metrics["train_total_loss"] = loss
        log_metrics["train_adv_loss"] = adv_loss
        log_metrics["train_task_loss"] = task_loss

        self.log_dict(log_metrics, on_step=True, on_epoch=False)

        # logging alpha and lambda when they exist (they exist for DANN and CDAN but not for DAN and JAN)
        self.log("alpha", self.alpha, on_step=False, on_epoch=True) if hasattr(self, "alpha") else None
        self.log("lambda", self.lamb_da, on_step=False, on_epoch=True) if hasattr(self, "lamb_da") else None

        return loss  # required, for backward pass

    def configure_optimizers(self):
        nets = [self.feat, self.classifier]
        parameters = set()
        for net in nets:
            parameters |= set(net.parameters())

        if self._adapt_lr:
            task_feat_optimizer, task_feat_sched = self._configure_optimizer(parameters)
            self.critic_opt, self.critic_sched = self._configure_optimizer(self.domain_classifier.parameters())
            self.critic_opt = self.critic_opt[0]
            self.critic_sched = self.critic_sched[0]
            return task_feat_optimizer, task_feat_sched
        else:
            task_feat_optimizer = self._configure_optimizer(parameters)
            self.critic_opt = self._configure_optimizer(self.domain_classifier.parameters())
            self.critic_sched = None
            self.critic_opt = self.critic_opt[0]
        return task_feat_optimizer


class WDGRLTrainerMod(WDGRLTrainer):
    """
    Implements a modified version WDGRL as described in
    Shen, Jian, et al.
    "Wasserstein distance guided representation learning for domain adaptation."
    Thirty-Second AAAI Conference on Artificial Intelligence. 2018.
    https://arxiv.org/pdf/1707.01217.pdf

    This class also implements the asymmetric (:math:`\\beta`) variant described in:
    Wu, Yifan, et al.
    "Domain adaptation with asymmetrically-relaxed distribution alignment."
    ICML (2019)
    https://arxiv.org/pdf/1903.01689.pdf
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        critic,
        k_critic=5,
        gamma=10,
        beta_ratio=0,
        **base_params,
    ):
        """
        parameters:

            k_critic: number of steps to train critic (called n in Algorithm 1 of the paper)
        """
        super().__init__(dataset, feature_extractor, task_classifier, critic, **base_params)
        self._k_critic = k_critic
        self._beta_ratio = beta_ratio
        self._gamma = gamma
        self.automatic_optimization = False

    def critic_update_steps(self, batch):
        (x_s, y_s), (x_tu, _) = batch
        with torch.no_grad():
            h_s = self.feat(x_s).data.view(x_s.shape[0], -1)
            h_t = self.feat(x_tu).data.view(x_tu.shape[0], -1)

        gp = losses.gradient_penalty(self.domain_classifier, h_s, h_t)

        critic_s = self.domain_classifier(h_s)
        critic_t = self.domain_classifier(h_t)
        wasserstein_distance = critic_s.mean() - (1 + self._beta_ratio) * critic_t.mean()

        critic_cost = -wasserstein_distance + self._gamma * gp

        log_metrics = {"train_critic_loss": critic_cost}

        return {
            "loss": critic_cost,  # required, for backward pass
            "progress_bar": {"critic loss": critic_cost},
            "log": log_metrics,
        }

    # def training_step(self, batch, batch_id, optimizer_idx):
    def training_step(self, batch, batch_id):
        # optimizer_step is not used in the new version of PyTorch Lightning,
        # so we need to implement staged optimizer in training_step.
        # This may casue the implementation to be a little different from the old version.

        self._update_batch_epoch_factors(batch_id)

        # Retrieve the critic and task optimizers
        critic_opt, task_opt = self.optimizers()

        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="train")
        if self.current_epoch < self._init_epochs:
            # init phase doesn't use few-shot learning
            # ad-hoc decision but makes models more comparable between each other
            loss = task_loss

            # do not update critic
            task_opt.step()
            task_opt.zero_grad()
        else:
            loss = task_loss + self.lamb_da * adv_loss

            critic_opt.step()
            critic_opt.zero_grad()

            # update discriminator opt every k_critic steps
            if (batch_id + 1) % self._k_critic == 0:
                task_opt.step()
                task_opt.zero_grad()

        log_metrics["train_total_loss"] = loss
        log_metrics["train_task_loss"] = task_loss

        self.log_dict(log_metrics, on_step=True, on_epoch=False)

        return loss  # required, for backward pass

    # PyTorch Lightning 2.0+ has a different optimizer_step, so we implement this staged optimization in training_step
    # above. We will retain optimizer_step until we have assessed WDGRLTrainerMod, after which it will be removed.
    # Add on_tpu=False etc following https://github.com/PyTorchLightning/pytorch-lightning/issues/2934
    # to fix error for WDGRLMod: TypeError: optimizer_step() got an unexpected keyword argument 'on_tpu'
    # def optimizer_step(
    #     self,
    #     current_epoch,
    #     batch_nb,
    #     optimizer,
    #     # optimizer_i,
    #     second_order_closure=None,
    #     # on_tpu=False,
    #     # using_native_amp=False,
    #     # using_lbfgs=False,
    # ):
    #     if current_epoch < self._init_epochs:
    #         # do not update critic
    #         if optimizer_i == 0:
    #             pass
    #         if optimizer_i == 1:
    #             optimizer.step()
    #             optimizer.zero_grad()
    #     else:
    #         if optimizer_i == 0:
    #             optimizer.step()
    #             optimizer.zero_grad()
    #
    #         # update discriminator opt every k_critic steps
    #         if optimizer_i == 1:
    #             if (batch_nb + 1) % self._k_critic == 0:
    #                 optimizer.step()
    #             optimizer.zero_grad()
    #
    #     optimizer.step(closure=second_order_closure)

    def configure_optimizers(self):
        nets = [self.feat, self.classifier]
        parameters = set()
        for net in nets:
            parameters |= set(net.parameters())

        optimizer = torch.optim.Adam(parameters, lr=self._init_lr, betas=(0.5, 0.999))

        critic_opt = torch.optim.Adam(self.domain_classifier.parameters(), lr=self._init_lr, betas=(0.5, 0.999))
        return [critic_opt, optimizer], []


class FewShotDANNTrainer(BaseDANNLike):
    """Implements adaptations of DANN to the semi-supervised setting

    naive: task classifier is trained on labeled target data, in addition to source
    data.
    MME: immplements Saito, Kuniaki, et al.
    "Semi-supervised domain adaptation via minimax entropy."
    Proceedings of the IEEE International Conference on Computer Vision. 2019
    https://arxiv.org/pdf/1904.06487.pdf

    """

    def __init__(self, dataset, feature_extractor, task_classifier, critic, method, **base_params):
        super().__init__(dataset, feature_extractor, task_classifier, critic, **base_params)
        self._method = Method(method)

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)
        x = x.view(x.size(0), -1)

        reverse_feature = GradReverse.apply(x, self.alpha)
        class_output = self.classifier(x)
        adversarial_output = self.domain_classifier(reverse_feature)
        return x, class_output, adversarial_output

    def compute_loss(self, batch, split_name="valid"):
        assert len(batch) == 3
        (x_s, y_s), (x_tl, y_tl), (x_tu, y_tu) = batch
        batch_size = len(y_s)

        _, y_hat, d_hat = self.forward(x_s)
        _, y_tl_hat, d_tl_hat = self.forward(x_tl)
        _, y_tu_hat, d_tu_hat = self.forward(x_tu)
        d_target_pred = torch.cat((d_tl_hat, d_tu_hat))

        loss_cls_s, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        loss_cls_tl, ok_tl = losses.cross_entropy_logits(y_tl_hat, y_tl)
        _, ok_tu = losses.cross_entropy_logits(y_tu_hat, y_tu)
        ok_tgt = torch.cat((ok_tl, ok_tu)).double().mean()
        ok_src = ok_src.double().mean()

        if self.current_epoch < self._init_epochs:
            # init phase doesn't use few-shot learning
            # ad-hoc decision but makes models more comparable between each other
            task_loss = loss_cls_s
        else:
            task_loss = (batch_size * loss_cls_s + len(y_tl) * loss_cls_tl) / (batch_size + len(y_tl))

        loss_dmn_src, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size))
        loss_dmn_tgt, dok_tgt = losses.cross_entropy_logits(d_target_pred, torch.ones(len(d_target_pred)))

        if self._method is Method.MME:
            # only keep accuracy, overwrite "domain" loss
            loss_dmn_src = 0
            loss_dmn_tgt = losses.entropy_logits_loss(y_tu_hat)

        adv_loss = loss_dmn_src + loss_dmn_tgt

        dok = torch.cat((dok_src, dok_tgt)).double().mean()
        dok_src = dok_src.double().mean()
        dok_tgt = dok_tgt.double().mean()

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
        }
        return task_loss, adv_loss, log_metrics


class BaseMMDLike(BaseAdaptTrainer):
    """Common API for MME-based deep learning DA methods: DAN, JAN"""

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        kernel_mul=2.0,
        kernel_num=5,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, **base_params)

        self._kernel_mul = kernel_mul
        self._kernel_num = kernel_num

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)
        x = x.view(x.size(0), -1)
        class_output = self.classifier(x)
        return x, class_output

    def _compute_mmd(self, phi_s, phi_t, y_hat, y_t_hat):
        raise NotImplementedError("You need to implement a MMD-loss")

    def compute_loss(self, batch, split_name="valid"):
        if len(batch) == 3:
            raise NotImplementedError("MMD does not support semi-supervised setting.")
        (x_s, y_s), (x_tu, y_tu) = batch

        phi_s, y_hat = self.forward(x_s)
        phi_t, y_t_hat = self.forward(x_tu)

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        mmd = self._compute_mmd(phi_s, phi_t, y_hat, y_t_hat)
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": mmd,
        }
        return task_loss, mmd, log_metrics


class DANTrainer(BaseMMDLike):
    """
    This is an implementation of DAN
    Long, Mingsheng, et al.
    "Learning Transferable Features with Deep Adaptation Networks."
    International Conference on Machine Learning. 2015.
    http://proceedings.mlr.press/v37/long15.pdf
    code based on https://github.com/thuml/Xlearn.
    """

    def __init__(self, dataset, feature_extractor, task_classifier, **base_params):
        super().__init__(dataset, feature_extractor, task_classifier, **base_params)

    def _compute_mmd(self, phi_s, phi_t, y_hat, y_t_hat):
        batch_size = int(phi_s.size()[0])
        kernels = losses.gaussian_kernel(
            phi_s,
            phi_t,
            kernel_mul=self._kernel_mul,
            kernel_num=self._kernel_num,
        )
        return losses.compute_mmd_loss(kernels, batch_size)


class JANTrainer(BaseMMDLike):
    """
    This is an implementation of JAN
    Long, Mingsheng, et al.
    "Deep transfer learning with joint adaptation networks."
    International Conference on Machine Learning, 2017.
    https://arxiv.org/pdf/1605.06636.pdf
    code based on https://github.com/thuml/Xlearn.
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        kernel_mul=(2.0, 2.0),
        kernel_num=(5, 1),
        **base_params,
    ):
        super().__init__(
            dataset,
            feature_extractor,
            task_classifier,
            kernel_mul=kernel_mul,
            kernel_num=kernel_num,
            **base_params,
        )

    def _compute_mmd(self, phi_s, phi_t, y_hat, y_t_hat):
        softmax_layer = torch.nn.Softmax(dim=-1)
        source_list = [phi_s, softmax_layer(y_hat)]
        target_list = [phi_t, softmax_layer(y_t_hat)]
        batch_size = int(phi_s.size()[0])

        joint_kernels = None
        for source, target, k_mul, k_num, sigma in zip(
            source_list, target_list, self._kernel_mul, self._kernel_num, [None, 1.68]
        ):
            kernels = losses.gaussian_kernel(source, target, kernel_mul=k_mul, kernel_num=k_num, fix_sigma=sigma)
            if joint_kernels is not None:
                joint_kernels = joint_kernels * kernels
            else:
                joint_kernels = kernels

        return losses.compute_mmd_loss(joint_kernels, batch_size)
</file>

<file path="kale/pipeline/fewshot_trainer.py">
# ==============================================================================
# Author: Wenrui Fan, winslow.fan@outlook.com
# ==============================================================================

"""
This module contains the ProtoNet trainer class and related functions. It trains a prototypical network model for few-shot learning problems under :math:`N`-way-:math:`K`-shot settings.

ProtoNet is a few-shot learning method that can be considered a clustering method.
It learns a feature space where samples from the same class are close to each other and samples from different classes are far apart.
The prototypes can be seen as the cluster centers, and the feature space is learned to make the samples cluster around these prototypes.
But note that ProtoNet operates in a supervised learning context, where the goal is to classify data points based on labeled training examples.
Clustering is typically an unsupervised learning task, where the objective is to group data points into clusters without prior knowledge of labels.

This is a ``PyTorch Lightning <https://github.com/Lightning-AI/lightning>`` version of the original implementation <https://github.com/jakesnell/prototypical-networks> of Prototypical Networks for Few-shot Learning <https://arxiv.org/abs/1703.05175>.
"""

from typing import Any

import pytorch_lightning as pl
import torch

from kale.evaluate.metrics import protonet_loss


class ProtoNetTrainer(pl.LightningModule):
    """ProtoNet trainer class.

    This class trains a ProtoNet model for few-shot learning problems under :math:`N`-way-:math:`K`-shot settings.
    It uses ``pl.LightningModule`` class of ``PyTorch Lightning`` to standardize the workflow.
    Updating other modules except ``kale.evaluate.metrics.protonet_loss`` and ``kale.embed.image_cnn`` will not affect this trainer.

    - :math:`N`-way: The number of classes under a particular setting. The model is presented with samples from these :math:`N` classes and needs to classify them. For example, 3-way means the model has to classify 3 different classes.

    - :math:`K`-shot: The number of samples for each class in the support set. For example, in a 2-shot setting, two support samples are provided per class.

    - Support set: It is a small, labeled dataset used to train the model with a few samples of each class. The support set consists of :math:`N` classes (:math:`N`-way), with :math:`K` samples (:math:`K`-shot) for each class. For example, under a 3-way-2-shot setting, the support set has 3 classes with 2 samples per class, totaling 6 samples.

    - Query set: It evaluates the model's ability to generalize what it has learned from the support set. It contains samples from the same :math:`N` classes but not included in the support set. Continuing with the 3-way-2-shot example, the query set would include additional samples from the 3 classes, which the model must classify after learning from the support set.

    Args:
        net (torch.nn.Module): A feature extractor without any task-specific heads. It outputs a 1-D feature vector.
        train_num_classes (int): Number of classes in training. It could be different from :math:`N` under :math:`N`-way-:math:`K`-shot settings in ProtoNet. Default: 30.
        train_num_support_samples (int): Number of samples per class in the support set in training. It corresponds to :math:`K` under :math:`N`-way-:math:`K`-shot settings. Default: 5.
        train_num_query_samples (int): Number of samples per class in the query set in training. Default: 15.
        val_num_classes (int): Number of classes in validation and testing. It corresponds to :math:`N` under :math:`N`-way-:math:`K`-shot settings. Default: 5.
        val_num_support_samples (int): Number of samples per class in the support set in validation and testing. It corresponds to :math:`K` under :math:`N`-way-:math:`K`-shot settings. Default: 5.
        val_num_query_samples (int): Number of samples per class in the query set in validation and testing. Default: 15.
        devices (str): Devices used for training. Default: "cuda".
        optimizer (str): Optimizer used for training. Default: "SGD".
        lr (float): Learning rate. Default: 0.001.
    """

    def __init__(
        self,
        net: torch.nn.Module,
        train_num_classes: int = 30,
        train_num_support_samples: int = 5,
        train_num_query_samples: int = 15,
        val_num_classes: int = 5,
        val_num_support_samples: int = 5,
        val_num_query_samples: int = 15,
        devices: str = "cuda",
        optimizer: str = "SGD",
        lr: float = 0.001,
    ) -> None:
        super().__init__()

        self.train_num_classes = train_num_classes
        self.train_num_support_samples = train_num_support_samples
        self.train_num_query_samples = train_num_query_samples
        self.val_num_classes = val_num_classes
        self.val_num_support_samples = val_num_support_samples
        self.val_num_query_samples = val_num_query_samples
        self.devices = devices
        self.optimizer = optimizer
        self.lr = lr

        # model
        self.model = net

        # loss
        self.loss_train = protonet_loss(
            num_classes=train_num_classes, num_query_samples=train_num_query_samples, device=self.devices
        )
        self.loss_val = protonet_loss(
            num_classes=val_num_classes, num_query_samples=val_num_query_samples, device=self.devices
        )

    def forward(self, x, num_support_samples, num_classes) -> torch.Tensor:
        x = x.to(self.devices)
        supports = x[0][0:num_support_samples]
        queries = x[0][num_support_samples:]
        for image in x[1:]:
            supports = torch.cat((supports, image[0:num_support_samples]), dim=0)
            queries = torch.cat((queries, image[num_support_samples:]), dim=0)
        feature_support = self.model(supports).reshape(num_classes, num_support_samples, -1)
        feature_query = self.model(queries)
        return feature_support, feature_query

    def compute_loss(self, feature_support, feature_query, mode="train") -> tuple:
        """Compute loss and accuracy.

        Here we use the same loss function for both training and validation, which is related to Euclidean distance.

        Args:
            feature_support (torch.Tensor): Support features.
            feature_query (torch.Tensor): Query features.
            mode (str): Mode of the trainer, "train", "val" or "test". Default: "train".

        Returns:
            loss (torch.Tensor): Loss value.
            return_dict (dict): Dictionary of loss and accuracy.
        """
        loss, acc = eval(f"self.loss_{mode}")(feature_support, feature_query)
        return_dict = {"{}_loss".format(mode): loss.item(), "{}_acc".format(mode): acc}
        return loss, return_dict

    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:
        """Training step.

        Compute loss and accuracy, and log them by ``self.log_dict``. For training, log on each step and each
         epoch. For validation and testing, only log on each epoch. This way can avoid using ``on_training_epoch_end()``
         and ``on_validation_epoch_end()``.
        """
        images, _ = batch
        feature_support, feature_query = self.forward(images, self.train_num_support_samples, self.train_num_classes)
        loss, log_metrics = self.compute_loss(feature_support, feature_query, mode="train")
        self.log_dict(log_metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch: Any, batch_idx: int) -> None:
        """Compute and return the validation loss and log_metrics on one step."""
        images, _ = batch
        feature_support, feature_query = self.forward(images, self.val_num_support_samples, self.val_num_classes)
        _, log_metrics = self.compute_loss(feature_support, feature_query, mode="val")
        self.log_dict(log_metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    def test_step(self, batch: Any, batch_idx: int) -> None:
        """Compute and return the test loss and log_metrics on one step."""
        images, _ = batch
        feature_support, feature_query = self.forward(images, self.val_num_support_samples, self.val_num_classes)
        _, log_metrics = self.compute_loss(feature_support, feature_query, mode="val")
        self.log_dict(log_metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    def configure_optimizers(self) -> torch.optim.Optimizer:
        """
        Configure optimizer for training. Can be modified to support different optimizers from ``torch.optim``.
        """
        optimizer = eval(f"torch.optim.{self.optimizer}")(self.model.parameters(), lr=self.lr)
        return optimizer
</file>

<file path="kale/pipeline/mpca_trainer.py">
# =============================================================================
# Author: Shuo Zhou, shuo.zhou@sheffield.ac.uk
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""Implementation of MPCA->Feature Selection->Linear SVM/LogisticRegression Pipeline

References:
    [1] Swift, A. J., Lu, H., Uthoff, J., Garg, P., Cogliano, M., Taylor, J., ... & Kiely, D. G. (2020). A machine
    learning cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial
    hypertension diagnosis. European Heart Journal-Cardiovascular Imaging.
    [2] Song, X., Meng, L., Shi, Q., & Lu, H. (2015, October). Learning tensor-based features for whole-brain fMRI
    classification. In International Conference on Medical Image Computing and Computer-Assisted Intervention
    (pp. 613-620). Springer, Cham.
    [3] Lu, H., Plataniotis, K. N., & Venetsanopoulos, A. N. (2008). MPCA: Multilinear principal component analysis of
    tensor objects. IEEE Transactions on Neural Networks, 19(1), 18-39.
"""

import logging

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.feature_selection import f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC, SVC
from sklearn.utils.validation import check_is_fitted

from ..embed.factorization import MPCA

param_c_grids = list(np.logspace(-4, 2, 7))
classifiers = {
    "svc": [SVC, {"kernel": ["linear"], "C": param_c_grids, "max_iter": [50000]}],
    "linear_svc": [LinearSVC, {"C": param_c_grids}],
    "lr": [LogisticRegression, {"C": param_c_grids}],
}

# k-fold cross validation used for grid search, i.e. searching for optimal value of C
default_search_params = {"cv": 5}
default_mpca_params = {"var_ratio": 0.97, "vectorize": True}


class MPCATrainer(BaseEstimator, ClassifierMixin):
    """Trainer of pipeline: MPCA->Feature selection->Classifier

    Args:
        classifier (str, optional): Available classifier options: {"svc", "linear_svc", "lr"}, where "svc" trains a
            support vector classifier, supports both linear and non-linear kernels, optimizes with library "libsvm";
            "linear_svc" trains a support vector classifier with linear kernel only, and optimizes with library
            "liblinear", which suppose to be faster and better in handling large number of samples; and "lr" trains
            a classifier with logistic regression. Defaults to "svc".
        classifier_params (dict, optional): Parameters of classifier. Defaults to 'auto'.
        classifier_param_grid (dict, optional): Grids for searching the optimal hyper-parameters. Works only when
            classifier_params == "auto". Defaults to None by searching from the following hyper-parameter values:
            1. svc, {"kernel": ["linear"], "C": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], "max_iter": [50000]},
            2. linear_svc, {"C": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},
            3. lr, {"C": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}
        mpca_params (dict, optional): Parameters of MPCA, e.g., {"var_ratio": 0.8}. Defaults to None, i.e., using the
            default parameters (https://pykale.readthedocs.io/en/latest/kale.embed.html#module-kale.embed.mpca).
        n_features (int, optional): Number of features for feature selection. Defaults to None, i.e., all features
            after dimension reduction will be used.
        search_params (dict, optional): Parameters of grid search, for more detail please see
            https://scikit-learn.org/stable/modules/grid_search.html#grid-search . Defaults to None, i.e., using the
            default params: {"cv": 5}.
    """

    def __init__(
        self,
        classifier="svc",
        classifier_params="auto",
        classifier_param_grid=None,
        mpca_params=None,
        n_features=None,
        search_params=None,
    ):
        if classifier not in ["svc", "linear_svc", "lr"]:
            error_msg = "Valid classifier should be 'svc', 'linear_svc', or 'lr', but given %s" % classifier
            logging.error(error_msg)
            raise ValueError(error_msg)

        self.classifier = classifier
        # init mpca object
        if mpca_params is None:
            self.mpca_params = default_mpca_params
        else:
            self.mpca_params = mpca_params
        self.mpca = MPCA(**self.mpca_params)
        # init feature selection parameters
        self.n_features = n_features
        self.feature_order = None
        # init classifier object
        if search_params is None:
            self.search_params = default_search_params
        else:
            self.search_params = search_params
        self.classifier_param_grid = classifier_param_grid

        self.auto_classifier_param = False
        if classifier_params == "auto":
            self.auto_classifier_param = True
            if self.classifier_param_grid is None:
                self.classifier_param_grid = classifiers[classifier][1]
            self.grid_search = GridSearchCV(
                classifiers[classifier][0](), param_grid=self.classifier_param_grid, **self.search_params
            )
            self.clf = None
        elif isinstance(classifier_params, dict):
            self.clf = classifiers[classifier][0](**classifier_params)
        else:
            error_msg = "Invalid classifier parameter type"
            logging.error(error_msg)
            raise ValueError(error_msg)

        self.classifier_params = classifier_params

    def fit(self, x, y):
        """Fit a pipeline with the given data x and labels y

        Args:
            x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)
            y (array-like): data labels, shape (n_samples, )

        Returns:
            self
        """
        # fit mpca
        self.mpca.fit(x)
        self.mpca.set_params(**{"vectorize": True})
        x_transformed = self.mpca.transform(x)

        # feature selection
        if self.n_features is None:
            self.n_features = x_transformed.shape[1]
            self.feature_order = self.mpca.idx_order
        else:
            f_score, p_val = f_classif(x_transformed, y)
            self.feature_order = (-1 * f_score).argsort()
        x_transformed = x_transformed[:, self.feature_order][:, : self.n_features]

        # fit classifier
        if self.auto_classifier_param:
            self.grid_search.param_grid["C"].append(1 / x.shape[0])
            self.grid_search.fit(x_transformed, y)
            self.clf = self.grid_search.best_estimator_
        if self.classifier == "svc":
            self.clf.set_params(**{"probability": True})

        self.clf.fit(x_transformed, y)

    def predict(self, x):
        """Predict the labels for the given data x

        Args:
            x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)

        Returns:
            array-like: Predicted labels, shape (n_samples, )
        """
        return self.clf.predict(self._extract_feature(x))

    def decision_function(self, x):
        """Decision scores of each class for the given data x

        Args:
            x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)

        Returns:
            array-like: decision scores, shape (n_samples,) for binary case, else (n_samples, n_class)
        """
        return self.clf.decision_function(self._extract_feature(x))

    def predict_proba(self, x):
        """Probability of each class for the given data x. Not supported by "linear_svc".

        Args:
            x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)

        Returns:
            array-like: probabilities, shape (n_samples, n_class)
        """
        if self.classifier == "linear_svc":
            error_msg = "Linear SVC does not support computing probability."
            logging.error(error_msg)
            raise ValueError(error_msg)
        return self.clf.predict_proba(self._extract_feature(x))

    def _extract_feature(self, x):
        """Extracting features for the given data x with MPCA->Feature selection

        Args:
            x (array-like tensor): input data, shape (n_samples, I_1, I_2, ..., I_N)

        Returns:
            array-like: n_new, shape (n_samples, n_features)
        """
        check_is_fitted(self.clf)
        x_transformed = self.mpca.transform(x)

        return x_transformed[:, self.feature_order][:, : self.n_features]
</file>

<file path="kale/pipeline/multi_domain_adapter.py">
# =============================================================================
# Author: Shuo Zhou, shuo.zhou@sheffield.ac.uk/sz144@outlook.com
# =============================================================================
"""Multi-source domain adaptation pipelines
"""

import torch
import torch.nn as nn
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.preprocessing import LabelBinarizer
from torch.linalg import multi_dot
from torch.nn.functional import one_hot

import kale.evaluate.metrics as losses
from kale.embed.image_cnn import _Bottleneck
from kale.pipeline.domain_adapter import BaseAdaptTrainer


def create_ms_adapt_trainer(method: str, dataset, feature_extractor, task_classifier, **train_params):
    """Methods for multi-source domain adaptation

    Args:
        method (str): Multi-source domain adaptation method, M3SDA or MFSAN
        dataset (kale.loaddata.multi_domain.MultiDomainAdapDataset): the multi-domain datasets to be used for train,
            validation, and tests.
        feature_extractor (torch.nn.Module): feature extractor network
        task_classifier (torch.nn.Module): task classifier network

    Returns:
        [pl.LightningModule]: Multi-source domain adaptation trainer.
    """
    method_dict = {"M3SDA": M3SDATrainer, "DIN": _DINTrainer, "MFSAN": MFSANTrainer}
    method = method.upper()
    if method not in method_dict.keys():
        raise ValueError("Unsupported multi-source domain adaptation methods %s" % method)
    else:
        return method_dict[method](dataset, feature_extractor, task_classifier, **train_params)


def _average_cls_output(x, classifiers: nn.ModuleDict):
    cls_output = [classifiers[key](x) for key in classifiers]

    return torch.stack(cls_output).mean(0)


class BaseMultiSourceTrainer(BaseAdaptTrainer):
    """Base class for all domain adaptation architectures

    Args:
        dataset (kale.loaddata.multi_domain): the multi-domain datasets to be used for train, validation, and tests.
        feature_extractor (torch.nn.Module): the feature extractor network
        task_classifier (torch.nn.Module): the task classifier network
        n_classes (int): number of classes
        target_domain (str): target domain name
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        n_classes: int,
        target_domain: str,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, **base_params)
        self.n_classes = n_classes
        self.feature_dim = feature_extractor.state_dict()[list(feature_extractor.state_dict().keys())[-2]].shape[0]
        self.domain_to_idx = dataset.domain_to_idx
        if target_domain not in self.domain_to_idx.keys():
            raise ValueError(
                "The given target domain %s not in the dataset! The available domain names are %s"
                % (target_domain, self.domain_to_idx.keys())
            )
        self.target_domain = target_domain
        self.target_label = self.domain_to_idx[target_domain]
        self.base_params = base_params

    def forward(self, x):
        if self.feat is not None:
            x = self.feat(x)

        return x

    def compute_loss(self, batch, split_name="valid"):
        raise NotImplementedError("Loss needs to be defined.")


class M3SDATrainer(BaseMultiSourceTrainer):
    """Moment matching for multi-source domain adaptation (M3SDA).

    Reference:
        Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., & Wang, B. (2019). Moment matching for multi-source
        domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision
        (pp. 1406-1415).
        https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.html
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        n_classes: int,
        target_domain: str,
        k_moment: int = 3,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, n_classes, target_domain, **base_params)
        self.classifiers = dict()
        for domain_ in self.domain_to_idx.keys():
            if domain_ != target_domain:
                self.classifiers[domain_] = task_classifier(self.feature_dim, n_classes)
        # init classifiers as nn.ModuleDict, otherwise it will not be optimized
        self.classifiers = nn.ModuleDict(self.classifiers)
        self.k_moment = k_moment

    def compute_loss(self, batch, split_name="valid"):
        x, y, domain_labels = batch
        phi_x = self.forward(x)
        moment_loss = self._compute_domain_dist(phi_x, domain_labels)
        src_idx = torch.where(domain_labels != self.target_label)[0]
        tgt_idx = torch.where(domain_labels == self.target_label)[0]
        cls_loss, ok_src = self._compute_cls_loss(phi_x[src_idx], y[src_idx], domain_labels[src_idx])
        if len(tgt_idx) > 0:
            y_tgt_hat = _average_cls_output(phi_x[tgt_idx], self.classifiers)
            _, ok_tgt = losses.cross_entropy_logits(y_tgt_hat, y[tgt_idx])
        else:
            ok_tgt = 0.0

        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()
        task_loss = cls_loss

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": moment_loss,
        }

        return task_loss, moment_loss, log_metrics

    def _compute_cls_loss(self, x, y, domain_labels: torch.Tensor):
        if len(y) == 0:
            return 0.0, 0.0
        else:
            cls_loss = 0.0
            ok_src = []
            n_src = 0
            for domain_ in self.domain_to_idx.keys():
                if domain_ == self.target_domain:
                    continue
                domain_idx = torch.where(domain_labels == self.domain_to_idx[domain_])[0]
                cls_output = self.classifiers[domain_](x[domain_idx])
                loss_cls_, ok_src_ = losses.cross_entropy_logits(cls_output, y[domain_idx])
                cls_loss += loss_cls_
                ok_src.append(ok_src_)
                n_src += 1
            cls_loss = cls_loss / n_src
            ok_src = torch.cat(ok_src)
            return cls_loss, ok_src

    def _compute_domain_dist(self, x, domain_labels):
        """Compute the k-th order moment divergence

        Args:
            x (torch.Tensor): input data, shape (n_samples, n_features)
            domain_labels (torch.Tensor): labels indicating which domain the instance is from, shape (n_samples,)

        Returns:
            torch.Tensor: divergence
        """

        moment_loss = 0
        for i in range(self.k_moment):
            moment_loss += losses._moment_k(x, domain_labels, i + 1)

        return moment_loss


class _DINTrainer(BaseMultiSourceTrainer):
    """Domain independent network (DIN). It is under development and will be updated with references later."""

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        n_classes: int,
        target_domain: str,
        kernel: str = "linear",
        kernel_mul: float = 2.0,
        kernel_num: int = 5,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, n_classes, target_domain, **base_params)
        self.kernel = kernel
        self.n_domains = len(self.domain_to_idx.values())
        self.classifier = task_classifier(self.feature_dim, n_classes)
        self._kernel_mul = kernel_mul
        self._kernel_num = kernel_num

    def compute_loss(self, batch, split_name="valid"):
        x, y, domain_labels = batch
        phi_x = self.forward(x)
        loss_dist = self._compute_domain_dist(phi_x, domain_labels)
        src_idx = torch.where(domain_labels != self.target_label)[0]
        tgt_idx = torch.where(domain_labels == self.target_label)[0]
        cls_output = self.classifier(phi_x)
        loss_cls, ok_src = losses.cross_entropy_logits(cls_output[src_idx], y[src_idx])
        _, ok_tgt = losses.cross_entropy_logits(cls_output[tgt_idx], y[tgt_idx])

        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": loss_dist,
        }

        return task_loss, loss_dist, log_metrics

    def _compute_domain_dist(self, x, domain_labels):
        if self.kernel == "linear":
            kx = torch.mm(x, x.T)
        else:
            raise ValueError("Other kernels have not been implemented yet!")
        domain_label_mat = one_hot(domain_labels, num_classes=self.n_domains)
        domain_label_mat = domain_label_mat.float()
        ky = torch.mm(domain_label_mat, domain_label_mat.T)
        return losses.hsic(kx, ky, device=self.device)


class MFSANTrainer(BaseMultiSourceTrainer):
    """Multiple Feature Spaces Adaptation Network (MFSAN)

    Reference: Zhu, Y., Zhuang, F. and Wang, D., 2019, July. Aligning domain-specific distribution and classifier
        for cross-domain classification from multiple sources. In AAAI.
        https://ojs.aaai.org/index.php/AAAI/article/view/4551

    Original implementation: https://github.com/easezyc/deep-transfer-learning/tree/master/MUDA/MFSAN
    """

    def __init__(
        self,
        dataset,
        feature_extractor,
        task_classifier,
        n_classes: int,
        target_domain: str,
        domain_feat_dim: int = 100,
        kernel_mul: float = 2.0,
        kernel_num: int = 5,
        input_dimension: int = 2,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, n_classes, target_domain, **base_params)
        self.classifiers = dict()
        self.domain_net = dict()
        self.src_domains = []
        for domain_ in dataset.domain_to_idx.keys():
            if domain_ != self.target_domain:
                self.domain_net[domain_] = _Bottleneck(
                    self.feature_dim, domain_feat_dim, input_dimension=input_dimension
                )
                self.classifiers[domain_] = task_classifier(domain_feat_dim, n_classes)
                self.src_domains.append(domain_)
        self.classifiers = nn.ModuleDict(self.classifiers)
        self.domain_net = nn.ModuleDict(self.domain_net)
        self._kernel_mul = kernel_mul
        self._kernel_num = kernel_num

    def compute_loss(self, batch, split_name="valid"):
        x, y, domain_labels = batch
        phi_x = self.forward(x)
        tgt_idx = torch.where(domain_labels == self.target_label)[0]
        n_src = len(self.src_domains)
        domain_dist = 0
        loss_cls = 0
        ok_src = []
        for src_domain in self.src_domains:
            src_domain_idx = torch.where(domain_labels == self.domain_to_idx[src_domain])[0]
            phi_src = self.domain_net[src_domain].forward(phi_x[src_domain_idx])
            phi_tgt = self.domain_net[src_domain].forward(phi_x[tgt_idx])
            kernels = losses.gaussian_kernel(
                phi_src,
                phi_tgt,
                kernel_mul=self._kernel_mul,
                kernel_num=self._kernel_num,
            )
            domain_dist += losses.compute_mmd_loss(kernels, len(phi_src))
            y_src_hat = self.classifiers[src_domain](phi_src)
            loss_cls_, ok_src_ = losses.cross_entropy_logits(y_src_hat, y[src_domain_idx])
            loss_cls += loss_cls_
            ok_src.append(ok_src_)

        domain_dist += self.cls_discrepancy(phi_x[tgt_idx])
        loss_cls = loss_cls / n_src
        ok_src = torch.cat(ok_src)

        y_tgt_hat = self._get_avg_cls_output(phi_x[tgt_idx])
        _, ok_tgt = losses.cross_entropy_logits(y_tgt_hat, y[tgt_idx])

        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": domain_dist,
        }

        return task_loss, domain_dist, log_metrics

    def _get_cls_output(self, x):
        return [self.classifiers[key](self.domain_net[key](x)) for key in self.classifiers]

    def _get_avg_cls_output(self, x):
        cls_output = self._get_cls_output(x)

        return torch.stack(cls_output).mean(0)

    def cls_discrepancy(self, x):
        """Compute discrepancy between all classifiers' probabilistic outputs"""
        cls_output = self._get_cls_output(x)
        n_domains = len(cls_output)
        cls_disc = 0
        for i in range(n_domains - 1):
            for j in range(i + 1, n_domains):
                cls_disc_ = nn.functional.softmax(cls_output[i], dim=1) - nn.functional.softmax(cls_output[j], dim=1)
                cls_disc += torch.mean(torch.abs(cls_disc_))

        return cls_disc * 2 / (n_domains * (n_domains - 1))


class CoIRLS(BaseEstimator, ClassifierMixin):
    """Covariate-Independence Regularized Least Squares (CoIRLS)

    Args:
        kernel (str, optional): {"linear", "rbf", "poly"}. Kernel to use. Defaults to "linear".
        kernel_kwargs (dict or None, optional): Hyperparameter for the kernel. Defaults to None.
        alpha (float, optional): Hyperparameter of the l2 (Ridge) penalty. Defaults to 1.0.
        lambda_ (float, optional): Hyperparameter of the covariate dependence.  Defaults to 1.0.

    Reference:
        [1] Zhou, S., 2022. Interpretable Domain-Aware Learning for Neuroimage Classification (Doctoral dissertation,
            University of Sheffield).
        [2] Zhou, S., Li, W., Cox, C.R., & Lu, H. (2020). Side Information Dependence as a Regularizer for Analyzing
            Human Brain Conditions across Cognitive Experiments. AAAI 2020, New York, USA.
    """

    def __init__(self, kernel="linear", kernel_kwargs=None, alpha=1.0, lambda_=1.0):
        super().__init__()
        self.kernel = kernel
        self.model = None
        self.alpha = alpha
        self.lambda_ = lambda_
        if kernel_kwargs is None:
            self.kernel_kwargs = dict()
        else:
            self.kernel_kwargs = kernel_kwargs
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.losses = {"ovr": [], "pred": [], "code": [], "reg": []}
        self.x = None
        self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)
        self.coef_ = None

    def fit(self, x, y, covariates):
        """fit a model with input data, labels and covariates

        Args:
            x (np.ndarray or tensor): shape (n_samples, n_features)
            y (np.ndarray or tensor): shape (n_samples, )
            covariates (np.ndarray or tensor): (n_samples, n_covariates)
        """
        self._label_binarizer.fit(y)
        x = torch.as_tensor(x)
        x = torch.cat([x, torch.ones(x.shape[0], 1)], 1)
        y = torch.as_tensor(y)
        krnl_x = torch.as_tensor(
            pairwise_kernels(x.detach().numpy(), metric=self.kernel, filter_params=True, **self.kernel_kwargs),
            dtype=torch.float,
        )
        n_samples = x.shape[0]
        n_classes = torch.unique(y).shape[0]
        n_labeled = y.shape[0]
        unit_mat = torch.eye(n_samples)
        ctr_mat = unit_mat - 1.0 / n_samples * torch.ones((n_samples, n_samples))
        mat_j = torch.zeros((n_samples, n_samples))
        mat_j[:n_labeled, :n_labeled] = torch.eye(n_labeled)

        covariates = torch.as_tensor(covariates, dtype=torch.float)
        krnl_cov = torch.mm(covariates, covariates.T)

        if n_classes == 2:
            mat_y = torch.zeros((n_samples, 1))
        else:
            mat_y = torch.zeros((n_samples, n_classes))
        mat_y[:n_labeled, :] = torch.as_tensor(self._label_binarizer.fit_transform(y))
        mat_y = torch.as_tensor(mat_y)

        mat_q = torch.mm(mat_j, krnl_x) + self.alpha * unit_mat
        mat_q += self.lambda_ * multi_dot((ctr_mat, krnl_cov, ctr_mat, krnl_x))

        self.coef_ = torch.linalg.solve(mat_q, mat_y)

        self.x = x

    def predict(self, x):
        """Predict labels for data x

        Args:
            x (np.ndarray or tensor): Samples need prediction, shape (n_samples, n_features)

        Returns:
            y (np.ndarray): Predicted labels, shape (n_samples, )
        """
        out = self.decision_function(x)
        if self._label_binarizer.y_type_ == "binary":
            pred = self._label_binarizer.inverse_transform(torch.sign(out).view(-1))
        else:
            pred = self._label_binarizer.inverse_transform(out)

        return pred

    def decision_function(self, x):
        """Compute decision scores for data x

        Args:
            x (np.ndarray or tensor): Samples need decision scores, shape (n_samples, n_features)

        Returns:
            scores (np.ndarray): Decision scores, shape (n_samples, )
        """
        x = torch.as_tensor(x)
        x = torch.cat([x, torch.ones(x.shape[0], 1)], dim=1)
        krnl_x = torch.as_tensor(
            pairwise_kernels(
                x.detach().numpy(),
                self.x.detach().numpy(),
                metric=self.kernel,
                filter_params=True,
                **self.kernel_kwargs,
            ),
            dtype=torch.float,
        )

        return torch.mm(krnl_x, self.coef_)
</file>

<file path="kale/pipeline/multiomics_trainer.py">
# =============================================================================
# Author: Sina Tabakhi, sina.tabakhi@gmail.com
# =============================================================================

"""
Construct a pipeline to run the MOGONET method based on PyTorch Lightning. MOGONET is a multiomics fusion framework for
cancer classification and biomarker identification that utilizes supervised graph convolutional networks for omics
datasets.

This code is written by refactoring the MOGONET code (https://github.com/txWang/MOGONET/blob/main/train_test.py)
within the PyTorch Lightning.

Reference:
Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021). MOGONET integrates multi-omics data
using graph convolutional networks allowing patient classification and biomarker identification. Nature communications.
https://www.nature.com/articles/s41467-021-23774-w
"""

from typing import List, Optional, Union

import pytorch_lightning as pl
import torch
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from torch import Tensor
from torch.nn import CrossEntropyLoss, ModuleList
from torch.optim.optimizer import Optimizer
from torch_geometric.loader import DataLoader
from torch_sparse import SparseTensor

from kale.embed.mogonet import MogonetGCN
from kale.loaddata.multiomics_datasets import SparseMultiomicsDataset
from kale.predict.decode import LinearClassifier, VCDN


class MultiomicsTrainer(pl.LightningModule):
    r"""The PyTorch Lightning implementation of the MOGONET method, a multiomics fusion method designed for
    classification tasks.

    Args:
        dataset (SparseMultiomicsDataset): The input dataset created in form of :class:`~torch_geometric.data.Dataset`.
        num_modalities (int): The total number of modalities in the dataset.
        num_classes (int): The total number of classes in the dataset.
        unimodal_encoder (List[MogonetGCN]): The list of GCN encoders for each modality.
        unimodal_decoder (List[LinearClassifier]): The list of linear classifier decoders for each modality.
        loss_fn (CrossEntropyLoss): The loss function used to gauge the error between the prediction outputs and the
            provided target values.
        multimodal_decoder (VCDN, optional): The VCDN decoder used in the multiomics dataset.
            (default: ``None``)
        train_multimodal_decoder (bool, optional): Whether to train VCDN module. (default: ``True``)
        gcn_lr (float, optional): The learning rate used in the GCN module. (default: 5e-4)
        vcdn_lr (float, optional): The learning rate used in the VCDN module. (default: 1e-3)
    """

    def __init__(
        self,
        dataset: SparseMultiomicsDataset,
        num_modalities: int,
        num_classes: int,
        unimodal_encoder: List[MogonetGCN],
        unimodal_decoder: List[LinearClassifier],
        loss_fn: CrossEntropyLoss,
        multimodal_decoder: Optional[VCDN] = None,
        train_multimodal_decoder: bool = True,
        gcn_lr: float = 5e-4,
        vcdn_lr: float = 1e-3,
    ) -> None:
        super().__init__()
        self.dataset = dataset
        self.num_modalities = num_modalities
        self.num_classes = num_classes
        self.unimodal_encoder = ModuleList(unimodal_encoder)
        self.unimodal_decoder = ModuleList(unimodal_decoder)
        self.multimodal_decoder = multimodal_decoder
        self.train_multimodal_decoder = train_multimodal_decoder
        self.loss_fn = loss_fn
        self.gcn_lr = gcn_lr
        self.vcdn_lr = vcdn_lr

        # activate manual optimization
        self.automatic_optimization = False

    def configure_optimizers(self) -> Union[Optimizer, List[Optimizer]]:
        """Return the optimizers used during training."""
        optimizers = []

        for modality in range(self.num_modalities):
            optimizers.append(
                torch.optim.Adam(
                    list(self.unimodal_encoder[modality].parameters())
                    + list(self.unimodal_decoder[modality].parameters()),
                    lr=self.gcn_lr,
                )
            )

        if self.multimodal_decoder is not None:
            optimizers.append(torch.optim.Adam(self.multimodal_decoder.parameters(), lr=self.vcdn_lr))

        return optimizers

    def forward(
        self, x: List[Tensor], adj_t: List[SparseTensor], multimodal: bool = False
    ) -> Union[Tensor, List[Tensor]]:
        """Same as :meth:`torch.nn.Module.forward()`.

        Raises:
            TypeError: If `multimodal_decoder` is `None` for multiomics datasets.
        """
        output = []

        for modality in range(self.num_modalities):
            output.append(
                self.unimodal_decoder[modality](self.unimodal_encoder[modality](x[modality], adj_t[modality]))
            )

        if not multimodal:
            return output

        if self.multimodal_decoder is not None:
            return self.multimodal_decoder(output)

        raise TypeError("multimodal_decoder must be defined for multiomics datasets.")

    def training_step(self, train_batch, batch_idx: int):
        """Compute and return the training loss.

        Args:
            train_batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):
                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.
            batch_idx (``int``): Integer displaying index of this batch.
        """
        optimizer = self.optimizers()

        x = []
        adj_t = []
        y = []
        sample_weight = []
        for modality in range(self.num_modalities):
            data = train_batch[modality]
            x.append(data.x[data.train_idx])
            adj_t.append(data.adj_t_train)
            y.append(data.y[data.train_idx])
            sample_weight.append(data.train_sample_weight)

        outputs = self.forward(x, adj_t, multimodal=False)

        for modality in range(self.num_modalities):
            loss = self.loss_fn(outputs[modality], y[modality])
            loss = torch.mean(torch.mul(loss, sample_weight[modality]))
            self.logger.log_metrics({f"train_unimodal_step_loss ({modality + 1})": loss.detach()}, self.global_step)

            optimizer[modality].zero_grad()
            self.manual_backward(loss)
            optimizer[modality].step()

        if self.train_multimodal_decoder and self.multimodal_decoder is not None:
            output = self.forward(x, adj_t, multimodal=True)
            multi_loss = self.loss_fn(output, y[0])
            multi_loss = torch.mean(torch.mul(multi_loss, sample_weight[0]))
            self.logger.log_metrics({"train_multimodal_step_loss": multi_loss.detach()}, self.global_step)

            optimizer[-1].zero_grad()
            self.manual_backward(multi_loss)
            optimizer[-1].step()

    def test_step(self, test_batch, batch_idx: int):
        """Compute and return the test loss.

        Args:
            test_batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):
                The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.
            batch_idx (int): Integer displaying index of this batch.
        """
        x = []
        adj_t = []
        y = []
        for modality in range(self.num_modalities):
            data = test_batch[modality]
            x.append(data.x)
            adj_t.append(data.adj_t)
            y.append(torch.argmax(data.y[data.test_idx], dim=1))

        if self.multimodal_decoder is not None:
            output = self.forward(x, adj_t, multimodal=True)
        else:
            output = self.forward(x, adj_t, multimodal=False)[0]

        pred_test_data = torch.index_select(output, dim=0, index=test_batch[0].test_idx)
        final_output = F.softmax(pred_test_data, dim=1).detach().cpu().numpy()
        actual_output = y[0].detach().cpu()

        if self.num_classes == 2:
            self.log("Accuracy", round(accuracy_score(actual_output, final_output.argmax(1)), 3))
            self.log("F1", round(f1_score(actual_output, final_output.argmax(1)), 3))
            self.log("AUC", round(roc_auc_score(actual_output, final_output[:, 1]), 3))
        else:
            self.log("Accuracy", round(accuracy_score(actual_output, final_output.argmax(1)), 3))
            self.log("F1 weighted", round(f1_score(actual_output, final_output.argmax(1), average="weighted"), 3))
            self.log("F1 macro", round(f1_score(actual_output, final_output.argmax(1), average="macro"), 3))

        return final_output

    def _custom_data_loader(self) -> DataLoader:
        """Return an iterable or a collection of iterables that specifies all the samples in the dataset."""
        dataloaders = DataLoader(self.dataset, batch_size=1)
        return dataloaders

    def train_dataloader(self) -> DataLoader:
        """Return an iterable or a collection of iterables that specifies training samples in the dataset."""
        return self._custom_data_loader()

    def test_dataloader(self) -> DataLoader:
        """Return an iterable or a collection of iterables that specifies test samples in the dataset."""
        return self._custom_data_loader()

    def __str__(self) -> str:
        r"""Returns a string representation of the multiomics trainer object.

        Returns:
            str: The string representation of the multiomics trainer object.
        """
        model_str = ["\nModel info:\n", "   Unimodal encoder:\n"]

        for modality in range(self.num_modalities):
            model_str.append(f"    ({modality + 1}) {self.unimodal_encoder[modality]}")

        model_str.append("\n\n  Unimodal decoder:\n")
        for modality in range(self.num_modalities):
            model_str.append(f"    ({modality + 1}) {self.unimodal_decoder[modality]}")

        if self.multimodal_decoder is not None:
            model_str.append("\n\n  Multimodal decoder:\n")
            model_str.append(f"    {self.multimodal_decoder}")

        return "".join(model_str)
</file>

<file path="kale/pipeline/README.md">
# Machine learning pipeline

Machine learning pipelines combining several other modules to solve specific domain problems in medical imaging, graph/networks and computer vision.
</file>

<file path="kale/pipeline/video_domain_adapter.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""Domain adaptation systems (pipelines) for video data, e.g., for action recognition.
Most are inherited from kale.pipeline.domain_adapter.
"""

import torch

import kale.evaluate.metrics as losses
from kale.loaddata.video_access import get_image_modality
from kale.pipeline.domain_adapter import (
    BaseMMDLike,
    CDANTrainer,
    DANNTrainer,
    GradReverse,
    Method,
    set_requires_grad,
    WDGRLTrainer,
)


def create_mmd_based_video(method: Method, dataset, image_modality, feature_extractor, task_classifier, **train_params):
    """MMD-based deep learning methods for domain adaptation on video data: DAN and JAN"""
    if not method.is_mmd_method():
        raise ValueError(f"Unsupported MMD method: {method}")
    if method is Method.DAN:
        return DANTrainerVideo(
            dataset=dataset,
            image_modality=image_modality,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            method=method,
            **train_params,
        )
    if method is Method.JAN:
        return JANTrainerVideo(
            dataset=dataset,
            image_modality=image_modality,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            method=method,
            kernel_mul=[2.0, 2.0],
            kernel_num=[5, 1],
            **train_params,
        )


def create_dann_like_video(
    method: Method, dataset, image_modality, feature_extractor, task_classifier, critic, **train_params
):
    """DANN-based deep learning methods for domain adaptation on video data: DANN, CDAN, CDAN+E"""

    # Uncomment for later work.
    # Set up a new create_fewshot_trainer for video data based on original one in `domain_adapter.py`

    # if dataset.is_semi_supervised():
    #     return create_fewshot_trainer_video(
    #         method, dataset, feature_extractor, task_classifier, critic, **train_params
    #     )

    if method.is_dann_method():
        alpha = 0.0 if method is Method.Source else 1.0
        return DANNTrainerVideo(
            alpha=alpha,
            image_modality=image_modality,
            dataset=dataset,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    elif method.is_cdan_method():
        return CDANTrainerVideo(
            dataset=dataset,
            image_modality=image_modality,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            use_entropy=method is Method.CDAN_E,
            **train_params,
        )
    elif method is Method.WDGRL:
        return WDGRLTrainerVideo(
            dataset=dataset,
            image_modality=image_modality,
            feature_extractor=feature_extractor,
            task_classifier=task_classifier,
            critic=critic,
            method=method,
            **train_params,
        )
    else:
        raise ValueError(f"Unsupported method: {method}")


class BaseMMDLikeVideo(BaseMMDLike):
    """Common API for MME-based domain adaptation on video data: DAN, JAN"""

    def __init__(
        self,
        dataset,
        image_modality,
        feature_extractor,
        task_classifier,
        kernel_mul=2.0,
        kernel_num=5,
        **base_params,
    ):
        super().__init__(dataset, feature_extractor, task_classifier, kernel_mul, kernel_num, **base_params)
        self.image_modality = image_modality
        self.rgb_feat = self.feat["rgb"]
        self.flow_feat = self.feat["flow"]

    def forward(self, x):
        if self.feat is not None:
            if self.image_modality in ["rgb", "flow"]:
                if self.rgb_feat is not None:
                    x = self.rgb_feat(x)
                else:
                    x = self.flow_feat(x)
                x = x.view(x.size(0), -1)
                class_output = self.classifier(x)
                return x, class_output

            elif self.image_modality == "joint":
                x_rgb = self.rgb_feat(x["rgb"])
                x_flow = self.flow_feat(x["flow"])
                x_rgb = x_rgb.view(x_rgb.size(0), -1)
                x_flow = x_flow.view(x_flow.size(0), -1)
                x = torch.cat((x_rgb, x_flow), dim=1)
                class_output = self.classifier(x)
                return [x_rgb, x_flow], class_output

    def compute_loss(self, batch, split_name="valid"):
        # _s refers to source, _tu refers to unlabeled target
        if self.image_modality == "joint" and len(batch) == 4:
            (x_s_rgb, y_s), (x_s_flow, y_s_flow), (x_tu_rgb, y_tu), (x_tu_flow, y_tu_flow) = batch
            [phi_s_rgb, phi_s_flow], y_hat = self.forward({"rgb": x_s_rgb, "flow": x_s_flow})
            [phi_t_rgb, phi_t_flow], y_t_hat = self.forward({"rgb": x_tu_rgb, "flow": x_tu_flow})
            mmd_rgb = self._compute_mmd(phi_s_rgb, phi_t_rgb, y_hat, y_t_hat)
            mmd_flow = self._compute_mmd(phi_s_flow, phi_t_flow, y_hat, y_t_hat)
            mmd = mmd_rgb + mmd_flow
        elif self.image_modality in ["rgb", "flow"] and len(batch) == 2:
            (x_s, y_s), (x_tu, y_tu) = batch
            phi_s, y_hat = self.forward(x_s)
            phi_t, y_t_hat = self.forward(x_tu)
            mmd = self._compute_mmd(phi_s, phi_t, y_hat, y_t_hat)
        else:
            raise NotImplementedError("Batch len is {}. Check the Dataloader.".format(len(batch)))

        # Uncomment when checking whether rgb & flow labels are equal.
        # print('rgb_s:{}, flow_s:{}, rgb_f:{}, flow_f:{}'.format(y_s, y_s_flow, y_tu, y_tu_flow))
        # print('equal: {}/{}'.format(torch.all(torch.eq(y_s, y_s_flow)), torch.all(torch.eq(y_tu, y_tu_flow))))

        # ok is abbreviation for (all) correct
        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        mmd = mmd.double().mean()
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": mmd,
        }
        return task_loss, mmd, log_metrics


class DANTrainerVideo(BaseMMDLikeVideo):
    """This is an implementation of DAN for video data."""

    def __init__(self, dataset, image_modality, feature_extractor, task_classifier, **base_params):
        super().__init__(dataset, image_modality, feature_extractor, task_classifier, **base_params)

    def _compute_mmd(self, phi_s, phi_t, y_hat, y_t_hat):
        batch_size = int(phi_s.size()[0])
        kernels = losses.gaussian_kernel(
            phi_s,
            phi_t,
            kernel_mul=self._kernel_mul,
            kernel_num=self._kernel_num,
        )
        return losses.compute_mmd_loss(kernels, batch_size)


class JANTrainerVideo(BaseMMDLikeVideo):
    """This is an implementation of JAN for video data."""

    def __init__(
        self,
        dataset,
        image_modality,
        feature_extractor,
        task_classifier,
        kernel_mul=(2.0, 2.0),
        kernel_num=(5, 1),
        **base_params,
    ):
        super().__init__(
            dataset,
            image_modality,
            feature_extractor,
            task_classifier,
            kernel_mul=kernel_mul,
            kernel_num=kernel_num,
            **base_params,
        )

    def _compute_mmd(self, phi_s, phi_t, y_hat, y_t_hat):
        softmax_layer = torch.nn.Softmax(dim=-1)
        source_list = [phi_s, softmax_layer(y_hat)]
        target_list = [phi_t, softmax_layer(y_t_hat)]
        batch_size = int(phi_s.size()[0])

        joint_kernels = None
        for source, target, k_mul, k_num, sigma in zip(
            source_list, target_list, self._kernel_mul, self._kernel_num, [None, 1.68]
        ):
            kernels = losses.gaussian_kernel(source, target, kernel_mul=k_mul, kernel_num=k_num, fix_sigma=sigma)
            if joint_kernels is not None:
                joint_kernels = joint_kernels * kernels
            else:
                joint_kernels = kernels

        return losses.compute_mmd_loss(joint_kernels, batch_size)


class DANNTrainerVideo(DANNTrainer):
    """This is an implementation of DANN for video data."""

    def __init__(
        self,
        dataset,
        image_modality,
        feature_extractor,
        task_classifier,
        critic,
        method,
        **base_params,
    ):
        super(DANNTrainerVideo, self).__init__(
            dataset, feature_extractor, task_classifier, critic, method, **base_params
        )
        self.image_modality = image_modality
        self.rgb, self.flow = get_image_modality(self.image_modality)
        self.rgb_feat = self.feat["rgb"]
        self.flow_feat = self.feat["flow"]

    def forward(self, x):
        if self.feat is not None:
            x_rgb = x_flow = adversarial_output_rgb = adversarial_output_flow = None

            # For joint input, both two ifs are used
            if self.rgb:
                x_rgb = self.rgb_feat(x["rgb"])
                x_rgb = x_rgb.view(x_rgb.size(0), -1)
                reverse_feature_rgb = GradReverse.apply(x_rgb, self.alpha)
                adversarial_output_rgb = self.domain_classifier(reverse_feature_rgb)
            if self.flow:
                x_flow = self.flow_feat(x["flow"])
                x_flow = x_flow.view(x_flow.size(0), -1)
                reverse_feature_flow = GradReverse.apply(x_flow, self.alpha)
                adversarial_output_flow = self.domain_classifier(reverse_feature_flow)

            if self.rgb:
                if self.flow:  # For joint input
                    x = torch.cat((x_rgb, x_flow), dim=1)
                else:  # For rgb input
                    x = x_rgb
            else:  # For flow input
                x = x_flow
            class_output = self.classifier(x)

            return [x_rgb, x_flow], class_output, [adversarial_output_rgb, adversarial_output_flow]

    def compute_loss(self, batch, split_name="valid"):
        # _s refers to source, _tu refers to unlabeled target
        x_s_rgb = x_tu_rgb = x_s_flow = x_tu_flow = None
        if self.rgb:
            if self.flow:  # For joint input
                (x_s_rgb, y_s), (x_s_flow, y_s_flow), (x_tu_rgb, y_tu), (x_tu_flow, y_tu_flow) = batch
            else:  # For rgb input
                (x_s_rgb, y_s), (x_tu_rgb, y_tu) = batch
        else:  # For flow input
            (x_s_flow, y_s), (x_tu_flow, y_tu) = batch

        _, y_hat, [d_hat_rgb, d_hat_flow] = self.forward({"rgb": x_s_rgb, "flow": x_s_flow})
        _, y_t_hat, [d_t_hat_rgb, d_t_hat_flow] = self.forward({"rgb": x_tu_rgb, "flow": x_tu_flow})
        batch_size = len(y_s)

        if self.rgb:
            loss_dmn_src_rgb, dok_src_rgb = losses.cross_entropy_logits(d_hat_rgb, torch.zeros(batch_size))
            loss_dmn_tgt_rgb, dok_tgt_rgb = losses.cross_entropy_logits(d_t_hat_rgb, torch.ones(batch_size))
        if self.flow:
            loss_dmn_src_flow, dok_src_flow = losses.cross_entropy_logits(d_hat_flow, torch.zeros(batch_size))
            loss_dmn_tgt_flow, dok_tgt_flow = losses.cross_entropy_logits(d_t_hat_flow, torch.ones(batch_size))

        if self.rgb and self.flow:  # For joint input
            loss_dmn_src = loss_dmn_src_rgb + loss_dmn_src_flow
            loss_dmn_tgt = loss_dmn_tgt_rgb + loss_dmn_tgt_flow
            dok = torch.cat((dok_src_rgb, dok_src_flow, dok_tgt_rgb, dok_tgt_flow))
            dok_src = torch.cat((dok_src_rgb, dok_src_flow))
            dok_tgt = torch.cat((dok_tgt_rgb, dok_tgt_flow))
        else:
            if self.rgb:  # For rgb input
                d_hat = d_hat_rgb
                d_t_hat = d_t_hat_rgb
            else:  # For flow input
                d_hat = d_hat_flow
                d_t_hat = d_t_hat_flow

            # ok is abbreviation for (all) correct, dok refers to domain correct
            loss_dmn_src, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size))
            loss_dmn_tgt, dok_tgt = losses.cross_entropy_logits(d_t_hat, torch.ones(batch_size))
            dok = torch.cat((dok_src, dok_tgt))

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        dok = torch.cat((dok_src, dok_tgt)).double().mean()
        dok_src = dok_src.double().mean()
        dok_tgt = dok_tgt.double().mean()

        adv_loss = loss_dmn_src + loss_dmn_tgt  # adv_loss = src + tgt
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
        }

        return task_loss, adv_loss, log_metrics

    def training_step(self, batch, batch_nb):
        self._update_batch_epoch_factors(batch_nb)

        task_loss, adv_loss, log_metrics = self.compute_loss(batch, split_name="train")
        if self.current_epoch < self._init_epochs:
            loss = task_loss
        else:
            loss = task_loss + self.lamb_da * adv_loss

        log_metrics["train_total_loss"] = loss
        log_metrics["train_adv_loss"] = adv_loss
        log_metrics["train_task_loss"] = task_loss

        self.log_dict(log_metrics, on_step=True, on_epoch=False)

        # logging alpha and lambda when they exist (they exist for DANN and CDAN but not for DAN and JAN)
        self.log("alpha", self.alpha, on_step=False, on_epoch=True) if hasattr(self, "alpha") else None
        self.log("lambda", self.lamb_da, on_step=False, on_epoch=True) if hasattr(self, "lamb_da") else None

        return {"loss": loss}


class CDANTrainerVideo(CDANTrainer):
    """This is an implementation of CDAN for video data."""

    def __init__(
        self,
        dataset,
        image_modality,
        feature_extractor,
        task_classifier,
        critic,
        use_entropy=False,
        use_random=False,
        random_dim=1024,
        **base_params,
    ):
        super(CDANTrainerVideo, self).__init__(
            dataset, feature_extractor, task_classifier, critic, use_entropy, use_random, random_dim, **base_params
        )
        self.image_modality = image_modality
        self.rgb, self.flow = get_image_modality(image_modality)
        self.rgb_feat = self.feat["rgb"]
        self.flow_feat = self.feat["flow"]

    def forward(self, x):
        if self.feat is not None:
            x_rgb = x_flow = adversarial_output_rgb = adversarial_output_flow = None

            # For joint input, both two ifs are used
            if self.rgb:
                x_rgb = self.rgb_feat(x["rgb"])
                x_rgb = x_rgb.view(x_rgb.size(0), -1)
                reverse_feature_rgb = GradReverse.apply(x_rgb, self.alpha)
            if self.flow:
                x_flow = self.flow_feat(x["flow"])
                x_flow = x_flow.view(x_flow.size(0), -1)
                reverse_feature_flow = GradReverse.apply(x_flow, self.alpha)

            if self.rgb:
                if self.flow:  # For joint input
                    x = torch.cat((x_rgb, x_flow), dim=1)
                else:  # For rgb input
                    x = x_rgb
            else:  # For flow input
                x = x_flow
            class_output = self.classifier(x)
            softmax_output = torch.nn.Softmax(dim=1)(class_output)
            reverse_out = GradReverse.apply(softmax_output, self.alpha)

            if self.rgb:
                feature_rgb = torch.bmm(reverse_out.unsqueeze(2), reverse_feature_rgb.unsqueeze(1))
                feature_rgb = feature_rgb.view(-1, reverse_out.size(1) * reverse_feature_rgb.size(1))
                if self.random_layer:
                    random_out_rgb = self.random_layer.forward(feature_rgb)
                    adversarial_output_rgb = self.domain_classifier(random_out_rgb.view(-1, random_out_rgb.size(1)))
                else:
                    adversarial_output_rgb = self.domain_classifier(feature_rgb)

            if self.flow:
                feature_flow = torch.bmm(reverse_out.unsqueeze(2), reverse_feature_flow.unsqueeze(1))
                feature_flow = feature_flow.view(-1, reverse_out.size(1) * reverse_feature_flow.size(1))
                if self.random_layer:
                    random_out_flow = self.random_layer.forward(feature_flow)
                    adversarial_output_flow = self.domain_classifier(random_out_flow.view(-1, random_out_flow.size(1)))
                else:
                    adversarial_output_flow = self.domain_classifier(feature_flow)
            return [x_rgb, x_flow], class_output, [adversarial_output_rgb, adversarial_output_flow]

    def compute_loss(self, batch, split_name="valid"):
        # _s refers to source, _tu refers to unlabeled target
        x_s_rgb = x_tu_rgb = x_s_flow = x_tu_flow = None
        if self.rgb:
            if self.flow:  # For joint input
                (x_s_rgb, y_s), (x_s_flow, y_s_flow), (x_tu_rgb, y_tu), (x_tu_flow, y_tu_flow) = batch
            else:  # For rgb input
                (x_s_rgb, y_s), (x_tu_rgb, y_tu) = batch
        else:  # For flow input
            (x_s_flow, y_s), (x_tu_flow, y_tu) = batch

        _, y_hat, [d_hat_rgb, d_hat_flow] = self.forward({"rgb": x_s_rgb, "flow": x_s_flow})
        _, y_t_hat, [d_t_hat_rgb, d_t_hat_flow] = self.forward({"rgb": x_tu_rgb, "flow": x_tu_flow})
        batch_size = len(y_s)

        if self.entropy:
            e_s = self._compute_entropy_weights(y_hat)
            e_t = self._compute_entropy_weights(y_t_hat)
            source_weight = e_s / torch.sum(e_s)
            target_weight = e_t / torch.sum(e_t)
        else:
            source_weight = None
            target_weight = None

        if self.rgb:
            loss_dmn_src_rgb, dok_src_rgb = losses.cross_entropy_logits(
                d_hat_rgb, torch.zeros(batch_size), source_weight
            )
            loss_dmn_tgt_rgb, dok_tgt_rgb = losses.cross_entropy_logits(
                d_t_hat_rgb, torch.ones(batch_size), target_weight
            )

        if self.flow:
            loss_dmn_src_flow, dok_src_flow = losses.cross_entropy_logits(
                d_hat_flow, torch.zeros(batch_size), source_weight
            )
            loss_dmn_tgt_flow, dok_tgt_flow = losses.cross_entropy_logits(
                d_t_hat_flow, torch.ones(batch_size), target_weight
            )

        # ok is abbreviation for (all) correct, dok refers to domain correct
        if self.rgb and self.flow:  # For joint input
            loss_dmn_src = loss_dmn_src_rgb + loss_dmn_src_flow
            loss_dmn_tgt = loss_dmn_tgt_rgb + loss_dmn_tgt_flow
            dok = torch.cat((dok_src_rgb, dok_src_flow, dok_tgt_rgb, dok_tgt_flow)).double().mean()
            dok_src = torch.cat((dok_src_rgb, dok_src_flow)).double().mean()
            dok_tgt = torch.cat((dok_tgt_rgb, dok_tgt_flow)).double().mean()
        else:
            if self.rgb:  # For rgb input
                d_hat = d_hat_rgb
                d_t_hat = d_t_hat_rgb
            else:  # For flow input
                d_hat = d_hat_flow
                d_t_hat = d_t_hat_flow

            loss_dmn_src, dok_src = losses.cross_entropy_logits(d_hat, torch.zeros(batch_size))
            loss_dmn_tgt, dok_tgt = losses.cross_entropy_logits(d_t_hat, torch.ones(batch_size))
            dok = torch.cat((dok_src, dok_tgt)).double().mean()
            dok_src = dok_src.double().mean()
            dok_tgt = dok_tgt.double().mean()

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        adv_loss = loss_dmn_src + loss_dmn_tgt
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
        }

        return task_loss, adv_loss, log_metrics


class WDGRLTrainerVideo(WDGRLTrainer):
    """This is an implementation of WDGRL for video data."""

    def __init__(
        self,
        dataset,
        image_modality,
        feature_extractor,
        task_classifier,
        critic,
        k_critic=5,
        gamma=10,
        beta_ratio=0,
        **base_params,
    ):
        super(WDGRLTrainerVideo, self).__init__(
            dataset, feature_extractor, task_classifier, critic, k_critic, gamma, beta_ratio, **base_params
        )
        self.image_modality = image_modality
        self.rgb, self.flow = get_image_modality(self.image_modality)
        self.rgb_feat = self.feat["rgb"]
        self.flow_feat = self.feat["flow"]

    def forward(self, x):
        if self.feat is not None:
            x_rgb = x_flow = adversarial_output_rgb = adversarial_output_flow = None

            # For joint input, both two ifs are used
            if self.rgb:
                x_rgb = self.rgb_feat(x["rgb"])
                x_rgb = x_rgb.view(x_rgb.size(0), -1)
                adversarial_output_rgb = self.domain_classifier(x_rgb)
            if self.flow:
                x_flow = self.flow_feat(x["flow"])
                x_flow = x_flow.view(x_flow.size(0), -1)
                adversarial_output_flow = self.domain_classifier(x_flow)

            if self.rgb:
                if self.flow:  # For joint input
                    x = torch.cat((x_rgb, x_flow), dim=1)
                else:  # For rgb input
                    x = x_rgb
            else:  # For flow input
                x = x_flow
            class_output = self.classifier(x)

            return [x_rgb, x_flow], class_output, [adversarial_output_rgb, adversarial_output_flow]

    def compute_loss(self, batch, split_name="valid"):
        # _s refers to source, _tu refers to unlabeled target
        x_s_rgb = x_tu_rgb = x_s_flow = x_tu_flow = None
        if self.rgb:
            if self.flow:  # For joint input
                (x_s_rgb, y_s), (x_s_flow, y_s_flow), (x_tu_rgb, y_tu), (x_tu_flow, y_tu_flow) = batch
            else:  # For rgb input
                (x_s_rgb, y_s), (x_tu_rgb, y_tu) = batch
        else:  # For flow input
            (x_s_flow, y_s), (x_tu_flow, y_tu) = batch

        _, y_hat, [d_hat_rgb, d_hat_flow] = self.forward({"rgb": x_s_rgb, "flow": x_s_flow})
        _, y_t_hat, [d_t_hat_rgb, d_t_hat_flow] = self.forward({"rgb": x_tu_rgb, "flow": x_tu_flow})
        batch_size = len(y_s)

        # ok is abbreviation for (all) correct, dok refers to domain correct
        if self.rgb:
            _, dok_src_rgb = losses.cross_entropy_logits(d_hat_rgb, torch.zeros(batch_size))
            _, dok_tgt_rgb = losses.cross_entropy_logits(d_t_hat_rgb, torch.ones(batch_size))
        if self.flow:
            _, dok_src_flow = losses.cross_entropy_logits(d_hat_flow, torch.zeros(batch_size))
            _, dok_tgt_flow = losses.cross_entropy_logits(d_t_hat_flow, torch.ones(batch_size))

        if self.rgb and self.flow:  # For joint input
            dok = torch.cat((dok_src_rgb, dok_src_flow, dok_tgt_rgb, dok_tgt_flow)).double().mean()
            dok_src = torch.cat((dok_src_rgb, dok_src_flow)).double().mean()
            dok_tgt = torch.cat((dok_tgt_rgb, dok_tgt_flow)).double().mean()
            wasserstein_distance_rgb = d_hat_rgb.mean() - (1 + self._beta_ratio) * d_t_hat_rgb.mean()
            wasserstein_distance_flow = d_hat_flow.mean() - (1 + self._beta_ratio) * d_t_hat_flow.mean()
            wasserstein_distance = (wasserstein_distance_rgb + wasserstein_distance_flow) / 2
        else:
            if self.rgb:  # For rgb input
                d_hat = d_hat_rgb
                d_t_hat = d_t_hat_rgb
                dok_src = dok_src_rgb
                dok_tgt = dok_tgt_rgb
            else:  # For flow input
                d_hat = d_hat_flow
                d_t_hat = d_t_hat_flow
                dok_src = dok_src_flow
                dok_tgt = dok_tgt_flow

            wasserstein_distance = d_hat.mean() - (1 + self._beta_ratio) * d_t_hat.mean()
            dok = torch.cat((dok_src, dok_tgt)).double().mean()
            dok_src = dok_src.double().mean()
            dok_tgt = dok_tgt.double().mean()

        loss_cls, ok_src = losses.cross_entropy_logits(y_hat, y_s)
        _, ok_tgt = losses.cross_entropy_logits(y_t_hat, y_tu)
        ok_src = ok_src.double().mean()
        ok_tgt = ok_tgt.double().mean()

        adv_loss = wasserstein_distance
        task_loss = loss_cls

        log_metrics = {
            f"{split_name}_source_acc": ok_src,
            f"{split_name}_target_acc": ok_tgt,
            f"{split_name}_domain_acc": dok,
            f"{split_name}_source_domain_acc": dok_src,
            f"{split_name}_target_domain_acc": dok_tgt,
            f"{split_name}_wasserstein_dist": wasserstein_distance,
        }
        return task_loss, adv_loss, log_metrics

    def configure_optimizers(self):
        if self.image_modality in ["rgb", "flow"]:
            if self.rgb_feat is not None:
                nets = [self.rgb_feat, self.classifier]
            else:
                nets = [self.flow_feat, self.classifier]
        elif self.image_modality == "joint":
            nets = [self.rgb_feat, self.flow_feat, self.classifier]
        parameters = set()

        for net in nets:
            parameters |= set(net.parameters())

        if self._adapt_lr:
            task_feat_optimizer, task_feat_sched = self._configure_optimizer(parameters)
            self.critic_opt, self.critic_sched = self._configure_optimizer(self.domain_classifier.parameters())
            self.critic_opt = self.critic_opt[0]
            self.critic_sched = self.critic_sched[0]
            return task_feat_optimizer, task_feat_sched
        else:
            task_feat_optimizer = self._configure_optimizer(parameters)
            self.critic_opt = self._configure_optimizer(self.domain_classifier.parameters())
            self.critic_sched = None
            self.critic_opt = self.critic_opt[0]
        return task_feat_optimizer

    def critic_update_steps(self, batch):
        if self.current_epoch < self._init_epochs:
            return

        set_requires_grad(self.domain_classifier, requires_grad=True)

        if self.image_modality in ["rgb", "flow"]:
            if self.rgb_feat is not None:
                set_requires_grad(self.rgb_feat, requires_grad=False)
                (x_s, y_s), (x_tu, _) = batch
                with torch.no_grad():
                    h_s = self.rgb_feat(x_s).data.view(x_s.shape[0], -1)
                    h_t = self.rgb_feat(x_tu).data.view(x_tu.shape[0], -1)
            else:
                set_requires_grad(self.flow_feat, requires_grad=False)
                (x_s, y_s), (x_tu, _) = batch
                with torch.no_grad():
                    h_s = self.flow_feat(x_s).data.view(x_s.shape[0], -1)
                    h_t = self.flow_feat(x_tu).data.view(x_tu.shape[0], -1)

            for _ in range(self._k_critic):
                # gp refers to gradient penelty in Wasserstein distance.
                gp = losses.gradient_penalty(self.domain_classifier, h_s, h_t)

                critic_s = self.domain_classifier(h_s)
                critic_t = self.domain_classifier(h_t)
                wasserstein_distance = critic_s.mean() - (1 + self._beta_ratio) * critic_t.mean()

                critic_cost = -wasserstein_distance + self._gamma * gp

                self.critic_opt.zero_grad()
                critic_cost.backward()
                self.critic_opt.step()
                if self.critic_sched:
                    self.critic_sched.step()

            if self.rgb_feat is not None:
                set_requires_grad(self.rgb_feat, requires_grad=True)
            else:
                set_requires_grad(self.flow_feat, requires_grad=True)
            set_requires_grad(self.domain_classifier, requires_grad=False)

        elif self.image_modality == "joint":
            set_requires_grad(self.rgb_feat, requires_grad=False)
            set_requires_grad(self.flow_feat, requires_grad=False)
            (x_s_rgb, y_s), (x_s_flow, _), (x_tu_rgb, _), (x_tu_flow, _) = batch
            with torch.no_grad():
                h_s_rgb = self.rgb_feat(x_s_rgb).data.view(x_s_rgb.shape[0], -1)
                h_t_rgb = self.rgb_feat(x_tu_rgb).data.view(x_tu_rgb.shape[0], -1)
                h_s_flow = self.flow_feat(x_s_flow).data.view(x_s_flow.shape[0], -1)
                h_t_flow = self.flow_feat(x_tu_flow).data.view(x_tu_flow.shape[0], -1)
                h_s = torch.cat((h_s_rgb, h_s_flow), dim=1)
                h_t = torch.cat((h_t_rgb, h_t_flow), dim=1)

            # Need to improve to process rgb and flow separately in the future.
            for _ in range(self._k_critic):
                # gp_x refers to gradient penelty for the input with the modality x.
                gp_rgb = losses.gradient_penalty(self.domain_classifier, h_s_rgb, h_t_rgb)
                gp_flow = losses.gradient_penalty(self.domain_classifier, h_s_flow, h_t_flow)

                critic_s_rgb = self.domain_classifier(h_s_rgb)
                critic_s_flow = self.domain_classifier(h_s_flow)
                critic_t_rgb = self.domain_classifier(h_t_rgb)
                critic_t_flow = self.domain_classifier(h_t_flow)
                wasserstein_distance_rgb = critic_s_rgb.mean() - (1 + self._beta_ratio) * critic_t_rgb.mean()
                wasserstein_distance_flow = critic_s_flow.mean() - (1 + self._beta_ratio) * critic_t_flow.mean()

                critic_cost = (
                    -wasserstein_distance_rgb
                    + -wasserstein_distance_flow
                    + self._gamma * gp_rgb
                    + self._gamma * gp_flow
                ) * 0.5

                self.critic_opt.zero_grad()
                critic_cost.backward()
                self.critic_opt.step()
                if self.critic_sched:
                    self.critic_sched.step()

            set_requires_grad(self.rgb_feat, requires_grad=True)
            set_requires_grad(self.flow_feat, requires_grad=True)
            set_requires_grad(self.domain_classifier, requires_grad=False)
</file>

<file path="kale/predict/class_domain_nets.py">
# =============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""Classification of data or domain

Modules for typical classification tasks (into class labels) and adversarial discrimination of source vs target domains,
from https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/models/modules.py
"""

import torch
import torch.nn as nn

from kale.embed.video_i3d import Unit3D


# Previously FFSoftmaxClassifier
class SoftmaxNet(nn.Module):
    """Regular and domain classifier network for regular-size images

    Args:
        input_dim (int, optional): the dimension of the final feature vector.. Defaults to 15.
        n_classes (int, optional): the number of classes. Defaults to 2.
        name (str, optional): the classifier name. Defaults to "c".
        hidden (tuple, optional): the hidden layer sizes. Defaults to ().
        activation_fn ([type], optional): the activation function. Defaults to nn.ReLU.
    """

    def __init__(
        self,
        input_dim=15,
        n_classes=2,
        name="c",
        hidden=(),
        activation_fn=nn.ReLU,
        **activation_args,
    ):
        super(SoftmaxNet, self).__init__()
        self._n_classes = n_classes
        self._activation_fn = activation_fn
        self.chain = nn.Sequential()
        self.name = name
        self._hidden_sizes = hidden if hidden is not None else ()
        last_dim = input_dim
        for i, h in enumerate(self._hidden_sizes):
            self.chain.add_module(f"{name}_fc{i}", nn.Linear(last_dim, h))
            self.chain.add_module(f"f_{activation_fn.__name__}{i}", activation_fn(**activation_args))
            last_dim = h
        self.chain.add_module(f"{name}_fc_last", nn.Linear(last_dim, self._n_classes))
        self.activation = nn.LogSoftmax(dim=1)
        self.loss_class = nn.NLLLoss()

    def forward(self, input_data):
        class_output = self.chain(input_data)
        return class_output

    def extra_repr(self):
        if len(self._hidden_sizes) > 0:
            return f"{self.name}: {self.hidden_sizes}x{self._activation_fn.__name__}xLin"
        return f"{self.name}: Linear"

    def n_classes(self):
        return self._n_classes


class ClassNet(nn.Module):
    """Simple classification prediction-head block to plug ontop of the 4D output of a CNN.

    Args:
        n_class (int, optional): the number of different classes that can be predicted. Defaults to 10.
        input_shape (tuples, optional): the shape that input to this head will have. Expected
                      to be (batch_size, channels, height, width). Defaults to (-1, 64, 8, 8).
    """

    def __init__(self, n_class=10, input_shape=(-1, 64, 8, 8)):
        super(ClassNet, self).__init__()
        self.avgpool = nn.AvgPool2d(input_shape[2])
        self.linear = nn.Linear(input_shape[1], n_class)

    def forward(self, x):
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.linear(x)
        return x


class ClassNetSmallImage(nn.Module):
    """Regular classifier network for small-size images

    Args:
        input_size (int, optional): the dimension of the final feature vector. Defaults to 128.
        n_class (int, optional): the number of classes. Defaults to 10.
    """

    def __init__(self, input_size=128, n_class=10):
        super(ClassNetSmallImage, self).__init__()
        self._n_classes = n_class
        self.fc1 = nn.Linear(input_size, 100)
        self.bn1 = nn.BatchNorm1d(100)
        self.relu1 = nn.ReLU()
        self.dp1 = nn.Dropout()
        self.fc2 = nn.Linear(100, 100)
        self.bn2 = nn.BatchNorm1d(100)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(100, n_class)

    def n_classes(self):
        return self._n_classes

    def forward(self, input):
        x = self.dp1(self.relu1(self.bn1(self.fc1(input))))
        x = self.relu2(self.bn2(self.fc2(x)))
        x = self.fc3(x)
        return x


class DomainNetSmallImage(nn.Module):
    """Domain classifier network for small-size images

    Args:
        input_size (int, optional): the dimension of the final feature vector. Defaults to 128.
        bigger_discrim (bool, optional): whether to use deeper network. Defaults to False.
    """

    def __init__(self, input_size=128, bigger_discrim=False):
        super(DomainNetSmallImage, self).__init__()
        output_size = 500 if bigger_discrim else 100

        self.bigger_discrim = bigger_discrim
        self.fc1 = nn.Linear(input_size, output_size)
        self.bn1 = nn.BatchNorm1d(output_size)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(output_size, 100) if bigger_discrim else nn.Linear(output_size, 2)
        self.bn2 = nn.BatchNorm1d(100)
        self.relu2 = nn.ReLU()
        self.fc3 = nn.Linear(100, 2)

    def forward(self, input):
        x = self.relu1(self.bn1(self.fc1(input)))
        if self.bigger_discrim:
            x = self.relu2(self.bn2(self.fc2(x)))
            x = self.fc3(x)
        else:
            x = self.fc2(x)
        return x


# For Video/Action Recognition, DataClassifier.
class ClassNetVideo(nn.Module):
    """Regular classifier network for video input.

    Args:
        input_size (int, optional): the dimension of the final feature vector. Defaults to 512.
        n_channel (int, optional): the number of channel for Linear and BN layers.
        dropout_keep_prob (int, optional): the dropout probability for keeping the parameters.
        n_class (int, optional): the number of classes. Defaults to 8.
    """

    def __init__(self, input_size=512, n_channel=100, dropout_keep_prob=0.5, n_class=8):
        super(ClassNetVideo, self).__init__()
        self._n_classes = n_class
        self.fc1 = nn.Linear(input_size, n_channel)
        self.bn1 = nn.BatchNorm1d(n_channel)
        self.relu1 = nn.ReLU()
        self.dp1 = nn.Dropout(dropout_keep_prob)
        self.fc2 = nn.Linear(n_channel, n_class)

    def n_classes(self):
        return self._n_classes

    def forward(self, input):
        x = self.dp1(self.relu1(self.bn1(self.fc1(input))))
        x = self.fc2(x)
        return x


class ClassNetVideoConv(nn.Module):
    """Classifier network for video input refer to MMSADA.

    Args:
        input_size (int, optional): the dimension of the final feature vector. Defaults to 1024.
        n_class (int, optional): the number of classes. Defaults to 8.

    References:
        Munro Jonathan, and Dima Damen. "Multi-modal domain adaptation for fine-grained action recognition."
        In CVPR, pp. 122-132. 2020.
    """

    def __init__(self, input_size=1024, n_class=8):
        super(ClassNetVideoConv, self).__init__()
        self.dp = nn.Dropout()
        self.logits = Unit3D(
            in_channels=input_size,
            output_channels=n_class,
            kernel_shape=[1, 1, 1],
            padding=0,
            activation_fn=None,
            use_batch_norm=False,
            use_bias=True,
        )

    def forward(self, input):
        x = self.logits(self.dp(input))
        return x


# For Video/Action Recognition, DomainClassifier.
class DomainNetVideo(nn.Module):
    """Regular domain classifier network for video input.

    Args:
        input_size (int, optional): the dimension of the final feature vector. Defaults to 512.
        n_channel (int, optional): the number of channel for Linear and BN layers.
    """

    def __init__(self, input_size=128, n_channel=100):
        super(DomainNetVideo, self).__init__()

        self.fc1 = nn.Linear(input_size, n_channel)
        self.bn1 = nn.BatchNorm1d(n_channel)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(n_channel, 2)

    def forward(self, input):
        x = self.relu1(self.bn1(self.fc1(input)))
        x = self.fc2(x)
        return x
</file>

<file path="kale/predict/decode.py">
"""
Provides implementations of various decoders based on neural network modules for prediction and classification tasks.
Refer to the PyTorch documentation for the accompanying tutorial on neural network modules:
https://pytorch.org/docs/stable/generated/torch.nn.Module.html
"""

from typing import List, Tuple

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F

from kale.embed.gripnet import GripNet
from kale.evaluate.metrics import auprc_auroc_ap
from kale.prepdata.graph_negative_sampling import typed_negative_sampling
from kale.prepdata.supergraph_construct import SuperGraph
from kale.utils.initialize_nn import bias_init, xavier_init


class MLPDecoder(nn.Module):
    """
    A generalized MLP model that can act as either a 2-layer MLPDecoder or a 4-layer MLPDecoder based on the include_decoder_layers parameter.

    Args:
        in_dim (int): the dimension of input feature.
        hidden_dim (int): the dimension of hidden layers.
        out_dim (int): the dimension of output layer.
        dropout_rate (float): the dropout rate during training.
        include_decoder_layers (bool): whether or not to include the additional layers that are part of the MLPDecoder
    """

    def __init__(self, in_dim, hidden_dim, out_dim, dropout_rate=0.1, include_decoder_layers=False):
        super(MLPDecoder, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.include_decoder_layers = include_decoder_layers

        if self.include_decoder_layers:
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, out_dim)
            self.fc4 = nn.Linear(out_dim, 1)
            torch.nn.init.normal_(self.fc4.weight)
            self.dropout = nn.Dropout(dropout_rate)
        else:
            self.fc2 = nn.Linear(hidden_dim, out_dim)
            self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        if self.include_decoder_layers:
            x = self.dropout(F.relu(x))
            x = F.relu(self.fc3(x))
            x = self.fc4(x)

        return x


class DistMultDecoder(torch.nn.Module):
    """
    Build `DistMult
    <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf>`_ factorization
    as GripNet decoder in PoSE dataset.
    Copy-paste with slight modifications from https://github.com/NYXFLOWER/GripNet.

    Args:
        in_channels (int): the dimension of input feature.
        num_edge_type (int): the number of edge types.
    """

    def __init__(self, in_channels: int, num_edge_type: int):
        super(DistMultDecoder, self).__init__()
        self.num_edge_type = num_edge_type
        self.in_channels = in_channels
        self.weight = torch.nn.Parameter(torch.Tensor(num_edge_type, in_channels))

        self.reset_parameters()

    def forward(self, x, edge_index: torch.Tensor, edge_type: torch.Tensor, sigmoid: bool = True) -> torch.Tensor:
        """
        Args:
            x: the input node feature embeddings.
            edge_index: the edge index in COO format with shape [2, num_edges].
            edge_type: the one-dimensional relation type/index for each target edge in edge_index.
            sigmoid: whether to use sigmoid function or not.
        """
        value = (x[edge_index[0]] * x[edge_index[1]] * self.weight[edge_type]).sum(dim=1)
        return torch.sigmoid(value) if sigmoid else value

    def reset_parameters(self):
        self.weight.data.normal_(std=1 / np.sqrt(self.in_channels))

    def __repr__(self) -> str:
        return "{}: DistMultLayer(in_channels={}, num_relations={})".format(
            self.__class__.__name__, self.in_channels, self.num_edge_type
        )


class GripNetLinkPrediction(pl.LightningModule):
    """
    Build GripNet-DistMult (encoder-decoder) model for link prediction.

    Args:
        supergraph (SuperGraph): the input supergraph.
        learning_rate (float): the learning rate for training.
        epsilon (float, optional): a small number in loss function to improve numerical stability. Defaults to 1e-13.
    """

    def __init__(self, supergraph: SuperGraph, learning_rate: float, epsilon: float = 1e-13):
        super().__init__()

        self.learning_rate = learning_rate
        self.epsilon = epsilon

        self.encoder = GripNet(supergraph)
        self.decoder = self.__init_decoder__()

    def __init_decoder__(self) -> DistMultDecoder:
        in_channels = self.encoder.out_channels
        supergraph = self.encoder.supergraph
        task_supervertex_name = supergraph.topological_order[-1]
        num_edge_type = supergraph.supervertex_dict[task_supervertex_name].num_edge_type

        # get the number of nodes on the task-associated supervertex
        self.num_task_nodes = supergraph.supervertex_dict[task_supervertex_name].num_node

        return DistMultDecoder(in_channels, num_edge_type)

    def forward(self, edge_index: torch.Tensor, edge_type: torch.Tensor, edge_type_range: torch.Tensor) -> Tuple:
        x = self.encoder()

        pos_score = self.decoder(x, edge_index, edge_type)
        pos_loss = -torch.log(pos_score + self.epsilon).mean()

        edge_index = typed_negative_sampling(edge_index, self.num_task_nodes, edge_type_range)

        neg_score = self.decoder(x, edge_index, edge_type)
        neg_loss = -torch.log(1 - neg_score + self.epsilon).mean()

        loss = pos_loss + neg_loss

        # compute averaged metric scores over edge types
        num_edge_type = edge_type_range.shape[0]
        record = []
        for i in range(num_edge_type):
            start, end = edge_type_range[i]
            pos_score_this_type, neg_score_this_type = pos_score[start:end], neg_score[start:end]

            score = torch.cat([pos_score_this_type, neg_score_this_type])
            target = torch.cat([torch.ones(pos_score_this_type.shape[0]), torch.zeros(neg_score_this_type.shape[0])])

            record.append(list(auprc_auroc_ap(target, score)))
        auprc, auroc, ave_precision = np.array(record).mean(axis=0)

        return loss, auprc, auroc, ave_precision

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)

        return optimizer

    def __step__(self, batch, mode="train"):
        edge_index, edge_type, edge_type_range = batch
        loss, auprc, auroc, ap = self.forward(
            edge_index.reshape((2, -1)), edge_type.flatten(), edge_type_range.reshape((-1, 2))
        )

        if mode == "train" or "val":
            self.log(f"{mode}_loss", loss)
        else:
            self.log(f"{mode}_auprc", auprc)
            self.log(f"{mode}_auroc", auroc)
            self.log(f"{mode}_ap@50", ap)

        return loss

    def training_step(self, batch, batch_idx):
        return self.__step__(batch)

    def validation_step(self, batch, batch_idx):
        return self.__step__(batch, mode="val")

    def test_step(self, batch, batch_idx):
        return self.__step__(batch, mode="test")

    def __repr__(self) -> str:
        return "{}: \nEncoder: {} ModuleDict(\n{})\n Decoder: {}".format(
            self.__class__.__name__, self.encoder.__class__.__name__, self.encoder.supervertex_module_dict, self.decoder
        )


class LinearClassifier(nn.Module):
    r"""Build a linear transformation module.

    Args:
        in_dim (int): Size of each input sample.
        out_dim (int): Size of each output sample.
        bias (bool, optional): If set to ``False``, the layer will not learn an additive bias. (default: ``True``)
    """

    def __init__(self, in_dim: int, out_dim: int, bias: bool = True) -> None:
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim, bias=bias)
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """Initialize the parameters of the model."""
        self.fc.apply(xavier_init)
        self.fc.apply(bias_init)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.fc(x)
        return x


class VCDN(nn.Module):
    r"""The View Correlation Discovery Network (VCDN) to learn the higher-level intra-view and cross-view correlations
    in the label space, implemented according to the method described in 'MOGONET integrates multi-omics data using
    graph convolutional networks allowing patient classification and biomarker identification'
    - Wang, T., Shao, W., Huang, Z., Tang, H., Zhang, J., Ding, Z., Huang, K. (2021).

    Args:
        num_modalities (int): The total number of modalities in the dataset.
        num_classes (int): The total number of classes in the dataset.
        hidden_dim (int): Size of the hidden layer.
    """

    def __init__(self, num_modalities: int, num_classes: int, hidden_dim: int) -> None:
        super().__init__()

        self.num_modalities = num_modalities
        self.num_classes = num_classes
        self.model = nn.Sequential(
            nn.Linear(pow(self.num_classes, self.num_modalities), hidden_dim),
            nn.LeakyReLU(0.25),
            nn.Linear(hidden_dim, self.num_classes),
        )
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """Initialize the parameters of the model."""
        self.model.apply(xavier_init)
        self.model.apply(bias_init)

    def forward(self, multimodal_input: List[torch.Tensor]) -> torch.Tensor:
        for modality in range(self.num_modalities):
            multimodal_input[modality] = torch.sigmoid(multimodal_input[modality])
        x = torch.reshape(
            torch.matmul(multimodal_input[0].unsqueeze(-1), multimodal_input[1].unsqueeze(1)),
            (-1, pow(self.num_classes, 2), 1),
        )
        for modality in range(2, self.num_modalities):
            x = torch.reshape(
                torch.matmul(x, multimodal_input[modality].unsqueeze(1)), (-1, pow(self.num_classes, modality + 1), 1)
            )
        input_tensor = torch.reshape(x, (-1, pow(self.num_classes, self.num_modalities)))
        output = self.model(input_tensor)

        return output
</file>

<file path="kale/predict/isonet.py">
"""
The ISONet module, which is based on the ResNet module,
from https://github.com/HaozhiQi/ISONet/blob/master/isonet/models/isonet.py
(based on https://github.com/facebookresearch/pycls/blob/master/pycls/models/resnet.py)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Stage depths for ImageNet models
_IN_STAGE_DS = {
    18: (2, 2, 2, 2),
    34: (3, 4, 6, 3),
    46: (3, 4, 12, 3),
    50: (3, 4, 6, 3),
    101: (3, 4, 23, 3),
    152: (3, 8, 36, 3),
}


def get_trans_fun(name):
    """Retrieves the transformation function by name."""
    trans_funs = {
        "basic_transform": BasicTransform,
        "bottleneck_transform": BottleneckTransform,
    }
    assert name in trans_funs.keys(), "Transformation function '{}' not supported".format(name)
    return trans_funs[name]


class SReLU(nn.Module):
    """Shifted ReLU"""

    def __init__(self, nc):
        super(SReLU, self).__init__()
        self.srelu_bias = nn.Parameter(torch.Tensor(1, nc, 1, 1))
        self.srelu_relu = nn.ReLU(inplace=True)
        nn.init.constant_(self.srelu_bias, -1.0)

    def forward(self, x):
        return self.srelu_relu(x - self.srelu_bias) + self.srelu_bias


# class SharedScale(nn.Module):
#     """Channel-shared scalar"""
#     def __init__(self):
#         super(SharedScale, self).__init__()
#         self.scale = nn.Parameter(torch.ones(1, 1, 1, 1) * C.ISON.RES_MULTIPLIER)

#     def forward(self, x):
#         return x * self.scale


class ResHead(nn.Module):
    """ResNet head."""

    def __init__(self, w_in, net_params):
        super(ResHead, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.use_dropout = net_params["use_dropout"]
        if self.use_dropout:
            self.dropout = nn.Dropout(p=net_params["dropout_rate"], inplace=True)
        self.fc = nn.Linear(w_in, net_params["nc"], bias=True)

    def forward(self, x):
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        if self.use_dropout:
            x = self.dropout(x)
        x = self.fc(x)
        return x


class BasicTransform(nn.Module):
    """Basic transformation: 3x3, 3x3"""

    def __init__(self, w_in, w_out, stride, has_bn, use_srelu, w_b=None, num_gs=1):
        assert w_b is None and num_gs == 1, "Basic transform does not support w_b and num_gs options"
        super(BasicTransform, self).__init__()
        self.has_bn = has_bn
        self.use_srelu = use_srelu
        self._construct(w_in, w_out, stride)

    def _construct(self, w_in, w_out, stride):
        # 3x3, BN, ReLU
        self.a = nn.Conv2d(
            w_in, w_out, kernel_size=3, stride=stride, padding=1, bias=not self.has_bn and not self.use_srelu
        )
        if self.has_bn:
            self.a_bn = nn.BatchNorm2d(w_out)
        self.a_relu = nn.ReLU(inplace=True) if not self.use_srelu else SReLU(w_out)
        # 3x3, BN
        self.b = nn.Conv2d(
            w_out, w_out, kernel_size=3, stride=1, padding=1, bias=not self.has_bn and not self.use_srelu
        )
        if self.has_bn:
            self.b_bn = nn.BatchNorm2d(w_out)
            self.b_bn.final_bn = True

        # if C.ISON.HAS_RES_MULTIPLIER:
        #     self.shared_scalar = SharedScale()

    def forward(self, x):
        for layer in self.children():
            x = layer(x)
        return x


class BottleneckTransform(nn.Module):
    """Bottleneck transformation: 1x1, 3x3, 1x1, only for very deep networks"""

    def __init__(self, w_in, w_out, stride, has_bn, use_srelu, w_b, num_gs):
        super(BottleneckTransform, self).__init__()
        self.has_bn = has_bn
        self.use_srelu = use_srelu
        self._construct(w_in, w_out, stride, w_b, num_gs)

    def _construct(self, w_in, w_out, stride, w_b, num_gs):
        # MSRA -> stride=2 is on 1x1; TH/C2 -> stride=2 is on 3x3
        (str1x1, str3x3) = (1, stride)
        # 1x1, BN, ReLU
        self.a = nn.Conv2d(
            w_in, w_b, kernel_size=1, stride=str1x1, padding=0, bias=not self.has_bn and not self.use_srelu
        )
        if self.has_bn:
            self.a_bn = nn.BatchNorm2d(w_b)
        self.a_relu = nn.ReLU(inplace=True) if not self.use_srelu else SReLU(w_b)
        # 3x3, BN, ReLU
        self.b = nn.Conv2d(
            w_b,
            w_b,
            kernel_size=3,
            stride=str3x3,
            padding=1,
            groups=num_gs,
            bias=not self.has_bn and not self.use_srelu,
        )
        if self.has_bn:
            self.b_bn = nn.BatchNorm2d(w_b)
        self.b_relu = nn.ReLU(inplace=True) if not self.use_srelu else SReLU(w_b)
        # 1x1, BN
        self.c = nn.Conv2d(w_b, w_out, kernel_size=1, stride=1, padding=0, bias=not self.has_bn and not self.use_srelu)
        if self.has_bn:
            self.c_bn = nn.BatchNorm2d(w_out)
            self.c_bn.final_bn = True

        # if C.ISON.HAS_RES_MULTIPLIER:
        #     self.shared_scalar = SharedScale()

    def forward(self, x):
        for layer in self.children():
            x = layer(x)
        return x


class ResBlock(nn.Module):
    """Residual block: x + F(x)"""

    def __init__(self, w_in, w_out, stride, trans_fun, has_bn, has_st, use_srelu, w_b=None, num_gs=1):
        super(ResBlock, self).__init__()
        self.has_bn = has_bn
        self.has_st = has_st
        self.use_srelu = use_srelu
        self._construct(w_in, w_out, stride, trans_fun, w_b, num_gs)

    def _add_skip_proj(self, w_in, w_out, stride):
        self.proj = nn.Conv2d(
            w_in, w_out, kernel_size=1, stride=stride, padding=0, bias=not self.has_bn and not self.use_srelu
        )
        if self.has_bn:
            self.bn = nn.BatchNorm2d(w_out)

    def _construct(self, w_in, w_out, stride, trans_fun, w_b, num_gs):
        # Use skip connection with projection if shape changes
        self.proj_block = (w_in != w_out) or (stride != 1)
        if self.proj_block and self.has_st:
            self._add_skip_proj(w_in, w_out, stride)
        self.f = trans_fun(w_in, w_out, stride, self.has_bn, self.use_srelu, w_b, num_gs)
        self.relu = nn.ReLU(True) if not self.use_srelu else SReLU(w_out)

    def forward(self, x):
        if self.proj_block:
            if self.has_bn and self.has_st:
                x = self.bn(self.proj(x)) + self.f(x)
            elif not self.has_bn and self.has_st:
                x = self.proj(x) + self.f(x)
            else:
                x = self.f(x)
        else:
            if self.has_st:
                x = x + self.f(x)
            else:
                x = self.f(x)
        x = self.relu(x)
        return x


class ResStage(nn.Module):
    """Stage of ResNet."""

    def __init__(self, w_in, w_out, stride, net_params, d, w_b=None, num_gs=1):
        super(ResStage, self).__init__()
        self.transfun = net_params["transfun"]
        self.has_bn = net_params["has_bn"]
        self.has_st = net_params["has_st"]
        self.use_srelu = net_params["use_srelu"]
        self._construct(w_in, w_out, stride, d, w_b, num_gs)

    def _construct(self, w_in, w_out, stride, d, w_b, num_gs):
        # Construct the blocks
        for i in range(d):
            # Stride and w_in apply to the first block of the stage
            b_stride = stride if i == 0 else 1
            b_w_in = w_in if i == 0 else w_out
            # Retrieve the transformation function
            trans_fun = get_trans_fun(self.transfun)
            # Construct the block
            res_block = ResBlock(
                b_w_in, w_out, b_stride, trans_fun, self.has_bn, self.has_st, self.use_srelu, w_b, num_gs
            )
            self.add_module("b{}".format(i + 1), res_block)

    def forward(self, x):
        for block in self.children():
            x = block(x)
        return x


class ResStem(nn.Module):
    """Stem of ResNet."""

    def __init__(
        self,
        w_in,
        w_out,
        net_params,
        kernelsize=3,
        stride=1,
        padding=1,
        use_maxpool=False,
        poolksize=3,
        poolstride=2,
        poolpadding=1,
    ):
        super(ResStem, self).__init__()
        self.has_bn = net_params["has_bn"]
        self.use_srelu = net_params["use_srelu"]
        self.kernelsize = kernelsize
        self.stride = stride
        self.padding = padding
        self.use_maxpool = use_maxpool
        self.poolksize = poolksize
        self.poolstride = poolstride
        self.poolpadding = poolpadding
        self._construct(w_in, w_out)

    def _construct(self, w_in, w_out):
        # 3x3, BN, ReLU for cifar and  7x7, BN, ReLU, maxpool for imagenet
        self.conv = nn.Conv2d(
            w_in,
            w_out,
            kernel_size=self.kernelsize,
            stride=self.stride,
            padding=self.padding,
            bias=not self.has_bn and not self.use_srelu,
        )
        if self.has_bn:
            self.bn = nn.BatchNorm2d(w_out)
        self.relu = nn.ReLU(True) if not self.use_srelu else SReLU(w_out)
        if self.use_maxpool:
            self.pool = nn.MaxPool2d(kernel_size=self.poolksize, stride=self.poolstride, padding=self.poolpadding)

    def forward(self, x):
        for layer in self.children():
            x = layer(x)
        return x


class ISONet(nn.Module):
    """ISONet, a modified ResNet model."""

    # def __init__(self, use_dirac=True):
    def __init__(self, net_params):
        super(ISONet, self).__init__()
        # define network structures
        # self._construct()
        self._construct(net_params)
        # initialization
        self._network_init(net_params["use_dirac"])

    # Depth for ResNet, e.g. [3, 4, 6, 3] for ResNet50
    def _construct(self, net_params):
        # Setting for ImageNet image size. To override if different.
        # Retrieve the number of blocks per stage
        (d1, d2, d3, d4) = _IN_STAGE_DS[net_params["depths"]]  # _depths
        # Compute the initial bottleneck width
        # Stem: (N, 3, 224, 224) -> (N, 64, 56, 56)
        self.stem = ResStem(w_in=3, w_out=64, net_params=net_params)
        # Stage 1: (N, 64, 56, 56) -> (N, 256, 56, 56)
        self.s1 = ResStage(w_in=64, w_out=64, stride=1, net_params=net_params, d=d1)
        # Stage 2: (N, 256, 56, 56) -> (N, 512, 28, 28)
        self.s2 = ResStage(w_in=64, w_out=128, stride=2, net_params=net_params, d=d2)
        # Stage 3: (N, 512, 56, 56) -> (N, 1024, 14, 14)
        self.s3 = ResStage(w_in=128, w_out=256, stride=2, net_params=net_params, d=d3)
        # Stage 4: (N, 1024, 14, 14) -> (N, 2048, 7, 7)
        self.s4 = ResStage(w_in=256, w_out=512, stride=2, net_params=net_params, d=d4)
        # Head: (N, 2048, 7, 7) -> (N, num_classes)
        self.head = ResHead(w_in=512, net_params=net_params)

    def _network_init(self, use_dirac=True):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if use_dirac:
                    # the first 7x7 convolution we use pytorch default initialization
                    # and not enforce orthogonality since the large input/output channel difference
                    if m.kernel_size != (7, 7):
                        nn.init.dirac_(m.weight)
                else:
                    # kaiming initialization used for ResNet results
                    fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                    m.weight.data.normal_(mean=0.0, std=np.sqrt(2.0 / fan_out))
                if hasattr(m, "bias") and m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                zero_init_gamma = hasattr(m, "final_bn") and m.final_bn
                m.weight.data.fill_(0.0 if zero_init_gamma else 1.0)
                m.bias.data.zero_()

    def forward(self, x):
        for module in self.children():
            x = module(x)
        return x

    def ortho(self, device):
        """regularizes the convolution kernel to be (near) orthogonal during training.
        This is called in Trainer.loss of the isonet example.
        """
        ortho_penalty = []
        cnt = 0
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if m.kernel_size == (7, 7) or m.weight.shape[1] == 3:
                    continue
                o = self.ortho_conv(m, device)
                cnt += 1
                ortho_penalty.append(o)
        ortho_penalty = sum(ortho_penalty)
        return ortho_penalty

    def ortho_conv(self, m, device):
        """regularizes the convolution kernel to be (near) orthogonal during training.

        Args:
            m (nn.module]): [description]
        """
        operator = m.weight
        operand = torch.cat(torch.chunk(m.weight, m.groups, dim=0), dim=1)
        transposed = m.weight.shape[1] < m.weight.shape[0]
        num_channels = m.weight.shape[1] if transposed else m.weight.shape[0]
        if transposed:
            operand = operand.transpose(1, 0)
            operator = operator.transpose(1, 0)
        gram = F.conv2d(
            operand, operator, padding=(m.kernel_size[0] - 1, m.kernel_size[1] - 1), stride=m.stride, groups=m.groups
        )
        identity = torch.zeros(gram.shape).to(device)
        identity[:, :, identity.shape[2] // 2, identity.shape[3] // 2] = torch.eye(num_channels).repeat(1, m.groups)
        out = torch.sum((gram - identity) ** 2.0) / 2.0
        return out
</file>

<file path="kale/predict/README.md">
# Prediction modules

Machine learning modules for predicting an output, including layers for building neural networks. Neural nets need to be made generic to handle different data to be here.
</file>

<file path="kale/predict/uncertainty_binning.py">
"""
Authors: Lawrence Schobs, lawrenceschobs@gmail.com
Module from the implementation of L. A. Schobs, A. J. Swift and H. Lu, "Uncertainty Estimation for Heatmap-Based Landmark Localization,"
in IEEE Transactions on Medical Imaging, vol. 42, no. 4, pp. 1021-1034, April 2023, doi: 10.1109/TMI.2022.3222730.

Functions to predict uncertainty quantiles from the quantile binning method. Includes:
   A) Binning Predictions: quantile_binning_predictions
   """
from typing import Dict, List, Optional, Union

import numpy as np


def quantile_binning_predictions(
    uncertainties_test: Dict[str, Union[int, float]],
    uncert_thresh: List[List[float]],
    save_pred_path: Optional[str] = None,
) -> Dict[str, int]:
    """
    Bin predictions based on quantile thresholds.

    Args:
        uncertainties_test (Dict): A dictionary of uncertainties with string ids and float/int uncertainty values.
        uncert_thresh (List[List[float]]): A list of quantile thresholds to determine binning.
        save_pred_path (str, optional): A path preamble to save predicted bins to.

    Returns:
        Dict: A dictionary of predicted quantile bins with string ids as keys and integer bin values as values.
    """
    # Test if dictionary correct structure
    if not isinstance(uncertainties_test, dict):
        raise ValueError("uncertainties_test must be of type dict")
    else:
        for i, (key, fc) in enumerate(uncertainties_test.items()):
            if not isinstance(key, str) or (not isinstance(fc, float) and not isinstance(fc, int)):
                raise ValueError(
                    r"Dict uncertainties_test should be of structure {string_id1: float_uncertainty1/int_uncertainty1, string_id2: float_uncertainty2/int_uncertainty2 } "
                )

    if np.array(uncert_thresh).shape != (len(uncert_thresh), 1):
        raise ValueError("uncert_thresh list should be 2D e.g. [[0.1], [0.2], [0.3]] ")

    all_binned_errors = {}

    for i, (key, fc) in enumerate(uncertainties_test.items()):
        for q in range(len(uncert_thresh) + 1):
            if q == 0:
                lower_c_bound = uncert_thresh[q][0]

                if fc <= lower_c_bound:
                    all_binned_errors[key] = q

            elif q < len(uncert_thresh):
                lower_c_bound = uncert_thresh[q - 1][0]
                upper_c_bound = uncert_thresh[q][0]

                if fc <= upper_c_bound:
                    if fc > lower_c_bound:
                        all_binned_errors[key] = q

            # Finally do the last bin
            else:
                lower_c_bound = uncert_thresh[q - 1][0]

                if fc > lower_c_bound:
                    all_binned_errors[key] = q

    return all_binned_errors
</file>

<file path="kale/prepdata/chem_transform.py">
"""
Functions for labeling and encoding chemical characters like Compound SMILES and atom string, refer to
https://github.com/hkmztrk/DeepDTA and https://github.com/thinng/GraphDTA.
"""

import logging

import numpy as np
from rdkit import Chem

CHARPROTSET = {
    "A": 1,
    "C": 2,
    "B": 3,
    "E": 4,
    "D": 5,
    "G": 6,
    "F": 7,
    "I": 8,
    "H": 9,
    "K": 10,
    "M": 11,
    "L": 12,
    "O": 13,
    "N": 14,
    "Q": 15,
    "P": 16,
    "S": 17,
    "R": 18,
    "U": 19,
    "T": 20,
    "W": 21,
    "V": 22,
    "Y": 23,
    "X": 24,
    "Z": 25,
}

CHARPROTLEN = 25

CHARISOSMISET = {
    "#": 29,
    "%": 30,
    ")": 31,
    "(": 1,
    "+": 32,
    "-": 33,
    "/": 34,
    ".": 2,
    "1": 35,
    "0": 3,
    "3": 36,
    "2": 4,
    "5": 37,
    "4": 5,
    "7": 38,
    "6": 6,
    "9": 39,
    "8": 7,
    "=": 40,
    "A": 41,
    "@": 8,
    "C": 42,
    "B": 9,
    "E": 43,
    "D": 10,
    "G": 44,
    "F": 11,
    "I": 45,
    "H": 12,
    "K": 46,
    "M": 47,
    "L": 13,
    "O": 48,
    "N": 14,
    "P": 15,
    "S": 49,
    "R": 16,
    "U": 50,
    "T": 17,
    "W": 51,
    "V": 18,
    "Y": 52,
    "[": 53,
    "Z": 19,
    "]": 54,
    "\\": 20,
    "a": 55,
    "c": 56,
    "b": 21,
    "e": 57,
    "d": 22,
    "g": 58,
    "f": 23,
    "i": 59,
    "h": 24,
    "m": 60,
    "l": 25,
    "o": 61,
    "n": 26,
    "s": 62,
    "r": 27,
    "u": 63,
    "t": 28,
    "y": 64,
}

CHARISOSMILEN = 64

CHARATOMSET = [
    "C",
    "N",
    "O",
    "S",
    "F",
    "Si",
    "P",
    "Cl",
    "Br",
    "Mg",
    "Na",
    "Ca",
    "Fe",
    "As",
    "Al",
    "I",
    "B",
    "V",
    "K",
    "Tl",
    "Yb",
    "Sb",
    "Sn",
    "Ag",
    "Pd",
    "Co",
    "Se",
    "Ti",
    "Zn",
    "H",
    "Li",
    "Ge",
    "Cu",
    "Au",
    "Ni",
    "Cd",
    "In",
    "Mn",
    "Zr",
    "Cr",
    "Pt",
    "Hg",
    "Pb",
    "Unknown",
]

CHARATOMLEN = 44


def integer_label_smiles(smiles, max_length=85, isomeric=False):
    """
    Integer encoding for SMILES string sequence.

    Args:
        smiles (str): Simplified molecular-input line-entry system, which is a specification in the form of a line
        notation for describing the structure of chemical species using short ASCII strings.
        max_length (int): Maximum encoding length of input SMILES string. (default: 85)
        isomeric (bool): Whether the input SMILES string includes isomeric information (default: False).
    """
    if not isomeric:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            logging.warning(f"rdkit cannot find this SMILES {smiles}.")
        else:
            smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True)
    encoding = np.zeros(max_length)
    for idx, letter in enumerate(smiles[:max_length]):
        try:
            encoding[idx] = CHARISOSMISET[letter]
        except KeyError:
            logging.warning(
                f"character {letter} does not exists in default SMILE category encoding, skip and treat as " f"padding."
            )

    return encoding


def integer_label_protein(sequence, max_length=1200):
    """
    Integer encoding for protein string sequence.

    Args:
        sequence (str): Protein string sequence.
        max_length: Maximum encoding length of input protein string. (default: 1200)
    """
    encoding = np.zeros(max_length)
    for idx, letter in enumerate(sequence[:max_length]):
        try:
            letter = letter.upper()
            encoding[idx] = CHARPROTSET[letter]
        except KeyError:
            logging.warning(
                f"character {letter} does not exists in sequence category encoding, skip and treat as " f"padding."
            )
    return encoding
</file>

<file path="kale/prepdata/graph_negative_sampling.py">
import numpy as np
import torch


def negative_sampling(pos_edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:
    r"""
    Negative sampling for link prediction. Copy-paste from https://github.com/NYXFLOWER/GripNet.

    Args:
        pos_edge_index (torch.Tensor): edge indices in COO format with shape [2, num_edges].
        num_nodes (int): the number of nodes in the graph.

    Returns:
        torch.Tensor: edge indices in COO format with shape [2, num_edges].
    """

    idx = pos_edge_index[0] * num_nodes + pos_edge_index[1]
    idx = idx.to(torch.device("cpu"))

    perm = torch.tensor(np.random.choice(num_nodes**2, idx.size(0)))
    mask = torch.from_numpy(np.isin(perm, idx).astype(np.uint8))
    rest = mask.nonzero().view(-1)
    while rest.numel() > 0:  # pragma: no cover
        tmp = torch.tensor(np.random.choice(num_nodes**2, rest.size(0)))
        mask = torch.from_numpy(np.isin(tmp, idx).astype(np.uint8))
        perm[rest] = tmp
        rest = mask.nonzero().view(-1)

    row, col = perm / num_nodes, perm % num_nodes
    return torch.stack([row, col], dim=0).long().to(pos_edge_index.device)


def typed_negative_sampling(pos_edge_index: torch.Tensor, num_nodes: int, range_list: torch.Tensor) -> torch.Tensor:
    r"""
    Typed negative sampling for link prediction. Copy-paste from https://github.com/NYXFLOWER/GripNet.

    Args:
        pos_edge_index (torch.Tensor): edge indices in COO format with shape [2, num_edges].
        num_nodes (int): the number of nodes in the graph.
        range_list (torch.Tensor): the range of edge types. [[start_index, end_index], ...]

    Returns:
        torch.Tensor: edge indices in COO format with shape [2, num_edges].
    """

    tmp = []
    for start, end in range_list:
        tmp.append(negative_sampling(pos_edge_index[:, start:end], num_nodes))
    return torch.cat(tmp, dim=1)
</file>

<file path="kale/prepdata/image_transform.py">
"""
Preprocessing of image datasets, i.e., transforms, from
https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/preprocessing.py

References for processing stacked images:
    Swift, A. J., Lu, H., Uthoff, J., Garg, P., Cogliano, M., Taylor, J., ... & Kiely, D. G. (2020). A machine
    learning cardiac magnetic resonance approach to extract disease features and automate pulmonary arterial
    hypertension diagnosis. European Heart Journal-Cardiovascular Imaging.
"""
import logging

import numpy as np
import torchvision.transforms as transforms
from skimage.transform import estimate_transform, rescale, warp


def get_transform(kind, augment=False):
    """
    Define transforms (for commonly used datasets)

    Args:
        kind ([type]): the dataset (transformation) name
        augment (bool, optional): whether to do data augmentation (random crop and flipping). Defaults to False.
            (Not implemented for digits yet.)

    """
    if kind == "mnist32":
        transform = transforms.Compose(
            [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        )
    elif kind == "mnist32rgb":
        transform = transforms.Compose(
            [
                transforms.Resize(32),
                transforms.Grayscale(3),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )
    elif kind == "usps32":
        transform = transforms.Compose(
            [transforms.ToPILImage(), transforms.Resize(32), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        )
    elif kind == "usps32rgb":
        transform = transforms.Compose(
            [
                transforms.ToPILImage(),
                transforms.Resize(32),
                transforms.Grayscale(3),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )
    elif kind == "mnistm":
        transform = transforms.Compose(
            [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
        )
    elif kind == "svhn":
        transform = transforms.Compose(
            [transforms.Resize(32), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
        )
    elif kind == "cifar":
        if augment:
            transform_aug = transforms.Compose(
                [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]
            )
        else:
            transform_aug = transforms.Compose([])
        transform = transforms.Compose(
            [
                transform_aug,
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            ]
        )
    elif kind == "office":
        if augment:
            transform_aug = transforms.Compose(
                [transforms.Resize(256), transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()]
            )
        else:
            transform_aug = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256)])
        transform = transforms.Compose(
            [
                transform_aug,
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )
    elif kind == "few-shot":
        # The image size is fixed to 224x224. If needed, we can add an argument about image_size.
        transform = transforms.Compose(
            [
                transforms.Resize(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )
    else:
        raise ValueError(f"Unknown transform kind '{kind}'")
    return transform


def reg_img_stack(images, coords, target_coords):
    """Registration for stacked images

    Args:
        images (list): Input data, where each sample in shape (n_phases, dim1, dim2).
        coords (array-like): Coordinates for registration, shape (n_samples, n_landmarks * 2).
        target_coords (array-like): Target coordinates for registration.

    Returns:
        list: Registered images, each sample in the list in shape (n_phases, dim1, dim2).
        array-like: Maximum distance of transformed source coordinates to destination coordinates, shape (n_samples,)
    """
    n_samples = len(images)
    if n_samples != coords.shape[0]:
        error_msg = "The sample size of images and coordinates does not match."
        logging.error(error_msg)
        raise ValueError(error_msg)
    n_landmarks = int(coords.shape[1] / 2)

    target_coords = target_coords.reshape((n_landmarks, 2))
    max_dist = np.zeros(n_samples)
    for i in range(n_samples):
        src_coord = coords[i, :]
        src_coord = src_coord.reshape((n_landmarks, 2))
        idx_valid = np.isnan(src_coord[:, 0])
        tform = estimate_transform(ttype="similarity", src=src_coord[~idx_valid, :], dst=target_coords[~idx_valid, :])
        # forward transform used here, inverse transform used for warp
        src_tform = tform(src_coord[~idx_valid, :])
        dists = np.linalg.norm(src_tform - target_coords[~idx_valid, :], axis=1)
        max_dist[i] = np.max(dists)
        n_phases = images[i].shape[0]
        for j in range(n_phases):
            src_img = images[i][j, ...].copy()
            warped = warp(src_img, inverse_map=tform.inverse, preserve_range=True)
            images[i][j, ...] = warped

    return images, max_dist


def rescale_img_stack(images, scale=0.5):
    """Rescale stacked images by a given factor

    Args:
        images (list): Input data list, where each sample in shape (n_phases, dim1, dim2).
        scale (float, optional): Scale factor. Defaults to 0.5.

    Returns:
        list: Rescaled images, each sample in the list in shape (n_phases, dim1 * scale, dim2 * scale).
    """
    n_samples = len(images)
    # n_phases = images.shape[:2]
    images_rescale = []
    for i in range(n_samples):
        stack_i = []
        n_phases = images[i].shape[0]
        for j in range(n_phases):
            img = images[i][j, ...]
            img_rescale = rescale(img, scale, preserve_range=True)
            # preserve_range should be true otherwise the output will be normalised values
            stack_i.append(img_rescale.reshape((1,) + img_rescale.shape))
        stack_i = np.concatenate(stack_i, axis=0)
        images_rescale.append(stack_i)

    return images_rescale


def mask_img_stack(images, mask):
    """Masking stacked images by a given mask

    Args:
        images (list): Input image data, where each sample in shape (n_phases, dim1, dim2).
        mask (array-like): mask, shape (dim1, dim2).
    Returns:
        list: masked images, each sample in the list in shape (n_phases, dim1, dim2).
    """
    n_samples = len(images)
    for i in range(n_samples):
        n_phases = images[i].shape[0]
        for j in range(n_phases):
            images[i][j, ...] = np.multiply(images[i][j, ...], mask)

    return images


def normalize_img_stack(images):
    """Normalize pixel values to (0, 1) for stacked images.

    Args:
        images (list): Input data, where each sample in shape (n_phases, dim1, dim2).

    Returns:
        list: Normalized images, each sample in the list in shape (n_phases, dim1, dim2).
    """
    n_samples = len(images)
    for i in range(n_samples):
        n_phases = images[i].shape[0]
        for j in range(n_phases):
            img = images[i][j, ...]
            images[i][j, ...] = (img - np.min(img)) / (np.max(img) - np.min(img))

    return images
</file>

<file path="kale/prepdata/README.md">
# Preprocessing modules

Preprocessing of data for learning and prediction, e.g. registration for images, graph construction.
</file>

<file path="kale/prepdata/string_transform.py">
"""Author: Lawrence Schobs, lawrenceschobs@gmail.com
    This file contains functions for string manipulation.
"""


def strip_for_bound(string_: str) -> list:
    """
    Convert a string containing comma-separated floats into a list of floats.
    Args:
        string_ (str): A string containing floats, separated by commas.
    Returns:
        list: A list of floats.
    Example:
        >>> strip_for_bound("[1.0, 2.0], [3.0, 4.0]")
        [[1.0, 2.0], [3.0, 4.0]]
    """
    bounds = []
    for entry in string_:
        entry = entry[1:-1]
        bounds.append([float(i) for i in entry.split(",")])
    return bounds
</file>

<file path="kale/prepdata/supergraph_construct.py">
# =============================================================================
# Author: Hao Xu, xu-hao@ucsd.edu or nyx0flower@gmail.com
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# =============================================================================

"""
The supergraph structure from the Pattern Recognition 2022 paper "GripNet: Graph Information Propagation on Supergraph
for Heterogeneous Graphs" <https://doi.org/10.1016/j.patcog.2022.108973>.
"""

import logging
from typing import Dict, List, Optional

import networkx as nx
import torch


class SuperVertex(object):
    r"""
    The supervertex structure in GripNet. Each supervertex is a subgraph containing nodes of the same category
        that are semantically-coherent. Supervertices can be homogeneous or heterogeneous.

    Args:
        name (str): the name of the supervertex.
        node_feat (torch.Tensor): node features of the supervertex with shape [#nodes, #features]. We recommend
            using `torch.sparse.FloatTensor()` if the node feature matrix is sparse.
        edge_index (torch.Tensor): edge indices in COO format with shape [2, #edges].
        edge_type (torch.Tensor, optional): one-dimensional relation type for each edge, indexed from 0. Defaults to None.
        edge_weight (torch.Tensor, optional): one-dimensional weight for each edge. Defaults to None.

    Examples:
        >>> import torch
        >>> node_feat = torch.randn(4, 20)
        >>> edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 0]])
        >>> edge_type = torch.tensor([0, 0, 1, 1])
        >>> edge_weight = torch.randn(4)
        >>> # create a supervertex with homogeneous edges
        >>> supervertex_homo = SuperVertex(node_feat, edge_index)
        >>> # create a supervertex with heterogeneous edges
        >>> supervertex_hete = SuperVertex(node_feat, edge_index, edge_type)
        >>> # create a supervertex with weighted edges
        >>> supervertex_weight1 = SuperVertex(node_feat, edge_index, edge_weight=edge_weight)
        >>> supervertex_weight2 = SuperVertex(node_feat, edge_index, edge_type, edge_weight)
    """

    def __init__(
        self,
        name: str,
        node_feat: torch.Tensor,
        edge_index: torch.Tensor,
        edge_type: torch.Tensor = None,
        edge_weight: torch.Tensor = None,
    ) -> None:
        self.name = name
        self.node_feat = node_feat
        self.edge_index = edge_index
        self.edge_type = edge_type
        self.edge_weight = edge_weight

        # get the number of nodes, node features and edges
        self.num_node, self.num_node_feat = node_feat.shape
        self.num_edge = edge_index.shape[1]

        # initialize in-supervertex and out-supervertex lists
        self.in_supervertex_list: List[str] = []
        self.out_supervertex_list: List[str] = []
        self.start_supervertex = True

        self.__process_edges__()

    def __process_edges__(self):
        r"""
        process the edges of the supervertex.
        """
        # get the number of edge types
        if self.edge_type is None:
            self.num_edge_type = 1
        else:
            unique_edge_type = self.edge_type.unique()
            self.num_edge_type = unique_edge_type.shape[0]

            # check whether the index of edge type is continuous and starts from 0
            if self.num_edge_type != unique_edge_type.max() + 1:
                error_msg = "`edge_type` is not set correctly. It should be continuous and start from 0."
                logging.error(error_msg)
                raise ValueError(error_msg)

            # sort the edges and edge types
            sort_index = torch.argsort(self.edge_type)
            self.edge_index = self.edge_index[:, sort_index]
            self.edge_type = self.edge_type[sort_index]

            self.__get_range_list__()

    def __get_range_list__(self):
        """get the range of edge types"""
        idx = 0
        range_list = [[0, 0]]

        for i, edge_type in enumerate(self.edge_type):
            if edge_type != idx:
                idx = edge_type
                range_list[-1][1] = i
                range_list.append([i, i])

        range_list[-1][1] = i + 1

        self.range_list = torch.tensor(range_list)

    def __repr__(self) -> str:
        return (
            f"SuperVertex(\n    name={self.name}, \n    node_feat={self.node_feat.shape}, \n    "
            f"edge_index={self.edge_index.shape}, \n    num_edge_type={self.num_edge_type})"
        )

    def add_in_supervertex(self, vertex_name: str):
        self.in_supervertex_list.append(vertex_name)

    def add_out_supervertex(self, vertex_name: str):
        self.out_supervertex_list.append(vertex_name)


class SuperEdge(object):
    r"""
    The superedge structure in GripNet. Each superedge is a bipartite subgraph containing nodes from two categories
    forming two node sets, connected by edges between them. A superedge can be regarded as a heterogeneous graph
    connecting two supervertices.

    Args:
        source_supervertex (str): the name of the source supervertex.
        target_supervertex (str): the name of the target supervertex.
        edge_index (torch.Tensor): edge indices in COO format with shape [2, #edges]. The first row is the index of
            source nodes, and the second row is the index of target nodes.
        edge_weight (torch.Tensor, optional): one-dimensional weight for each edge. Defaults to None.
    """

    def __init__(
        self,
        source_supervertex: str,
        target_supervertex: str,
        edge_index: torch.Tensor,
        edge_weight: torch.Tensor = None,
    ) -> None:
        self.direction = (source_supervertex, target_supervertex)
        self.source_supervertex = source_supervertex
        self.target_supervertex = target_supervertex
        self.edge_index = edge_index
        self.edge_weight = edge_weight

    def __repr__(self) -> str:
        return (
            f"SuperEdges(\n    edge_direction={self.source_supervertex}->{self.target_supervertex}, "
            f"\n    edge_index={self.edge_index.shape})"
        )


class SuperVertexParaSetting(object):
    r"""Parameter settings for each supervertex.

    Args:
        supervertex_name (str): the name of the supervertex.
        inter_feat_channels (int): the dimension of the output of the internal feature layer.
        inter_agg_channels_list (List[int]): the output dimensions of a sequence of internal aggregation layers.
        exter_agg_channels_dict (Dict[str, int], optional): the dimension of received message vector from parent supervertices.
            Defaults to None.
        mode (str, optional): the allowed gripnet mode--'cat' or 'add'. Defaults to None.
        num_bases (int, optional): the number of bases used for basis-decomposition if the
            supervertex is multi-relational. Defaults to 32.
        concat_output (bool, optional): whether to concatenate the output of each layers. Defaults to True.
    """

    def __init__(
        self,
        supervertex_name: str,
        inter_feat_channels: int,
        inter_agg_channels_list: List[int],
        exter_agg_channels_dict: Optional[Dict[str, int]] = None,
        mode: Optional[str] = None,
        num_bases: int = 32,
        concat_output: bool = True,
    ) -> None:
        self.supervertex_name = supervertex_name
        self.inter_feat_channels = inter_feat_channels
        self.inter_agg_channels_list = inter_agg_channels_list
        self.mode = mode
        self.num_bases = num_bases
        self.concat_output = concat_output
        self.exter_agg_channels_dict = exter_agg_channels_dict

        # check whether the mode is valid
        if self.mode is not None and self.mode not in ["cat", "add"]:
            error_msg = "`mode` value is invalid. It should be 'cat' or 'add'."
            logging.error(error_msg)
            raise ValueError(error_msg)


class SuperGraph(object):
    r"""
    The supergraph structure in GripNet. Each supergraph is a directed acyclic graph (DAG) containing
    supervertices and superedges.

    Args:
        supervertex_list (list[SuperVertex]): a list of supervertices.
        superedge_list (list[SuperEdge]): a list of superedges.
        supervertex_para_setting (dict[str, SuperVertexParaSetting], Optional): the parameter settings for each supervertex.
    """

    def __init__(
        self,
        supervertex_list: List[SuperVertex],
        superedge_list: List[SuperEdge],
        supervertex_setting_dict: Optional[Dict[str, SuperVertexParaSetting]] = None,
    ) -> None:
        self.supervertex_dict = {supervertex.name: supervertex for supervertex in supervertex_list}
        self.superedge_dict = {superedge.direction: superedge for superedge in superedge_list}
        self.supervertex_setting_dict = supervertex_setting_dict

        self.__process_supergraph__()
        self.__update_supervertex__()

    def __process_supergraph__(self):
        r"""
        Process the graph of the supergraph.
        """
        # initialize the supergraph
        self.G = nx.DiGraph()
        self.G.add_edges_from(self.superedge_dict.keys())

        # check whether the graph is a DAG
        if not nx.is_directed_acyclic_graph(self.G):
            error_msg = "The supergraph is not a directed acyclic graph."
            logging.error(error_msg)
            raise TypeError(error_msg)

        self.num_supervertex = self.G.number_of_nodes()
        self.num_superedge = self.G.number_of_edges()

        # get the topological order of the supergraph
        self.topological_order = list(nx.topological_sort(self.G))

    def __update_supervertex__(self):
        r"""
        Update the supervertices according to the superedges of the supergraph.
        """
        # update the in- and out-supervertex lists of each supervertex
        for n1, n2 in self.G.edges():
            self.supervertex_dict[n2].add_in_supervertex(n1)
            self.supervertex_dict[n1].add_out_supervertex(n2)

        # update if the supervertex is a start supervertex of the supergraph
        for _, sv in self.supervertex_dict.items():
            if sv.in_supervertex_list:
                sv.start_supervertex = False

    def set_supergraph_para_setting(self, supervertex_setting_list: List[SuperVertexParaSetting]):
        """Set the parameters of the supergraph.

        Args:
            supervertex_setting_list (list[SuperVertexParaSetting]): a list of parameter settings for each supervertex.
        """
        self.supervertex_setting_dict = {sv.supervertex_name: sv for sv in supervertex_setting_list}

    def __repr__(self) -> str:
        return (
            f"SuperGraph(\n  svertex_dict={self.supervertex_dict.values()}, "
            f"\n  sedge_dict={self.superedge_dict.values()}, \n  G={self.G}), "
            f"\n  topological_order={self.topological_order}"
        )
</file>

<file path="kale/prepdata/tabular_transform.py">
"""
Functions for manipulating/transforming tabular data
"""
# =============================================================================
# Author: Sina Tabakhi, sina.tabakhi@gmail.com
#         Lawrance Schobs, lawrenceschobs@gmail.com
# =============================================================================

import os
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import torch
import torch.nn.functional as F
from torch import Tensor


class ToTensor(object):
    r"""Convert an array_like data to a tensor of the same shape. This class provides a callable object that allows
    instances of the class to be called as a function. In other words, this class wraps the functionality of
    `torch.tensor <https://pytorch.org/docs/stable/generated/torch.tensor.html>`__ and allows users to use it as a
    callable instance.

    Args:
        dtype (torch.dtype, optional): The desired data type of returned tensor. Default: if :obj:`None`, infers data
            type from data.
        device (torch.device, optional): The device of the constructed tensor. If :obj:`None` and data is a tensor then
            the device of data is used. If None and data is not a tensor then the result tensor is constructed on the
            CPU.
    """

    def __init__(self, dtype: Optional[torch.dtype] = None, device: Optional[torch.device] = None) -> None:
        self.dtype = dtype
        self.device = device

    def __call__(self, data: Any) -> Tensor:
        """
        Args:
            data (array_like): Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other
                types.

        Returns:
            Data converted to tensor.
        """
        return torch.tensor(data, dtype=self.dtype, device=self.device)


class ToOneHotEncoding(object):
    r"""Convert an array_like of class values of shape ``(*,)`` to a tensor of shape ``(*, num_classes)`` that have
    zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in
    which case it will be 1.

    Note that this class provides a callable object that allows instances of the class to be called as a function. In
    other words, this class wraps the functionality of the one_hot method in the
    `PyTorch <https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html>`__ and allows users to use it
     as a callable instance.

    Args:
        num_classes (int, optional): Total number of classes. If set to -1, the number of classes will be inferred as
            one greater than the largest class value in the input data.
        dtype (torch.dtype, optional): The desired data type of returned tensor. Default: if :obj:`None`, infers data
            type from data.
        device (torch.device, optional): The device of the constructed tensor. If :obj:`None` and data is a tensor then
            the device of data is used. If None and data is not a tensor then the result tensor is constructed on the
            CPU.
    """

    def __init__(
        self,
        num_classes: Optional[int] = -1,
        dtype: Optional[torch.dtype] = None,
        device: Optional[torch.device] = None,
    ) -> None:
        self.num_classes = num_classes
        self.dtype = dtype
        self.device = device

    def __call__(self, data: Any) -> Tensor:
        """
        Args:
            data (list): Class values of any shape.

        Returns:
            torch.Tensor: The constructed tensor that has one more dimension with 1 values at the index of last
                dimension indicated by the input, and 0 everywhere else.
        """
        data_tensor = torch.tensor(data, dtype=torch.long)
        data_tensor = F.one_hot(data_tensor, num_classes=self.num_classes)
        return torch.tensor(data_tensor, dtype=self.dtype, device=self.device)


def apply_confidence_inversion(data: pd.DataFrame, uncertainty_measure: str) -> Tuple[Any, Any]:
    """Invert a list of numbers, add a small number to avoid division by zero.

    Args:
        data (Dict): Dictionary of data to invert.
        uncertainty_measure (str): Key of dict to invert.

    Returns:
        Dict: Dictionary with inverted data.
    """

    if uncertainty_measure not in data:
        raise KeyError("The key %s not in the dictionary provided" % uncertainty_measure)

    # Make sure no value is less than zero.
    min_not_zero = min(i for i in data[uncertainty_measure] if i > 0)
    data.loc[data[uncertainty_measure] < 0, uncertainty_measure] = min_not_zero
    data[uncertainty_measure] = 1 / data[uncertainty_measure] + 0.0000000000001
    return data


def generate_struct_for_qbin(
    models_to_compare: List[str], targets: List[int], saved_bins_path_pre: str, dataset: str
) -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:
    """
    Returns dictionaries of pandas dataframes for:
        a) all error and prediction info (all prediction data across targets for each model),
        b) target indices for separated error and prediction info (prediction data for each model and each target),
        c) all estimated error bounds (estimated error bounds across targets for each model),
        d) target separated estimated error bounds (estimated error bounds for each model and each target).

    Args:
        models_to_compare: List of set models to add to data struct.
        targets: List of targets to add to data struct.
        saved_bins_path_pre: Preamble to path of where the predicted quantile bins are saved.
        dataset: String of what dataset you're measuring.

    Returns:
        data_structs: Dictionary where keys are model names and values are pandas dataframes containing
                      all prediction data across targets for that model.

        data_struct_sep: Dictionary where keys are a combination of model names and target indices (e.g., "model1 T1"),
                         and values are pandas dataframes containing prediction data for the corresponding model and target.

        data_struct_bounds: Dictionary where keys are a combination of model names and the string " Error Bounds"
                            (e.g., "model1 Error Bounds"), and values are pandas dataframes containing all estimated
                            error bounds across targets for that model.

        data_struct_bounds_sep: Dictionary where keys are a combination of model names, target indices and the string
                                "Error Bounds" (e.g., "model1 Error Bounds L1"), and values are pandas dataframes containing
                                estimated error bounds for the corresponding model and target.
    """
    data_structs = {}
    data_struct_sep = {}  #

    data_struct_bounds = {}
    data_struct_bounds_sep = {}

    for model in models_to_compare:
        all_targets = []
        all_err_bounds = []

        for target_idx in targets:
            bin_pred_path = os.path.join(saved_bins_path_pre, model, dataset, "res_predicted_bins_t" + str(target_idx))
            bin_preds = pd.read_csv(bin_pred_path + ".csv", header=0)
            bin_preds["target_idx"] = target_idx

            error_bounds_path = os.path.join(
                saved_bins_path_pre, model, dataset, "estimated_error_bounds_t" + str(target_idx)
            )
            error_bounds_pred = pd.read_csv(error_bounds_path + ".csv", header=0)
            error_bounds_pred["target"] = target_idx

            all_targets.append(bin_preds)
            all_err_bounds.append(error_bounds_pred)
            data_struct_sep[model + " L" + str(target_idx)] = bin_preds
            data_struct_bounds_sep[model + "Error Bounds L" + str(target_idx)] = error_bounds_pred

        data_structs[model] = pd.concat(all_targets, axis=0, ignore_index=True)
        data_struct_bounds[model + " Error Bounds"] = pd.concat(all_err_bounds, axis=0, ignore_index=True)

    return data_structs, data_struct_sep, data_struct_bounds, data_struct_bounds_sep
</file>

<file path="kale/prepdata/tensor_reshape.py">
import torch

# dimension locations in a typical image batch tensor
SPATIAL_BATCH_DIMENSION = 0
SPATIAL_CHANNEL_DIMENSION = 1
SPATIAL_HEIGHT_DIMENSION = 2
SPATIAL_WIDTH_DIMENSION = 3

NUMBER_OF_DIMENSIONS = 4


def spatial_to_seq(image_tensor: torch.Tensor):
    """
    Takes a torch tensor of shape (batch_size, channels, height, width)
    as used and outputted by CNNs and creates a sequence view of shape
    (sequence_length, batch_size, channels) as required by
    torch's transformer module. In other words, unrolls the
    spatial grid into the sequence length and rearranges the
    dimension ordering.

    Args:
        image_tensor: tensor of shape (batch_size, channels, height, width) (required).
    """
    original_size = image_tensor.size()

    batch_size = original_size[SPATIAL_BATCH_DIMENSION]
    num_channels = original_size[SPATIAL_CHANNEL_DIMENSION]
    spatial_height = original_size[SPATIAL_HEIGHT_DIMENSION]
    spatial_width = original_size[SPATIAL_WIDTH_DIMENSION]

    permuted_tensor = image_tensor.permute(
        SPATIAL_HEIGHT_DIMENSION, SPATIAL_WIDTH_DIMENSION, SPATIAL_BATCH_DIMENSION, SPATIAL_CHANNEL_DIMENSION
    )

    sequence_tensor = permuted_tensor.view(spatial_height * spatial_width, batch_size, num_channels)

    return sequence_tensor


# dimension locations in a typical Transformer sequence batch tensor
SEQUENCE_LENGTH_DIMENSION = 0
SEQUENCE_BATCH_DIMENSION = 1
SEQUENCE_FEATURE_DIMENSION = 2

SEQUENCE_NUMBER_OF_DIMENSIONS = 3


def seq_to_spatial(sequence_tensor: torch.Tensor, desired_height: int, desired_width: int):
    """Takes a torch tensor of shape (sequence_length, batch_size, num_features)
    as used and outputted by Transformers and creates a view of shape
    (batch_size, num_features, height, width) as used and outputted by CNNs.
    In other words, rearranges the dimension ordering and rolls
    sequence_length into (height,width). height*width must equal
    the sequence length of the input sequence.

    Args:
        sequence_tensor: sequence tensor of shape (sequence_length, batch_size, num_features) (required).
        desired_height: the height into which the sequence length should be rolled into (required).
        desired_width: the width into which the sequence length should be rolled into (required).

    """
    original_size = sequence_tensor.size()

    batch_size = original_size[SEQUENCE_BATCH_DIMENSION]
    num_channels = original_size[SEQUENCE_FEATURE_DIMENSION]

    permuted_tensor = sequence_tensor.permute(
        SEQUENCE_BATCH_DIMENSION, SEQUENCE_FEATURE_DIMENSION, SEQUENCE_LENGTH_DIMENSION
    )

    spatial_tensor = permuted_tensor.view(batch_size, num_channels, desired_height, desired_width)

    return spatial_tensor
</file>

<file path="kale/prepdata/video_transform.py">
import torch
from torchvision import transforms


def get_transform(kind, image_modality):
    """
    Define transforms (for commonly used datasets)

    Args:
        kind ([type]): the dataset (transformation) name
        image_modality (string): image type (RGB or Optical Flow)
    """

    if kind in ["epic", "gtea", "adl", "kitchen"]:
        transform = dict()
        if image_modality == "rgb":
            transform = {
                "train": transforms.Compose(
                    [
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        # transforms.CenterCrop(size=224),
                        transforms.RandomCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
                "valid": transforms.Compose(
                    [
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        transforms.CenterCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
                "test": transforms.Compose(
                    [
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        transforms.CenterCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
            }
        elif image_modality == "flow":
            transform = {
                "train": transforms.Compose(
                    [
                        # Stack(),
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        transforms.RandomCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
                "valid": transforms.Compose(
                    [
                        # Stack(),
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        transforms.CenterCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
                "test": transforms.Compose(
                    [
                        # Stack(),
                        ImglistToTensor(),
                        transforms.Resize(size=256),
                        transforms.CenterCrop(size=224),
                        transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5]),
                        TensorPermute(),
                    ]
                ),
            }
        else:
            raise ValueError("Input modality is not in [rgb, flow, joint]. Current is {}".format(image_modality))
    else:
        raise ValueError(f"Unknown transform kind '{kind}'")
    return transform


class ImglistToTensor(torch.nn.Module):
    """
    Converts a list of PIL images in the range [0,255] to a torch.FloatTensor
    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1].
    Can be used as first transform for ``kale.loaddata.videos.VideoFrameDataset``.
    """

    def forward(self, img_list):
        """
        For RGB input, converts each PIL image in a list to a torch Tensor and stacks them into a single tensor.
        For flow input, converts every two PIL images (x(u)_img, y(v)_img) in a list to a torch Tensor and stacks them.
        For example, if input list size is 16, the dimension is [16, 1, 224, 224] and the frame order is
        [frame 1_x, frame 1_y, frame 2_x, frame 2_y, frame 3_x, ..., frame 8_x, frame 8_y]. The output will be
        [[frame 1_x, frame 1_y], [frame 2_x, frame 2_y], [frame 3_x, ..., [frame 8_x, frame 8_y]] and the dimension is
        [8, 2, 224, 224].

        Args:
            img_list: list of PIL images.
        Returns:
            tensor of size `` NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``
        """
        if img_list[0].mode == "RGB":
            return torch.stack([transforms.functional.to_tensor(pic) for pic in img_list])
        elif img_list[0].mode == "L":
            it = iter([transforms.functional.to_tensor(pic) for pic in img_list])
            return torch.stack([torch.cat((i, next(it)), dim=0) for i in it])
        else:
            raise RuntimeError("Image modality is not in [rgb, flow].")


class TensorPermute(torch.nn.Module):
    """
    Convert a torch.FloatTensor of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) to
    a torch.FloatTensor of shape (CHANNELS x NUM_IMAGES x HEIGHT x WIDTH).
    """

    def forward(self, tensor):
        return tensor.permute(1, 0, 2, 3).contiguous()
</file>

<file path="kale/utils/download.py">
# ===============================================================================
# Author: Xianyuan Liu, xianyuan.liu@outlook.com
#         Raivo Koot, rekoot1@sheffield.ac.uk
#         Haiping Lu, h.lu@sheffield.ac.uk or hplu@ieee.org
# ===============================================================================

"""Data downloading and compressed data extraction functions, Based on
https://github.com/pytorch/vision/blob/master/torchvision/datasets/utils.py
https://github.com/pytorch/pytorch/blob/master/torch/hub.py
"""

import logging
import os
from pathlib import Path

from torch.hub import download_url_to_file
from torchvision.datasets.utils import download_and_extract_archive, download_file_from_google_drive, extract_archive


def download_file_by_url(url, output_directory, output_file_name, file_format=None):
    """Download file/compressed file by url.

    Args:
        url (string): URL of the object to download
        output_directory (string, optional): Full path where object will be saved
                                             Abosolute path recommended. Relative path also works.
        output_file_name (string, optional): File name which object will be saved as
        file_format (string, optional): File format
                                For compressed file, support ["tar.xz", "tar", "tar.gz", "tgz", "gz", "zip"]

    Example: (Grab the raw link from GitHub. Notice that using "raw" in the URL.)
        >>> url = "https://github.com/pykale/data/raw/main/videos/video_test_data/ADL/annotations/labels_train_test/adl_P_04_train.pkl"
        >>> download_file_by_url(url, "data", "a.pkl", "pkl")

        >>> url = "https://github.com/pykale/data/raw/main/videos/video_test_data.zip"
        >>> download_file_by_url(url, "data", "video_test_data.zip", "zip")

    """

    output_directory = Path(output_directory).absolute()
    file = Path(output_directory).joinpath(output_file_name)

    if os.path.exists(file):
        logging.info("Skipping Download and Extraction")

        return
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    if file_format in ["tar.xz", "tar", "tar.gz", "tgz", "gz", "zip"]:
        logging.info("Downloading and extracting {}.".format(output_file_name))

        download_and_extract_archive(url=url, download_root=output_directory, filename=output_file_name)
        logging.info("Datasets downloaded and extracted in {}".format(file))
    else:
        logging.info("Downloading {}.".format(output_file_name))
        download_url_to_file(url, file)
        logging.info("Datasets downloaded in {}".format(file))


def download_file_gdrive(id, output_directory, output_file_name, file_format=None):
    """Download file/compressed file by Google Drive id.

    Args:
        id (string): Google Drive file id of the object to download
        output_directory (string, optional): Full path where object will be saved
                                             Abosolute path recommended. Relative path also works.
        output_file_name (string, optional): File name which object will be saved as
        file_format (string, optional): File format
                                For compressed file, support ["tar.xz", "tar", "tar.gz", "tgz", "gz", "zip"]

    Example:
        >>> gdrive_id = "1U4D23R8u8MJX9KVKb92bZZX-tbpKWtga"
        >>> download_file_gdrive(gdrive_id, "data", "demo_datasets.zip", "zip")

        >>> gdrive_id = "1SV7fmAnWj-6AU9X5BGOrvGMoh2Gu9Nih"
        >>> download_file_gdrive(gdrive_id, "data", "dummy_data.csv", "csv")
    """

    output_directory = Path(output_directory).absolute()
    file = Path(output_directory).joinpath(output_file_name)
    if os.path.exists(file):
        logging.info("Skipping Download and Extraction")
        return
    os.makedirs(output_directory, exist_ok=True)

    logging.info("Downloading {}.".format(output_file_name))
    download_file_from_google_drive(id, output_directory, output_file_name)

    if file_format is not None and file_format in ["tar.xz", "tar", "tar.gz", "tgz", "gz", "zip"]:
        logging.info("Extracting {}.".format(output_file_name))
        extract_archive(file.as_posix())
        logging.info("Datasets downloaded and extracted in {}".format(file))
    else:
        logging.info("Datasets downloaded in {}".format(file))
</file>

<file path="kale/utils/initialize_nn.py">
# =============================================================================
# Author: Sina Tabakhi, sina.tabakhi@gmail.com
# =============================================================================

"""
Provide methods for initializing neural network parameters (i.e., weights and biases).
"""

import torch.nn as nn


def xavier_init(module) -> None:
    r"""Fills the weight of the input Tensor with values using a normal distribution.

    Args:
        module (torch.Tensor): The input module.
    """
    if isinstance(module, nn.Linear):
        nn.init.xavier_normal_(module.weight)


def bias_init(module) -> None:
    r"""Fills the bias of the input Tensor with zeros.

    Args:
        module (torch.Tensor): The input module.
    """
    if isinstance(module, nn.Linear) and module.bias is not None:
        module.bias.data.fill_(0.0)
</file>

<file path="kale/utils/logger.py">
"""Logging functions, based on https://github.com/HaozhiQi/ISONet/blob/master/isonet/utils/logger.py"""

import datetime
import logging
import os
import uuid


def out_file_core():
    """Creates an output file name concatenating a formatted date and uuid, but without an extension.

    Returns:
        string: A string to be used in a file name.
    """
    date = str(datetime.datetime.now().strftime("%Y%d%m_%H%M%S"))
    return f"log-{date}-{str(uuid.uuid4())}"


def construct_logger(name, save_dir, log_to_terminal=False):
    """Constructs a logger. Saves the output as a text file at a specified path.
    Also saves the output of `git diff HEAD` to the same folder.
    Takes option to log to terminal, which will print logging statements. Default is False.

    The logger is configured to output messages at the DEBUG level, and it saves the output as a text file with a name
    based on the current timestamp and the specified name. It also saves the output of `git diff HEAD` to a file with
    the same name and the extension `.gitdiff.patch`.

    Args:
        name (str): The name of the logger, typically the name of the method being logged.
        save_dir (str): The directory where the log file and git diff file will be saved.
        log_to_terminal (bool, optional): Whether to also log messages to the terminal. Defaults to False.

    Returns:
        logging.Logger: The constructed logger.

    Reference:
        https://docs.python.org/3/library/logging.html

    Raises:
        None.
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    file_no_ext = out_file_core()

    fh = logging.FileHandler(os.path.join(save_dir, file_no_ext + ".txt"), encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    formatter = logging.Formatter("%(asctime)s %(name)s %(levelname)s: %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    gitdiff_patch = os.path.join(save_dir, file_no_ext + ".gitdiff.patch")
    os.system(f"git diff HEAD > {gitdiff_patch}")

    if log_to_terminal:
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    return logger
</file>

<file path="kale/utils/print.py">
"""Screen printing functions, from https://github.com/HaozhiQi/ISONet/blob/master/isonet/utils/misc.py"""


def tprint(*args):
    """Temporarily prints things on the screen so that it won't be flooded"""
    print("\r", end="")  # noqa: T201
    print(*args, end="")  # noqa: T201


def pprint(*args):  # noqa: T204
    """Permanently prints things on the screen to have all info displayed"""
    print("\r", end="")  # noqa: T201
    print(*args)  # noqa: T201


def pprint_without_newline(*args):
    """Permanently prints things on the screen, separated by space rather than newline"""
    print("\r", end="")  # noqa: T201
    print(*args, end=" ")  # noqa: T201
</file>

<file path="kale/utils/README.md">
# Modules for utilities

Print, log, seed, etc.
</file>

<file path="kale/utils/save_xlsx.py">
"""
Authors: Lawrence Schobs, lawrenceschobs@gmail.com

Functions to save results to Excel files.

"""

from typing import Any, Dict

import numpy as np
import pandas as pd


def generate_summary_df(
    results_dictionary: dict, cols_to_save: list, sheet_name: str, save_location: str
) -> pd.DataFrame:
    """
    Generates pandas dataframe with summary statistics.
    Designed for use in Quantile Binning (/pykale/examples/landmark_uncertainty/main.py).

    Args:
        results_dictionary (dict): A dictionary containing results for each quantile bin and uncertainty method.
            The keys are strings indicating the name of each uncertainty method.
            The values are dictionaries containing results for each quantile bin.
        cols_to_save (list): A list of 2-element lists, each containing a string indicating the key in the
            results_dictionary and a string indicating the name to use for that column in the output dataframe.
        sheet_name (str): The name of the sheet to create in the output Excel file.
        save_location (str): The file path and name to use for the output Excel file.
    Returns:
        pd.DataFrame: A dataframe with statistics including mean error, std error of All and individual
        targets. Also includes the Sucess detection rates (SDR).
        The dataframe should have the following structure:
            df = {
                "All um col_save_name Mean": value,
                "All um col_save_name Std": value,
                "B1 um col_save_name Mean": value,
                "B1 um col_save_name Std": value,
                ...
            }
    """

    # Save data to csv files
    summary_dict = {}
    for [col_dict_key, col_save_name] in cols_to_save:
        for um in results_dictionary[col_dict_key].keys():
            col_data = results_dictionary[col_dict_key][um]
            # Remove instances of None (which were added when the list was empty, rather than nan)
            summary_dict["All " + um + " " + col_save_name + " Mean"] = np.mean(
                [x for sublist in col_data for x in sublist if x is not None]
            )
            summary_dict["All " + um + " " + col_save_name + " Std"] = np.std(
                [x for sublist in col_data for x in sublist if x is not None]
            )
            for bin_idx, bin_data in enumerate(col_data):
                filtered_bin_data = [x for x in bin_data if x is not None]
                summ_mean = np.mean(filtered_bin_data) if (len(filtered_bin_data) > 0) else None
                summ_std = np.std(filtered_bin_data) if len(filtered_bin_data) > 0 else None

                summary_dict["B" + str(bin_idx + 1) + " " + um + " " + col_save_name + " Mean"] = summ_mean
                summary_dict["B" + str(bin_idx + 1) + " " + um + " " + col_save_name + " Std"] = summ_std

    save_dict_xlsx(summary_dict, save_location, sheet_name)


def save_dict_xlsx(data_dict: Dict[Any, Any], save_location: str, sheet_name: str) -> None:
    """
    Save a dictionary to an Excel file using the XlsxWriter engine.

    Args:
        data_dict (Dict[Any, Any]): The dictionary that needs to be saved to an Excel file. The keys represent the row
                                    index and the values represent the data in the row. If a dictionary value is a list
                                    or a series, each element in the list/series will be a column in the row.

        save_location (str): The location where the Excel file will be saved. This should include the full path and the
                            filename, for example, "/path/to/save/data.xlsx". Overwrites the file if it already exists.

        sheet_name (str): The name of the sheet where the dictionary will be saved in the Excel file.

    Returns:
        None: This function does not return anything. It saves the dictionary as an Excel file at the specified location.
    """
    pd_df = pd.DataFrame.from_dict(data_dict, orient="index")

    with pd.ExcelWriter(save_location, engine="xlsxwriter") as writer:  # pylint: disable=abstract-class-instantiated
        for n, df in (pd_df).items():
            df.to_excel(writer, sheet_name=sheet_name)
</file>

<file path="kale/utils/seed.py">
"""Setting seed for reproducibility"""

import os
import random

import numpy as np
import torch


# Results can be software/hardware-dependent
# Exactly reproduciable results are expected only on the same software and hardware
def set_seed(seed=1000):
    """Sets the seed for generating random numbers to get (as) reproducible (as possible) results.

    The CuDNN options are set according to the official PyTorch guidance on reproducibility:
    https://pytorch.org/docs/stable/notes/randomness.html. Another references are
    https://discuss.pytorch.org/t/difference-between-torch-manual-seed-and-torch-cuda-manual-seed/13848/6
    https://pytorch.org/docs/stable/cuda.html#torch.cuda.manual_seed
    https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/utils.py#L58

    Args:
        seed (int, optional): The desired seed. Defaults to 1000.
    """

    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value
    os.environ["PYTHONHASHSEED"] = str(seed)
    # 2. Set `python` built-in pseudo-random generator at a fixed value
    random.seed(seed)
    # 3. Set `numpy` pseudo-random generator at a fixed value
    np.random.seed(seed)
    # 4. Set `pytorch` pseudo-random generator at a fixed value
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
</file>

<file path="kale/__init__.py">
__version__ = "0.1.2"
</file>

<file path="kale/README.md">
# Modules

We organise modules according to the following pipeline

* `loaddata` Data loading modules to get the input data from somewhere.
* `prepdata` Preprocessing of data for learning and prediction, e.g. registration for images, graph construction
* `embed` Machine learning modules for embedding data into new spaces to learn a new representation, including feature extraction and feature selection
* `predict` Machine learning modules for predicting a desired output
* `evaluate` Performance evaluation using some metrices
* `interpret` Post-prediction interpretation, e.g. via visulisation, for further anlysis
* `pipeline` Machine learning pipelines combining several other modules to solve specific domain problems in medical imaging, graph/networks and computer vision.
</file>

<file path="tests/embed/test_attention_cnn.py">
import numpy.testing as testing
import torch
import torch.nn as nn

from kale.embed.attention_cnn import CNNTransformer
from kale.prepdata.tensor_reshape import seq_to_spatial
from kale.utils.seed import set_seed


def test_shapes():
    set_seed(36)

    CNN_OUT_HEIGHT = 32
    CNN_OUT_WIDTH = 32
    CNN_OUT_CHANNELS = 256

    cnn = nn.Sequential(
        nn.Conv2d(in_channels=3, out_channels=CNN_OUT_CHANNELS, kernel_size=3, padding=1), nn.MaxPool2d(kernel_size=2)
    )
    cnn_transformer = CNNTransformer(
        cnn=cnn,
        cnn_output_shape=(-1, CNN_OUT_CHANNELS, CNN_OUT_HEIGHT, CNN_OUT_WIDTH),
        num_layers=4,
        num_heads=4,
        dim_feedforward=1024,
        dropout=0.1,
        output_type="spatial",
        positional_encoder=None,
    )

    cnn_transformer.eval()

    BATCH_SIZE = 2

    input_batch = torch.randn((BATCH_SIZE, 3, 64, 64))

    out_spatial = cnn_transformer(input_batch)
    cnn_transformer.output_type = "sequence"
    out_seq = cnn_transformer(input_batch)

    assert out_spatial.size() == (BATCH_SIZE, CNN_OUT_CHANNELS, CNN_OUT_HEIGHT, CNN_OUT_WIDTH)
    assert out_seq.size() == (CNN_OUT_HEIGHT * CNN_OUT_WIDTH, BATCH_SIZE, CNN_OUT_CHANNELS)

    # simply reshape them both to have same shape
    out_spatial_2 = seq_to_spatial(out_seq, CNN_OUT_HEIGHT, CNN_OUT_WIDTH)

    testing.assert_almost_equal(out_spatial.detach().numpy(), out_spatial_2.detach().numpy())
</file>

<file path="tests/embed/test_factorization.py">
import os

import numpy as np
import pytest
from numpy import testing
from scipy.io import loadmat
from sklearn.datasets import make_blobs
from sklearn.preprocessing import OneHotEncoder
from tensorly.tenalg import multi_mode_dot

from kale.embed.factorization import MIDA, MPCA

N_COMPS = [1, 50, 100]
VAR_RATIOS = [0.7, 0.95]
relative_tol = 0.00001
baseline_url = "https://github.com/pykale/data/raw/main/videos/gait/mpca_baseline.mat"


@pytest.fixture(scope="module")
def baseline_model(download_path):
    return loadmat(os.path.join(download_path, "baseline.mat"))


@pytest.mark.parametrize("n_components", N_COMPS)
@pytest.mark.parametrize("var_ratio", VAR_RATIOS)
def test_mpca(var_ratio, n_components, gait):
    # basic mpca test, return tensor
    x = gait["fea3D"].transpose((3, 0, 1, 2))
    mpca = MPCA(var_ratio=var_ratio, vectorize=False)
    x_proj = mpca.fit(x).transform(x)

    testing.assert_equal(x_proj.ndim, x.ndim)
    testing.assert_equal(x_proj.shape[0], x.shape[0])
    assert mpca.n_components <= np.prod(x.shape[1:])
    assert n_components < mpca.n_components
    for i in range(1, x.ndim):
        assert x_proj.shape[i] <= x.shape[i]
        testing.assert_equal(mpca.proj_mats[i - 1].shape[1], x.shape[i])

    x_rec = mpca.inverse_transform(x_proj)
    testing.assert_equal(x_rec.shape, x.shape)

    # test return vector
    mpca.set_params(**{"vectorize": True, "n_components": n_components})

    x_proj = mpca.transform(x)
    testing.assert_equal(x_proj.ndim, 2)
    testing.assert_equal(x_proj.shape[0], x.shape[0])
    testing.assert_equal(x_proj.shape[1], n_components)
    x_rec = mpca.inverse_transform(x_proj)
    testing.assert_equal(x_rec.shape, x.shape)

    # test n_samples = 1
    x0_proj = mpca.transform(x[0])
    testing.assert_equal(x0_proj.ndim, 2)
    testing.assert_equal(x0_proj.shape[0], 1)
    testing.assert_equal(x0_proj.shape[1], n_components)
    x0_rec = mpca.inverse_transform(x0_proj.reshape(-1))
    testing.assert_equal(x0_rec.shape[1:], x[0].shape)

    # test n_components exceeds upper limit
    mpca.set_params(**{"vectorize": True, "n_components": np.prod(x.shape[1:]) + 1})
    x_proj = mpca.transform(x)
    testing.assert_equal(x_proj.shape[1], np.prod(mpca.shape_out))


def test_mpca_against_baseline(gait, baseline_model):
    x = gait["fea3D"].transpose((3, 0, 1, 2))
    baseline_proj_mats = [baseline_model["tUs"][i][0] for i in range(baseline_model["tUs"].size)]
    baseline_mean = baseline_model["TXmean"]
    mpca = MPCA(var_ratio=0.97)
    x_proj = mpca.fit(x).transform(x)
    testing.assert_allclose(baseline_mean, mpca.mean_)
    baseline_proj_x = multi_mode_dot(x - baseline_mean, baseline_proj_mats, modes=[1, 2, 3])
    # check whether the output embeddings is close to the baseline output by keeping the same variance ratio 97%
    testing.assert_allclose(x_proj**2, baseline_proj_x**2, rtol=relative_tol)
    # testing.assert_equal(x_proj.shape, baseline_proj_x.shape)

    for i in range(x.ndim - 1):
        # check whether each eigen-vector column is equal to/opposite of corresponding baseline eigen-vector column
        # testing.assert_allclose(abs(mpca.proj_mats[i]), abs(baseline_proj_mats[i]))
        testing.assert_allclose(mpca.proj_mats[i] ** 2, baseline_proj_mats[i] ** 2, rtol=relative_tol)


@pytest.mark.parametrize("kernel", ["linear", "rbf"])
@pytest.mark.parametrize("augmentation", [True, False])
def test_mida(kernel, augmentation):
    np.random.seed(29118)
    # Generate toy data
    n_samples = 200

    xs, ys = make_blobs(n_samples, n_features=3, centers=[[0, 0, 0], [0, 2, 1]], cluster_std=[0.3, 0.35])
    xt, yt = make_blobs(n_samples, n_features=3, centers=[[2, -2, 2], [2, 0.2, -1]], cluster_std=[0.35, 0.4])
    x = np.concatenate((xs, xt), axis=0)

    covariates = np.zeros(n_samples * 2)
    covariates[:n_samples] = 1

    enc = OneHotEncoder(handle_unknown="ignore")
    covariates_mat = enc.fit_transform(covariates.reshape(-1, 1)).toarray()
    mida = MIDA(n_components=2, kernel=kernel, augmentation=augmentation)
    x_transformed = mida.fit_transform(x, covariates=covariates_mat)
    testing.assert_equal(x_transformed.shape, (n_samples * 2, 2))

    x_transformed = mida.fit_transform(x, ys, covariates=covariates_mat)
    testing.assert_equal(x_transformed.shape, (n_samples * 2, 2))
</file>

<file path="tests/embed/test_feature_fusion.py">
import torch

from kale.embed.feature_fusion import BimodalInteractionFusion, Concat, LowRankTensorFusion


def test_concat():
    concat = Concat()
    m1 = torch.randn(4, 3, 5)
    m2 = torch.randn(4, 2, 5)
    output = concat([m1, m2])
    assert output.shape == (4, 5 * (3 + 2))


def bimodal_interaction_fusion():
    mi2m_vector = BimodalInteractionFusion(input_dims=(10, 20), output_dim=30, output="vector")
    m1 = torch.randn(4, 10)
    m2 = torch.randn(4, 20)
    output = mi2m_vector([m1, m2])
    assert output.shape == (4, 20)

    mi2m_matrix = BimodalInteractionFusion(input_dims=(10, 20), output_dim=30, output="matrix")
    m1 = torch.randn(4, 10)
    m2 = torch.randn(4, 20)
    output = mi2m_matrix([m1, m2])
    assert output.shape == (4, 30)

    mi2m_scalar = BimodalInteractionFusion(input_dims=(10, 20), output_dim=30, output="scalar")
    m1 = torch.randn(4, 10)
    m2 = torch.randn(4, 20)
    output = mi2m_scalar([m1, m2])
    assert output.shape == (4, 20)


def test_low_rank_tensor_fusion():
    lrtf = LowRankTensorFusion(input_dims=(3, 3, 3), output_dim=1, rank=2)
    m1 = torch.randn(4, 3)
    m2 = torch.randn(4, 3)
    m3 = torch.randn(4, 3)
    output = lrtf([m1, m2, m3])
    assert output.shape == (4, 1)
</file>

<file path="tests/embed/test_gripnet.py">
import pytest
import torch

from kale.embed.gripnet import GripNet
from kale.predict.decode import GripNetLinkPrediction
from kale.prepdata.supergraph_construct import SuperEdge, SuperGraph, SuperVertex, SuperVertexParaSetting


@pytest.mark.parametrize("mode,test_in_channels,test_out_channels", [("cat", 90, 115), ("add", 30, 55)])
def test_gripnet(mode, test_in_channels, test_out_channels):
    """GripNet and SuperGraph Test"""
    # create three supervertices
    node_feat = torch.randn(4, 20)
    edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 3]], dtype=torch.long)
    edge_type = torch.tensor([0, 0, 1, 1], dtype=torch.long)

    supervertex1 = SuperVertex("1", node_feat, edge_index)
    supervertex2 = SuperVertex("2", node_feat, edge_index, edge_type)
    supervertex3 = SuperVertex("3", node_feat, edge_index, edge_type)

    # determine the superedges among them
    edge_index = torch.tensor([[0, 1, 2, 3], [1, 1, 3, 3]])

    superedge1 = SuperEdge(supervertex1.name, supervertex3.name, edge_index)
    superedge2 = SuperEdge(supervertex2.name, supervertex3.name, edge_index)

    # create a supergraph
    supergraph = SuperGraph([supervertex1, supervertex2, supervertex3], [superedge1, superedge2])

    setting1 = SuperVertexParaSetting("1", 20, [10, 10])
    setting2 = SuperVertexParaSetting("2", 20, [10, 10])
    setting3 = SuperVertexParaSetting("3", 30, [15, 10], exter_agg_channels_dict={"1": 30, "2": 30}, mode=mode)

    supergraph.set_supergraph_para_setting([setting1, setting2, setting3])
    gripnet = GripNet(supergraph)

    assert (
        gripnet.supervertex_module_dict["3"][-1].inter_agg_layers[0].in_channels == test_in_channels
    ), "ValueError: invalid exter_agg_channels_dict settings in the task vertex."

    y = gripnet()
    error_message = "ValueError: dimension mismatch in the task vertex"

    assert y.shape[1] == test_out_channels, error_message

    if mode == "cat":
        assert gripnet.out_embed_dict["1"].shape[1] == 20 + 10 + 10, error_message
        assert gripnet.out_embed_dict["2"].shape[1] == 20 + 10 + 10, error_message

    # general tests
    assert supervertex1.__repr__()
    assert superedge1.__repr__()
    assert gripnet.__repr__() is not None

    # test GripNetLinkPrediction predictor
    GripNetLinkPrediction(supergraph, 0.1)
</file>

<file path="tests/embed/test_image_cnn.py">
import pytest
import torch

from kale.embed.image_cnn import (
    Flatten,
    Identity,
    LeNet,
    ResNet18Feature,
    ResNet34Feature,
    ResNet50Feature,
    ResNet101Feature,
    ResNet152Feature,
    SimpleCNNBuilder,
    SmallCNNFeature,
)

BATCH_SIZE = 64

# the default input shape is batch_size * num_channel * height * weight
INPUT_BATCH = torch.randn(BATCH_SIZE, 3, 32, 32)
PARAM = [
    (ResNet18Feature, 512),
    (ResNet34Feature, 512),
    (ResNet50Feature, 2048),
    (ResNet101Feature, 2048),
    (ResNet152Feature, 2048),
]


def test_smallcnnfeature_shapes():
    model = SmallCNNFeature()
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, 128)


def test_simplecnnbuilder_shapes():
    model = SimpleCNNBuilder(
        conv_layers_spec=[[16, 3], [32, 3], [64, 3], [32, 1], [64, 3], [128, 3], [256, 3], [64, 1]]
    )
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, 64, 8, 8)


@pytest.mark.parametrize("param", PARAM)
def test_shapes(param):
    model, out_size = param
    model = model(weights="DEFAULT")
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, out_size)
    assert model.output_size() == out_size


def test_lenet_output_shapes():
    input_channels = 3
    output_channels = 6
    additional_layers = 2
    lenet = LeNet(input_channels, output_channels, additional_layers)
    x = torch.randn(16, 3, 32, 32)
    output = lenet(x)
    assert output.shape == (16, 24, 4, 4), "Unexpected output shape"


def test_flatten_output_shapes():
    flatten = Flatten()
    x = torch.randn(16, 3, 32, 32)
    output = flatten(x)
    assert output.shape == (16, 3072), "Unexpected output shape"


def test_identity_output_shapes():
    identity = Identity()
    x = torch.randn(16, 3, 32, 32)
    output = identity(x)
    assert output.shape == (16, 3, 32, 32), "Unexpected output shape"
</file>

<file path="tests/embed/test_mogonet.py">
import pytest
import torch
from torch_geometric.data import Data
from torch_sparse import SparseTensor

from kale.embed.mogonet import MogonetGCN, MogonetGCNConv


@pytest.fixture(scope="module")
def test_data():
    x = torch.Tensor([[0.0, 0.1], [0.2, 0.3], [0.4, 0.5]])
    edge_index = torch.tensor([[0, 1, 2, 2], [1, 0, 1, 2]])
    edge_weight = torch.tensor([0.5, 0.2, 0.3, 0.4])
    adj_t = SparseTensor(row=edge_index[0], col=edge_index[1], value=edge_weight)

    return Data(x=x, edge_index=edge_index, adj_t=adj_t)


def test_mogonet_gcn_conv_model_shape():
    in_channels = 2
    out_channels = 8
    conv = MogonetGCNConv(in_channels=in_channels, out_channels=out_channels)

    # Test the shape of the module
    assert conv.in_channels == in_channels
    assert conv.out_channels == out_channels

    # Test the shape of the weight tensor
    assert conv.weight.shape == (in_channels, out_channels)


def test_mogonet_gcn_conv_no_bias():
    in_channels = 2
    out_channels = 8
    conv = MogonetGCNConv(in_channels=in_channels, out_channels=out_channels, bias=False)
    assert conv.bias is None


def test_mogonet_gcn_conv_reset_parameters():
    in_channels = 2
    out_channels = 8
    conv = MogonetGCNConv(in_channels=in_channels, out_channels=out_channels)
    # Set parameters to a fixed value
    with torch.no_grad():
        for param in conv.parameters():
            param.fill_(1.0)
    # Reset the parameters
    conv.reset_parameters()
    # Check that the parameters are now different
    for param in conv.parameters():
        assert torch.any(param != 1.0)


def test_mogonet_gcn_conv_output(test_data):
    in_channels = 2
    out_channels = 8
    num_samples = 3
    conv = MogonetGCNConv(in_channels=in_channels, out_channels=out_channels)

    # Test the forward pass
    x = test_data.x
    adj_t = test_data.adj_t
    output = conv(x, adj_t)
    assert output.shape == (num_samples, out_channels)

    # Test the message function
    x_j = torch.randn(3, 5)
    msg = conv.message(x_j)
    assert msg.shape == (3, 5)

    # Test the message_and_aggregate function
    agg = conv.message_and_aggregate(adj_t, x)
    assert agg.shape == (num_samples, in_channels)

    # Test the update function
    aggr_out = torch.randn(num_samples, out_channels)
    updated = conv.update(aggr_out)
    assert updated.shape == aggr_out.shape


def test_mogonet_gcn_model_shape():
    model = MogonetGCN(in_channels=2, hidden_channels=[4, 5, 6], dropout=0.5)
    assert isinstance(model.conv1, MogonetGCNConv)
    assert isinstance(model.conv2, MogonetGCNConv)
    assert isinstance(model.conv3, MogonetGCNConv)

    assert model.conv1.weight.shape == (2, 4)
    assert model.conv2.weight.shape == (4, 5)
    assert model.conv3.weight.shape == (5, 6)


def test_mogonet_gcn_output(test_data):
    model = MogonetGCN(in_channels=2, hidden_channels=[4, 5, 6], dropout=0.5)
    x = test_data.x
    adj_t = test_data.adj_t
    output = model(x, adj_t)
    assert output.shape == (3, 6)
</file>

<file path="tests/embed/test_seq_nn.py">
import torch

from kale.embed.seq_nn import CNNEncoder, GCNEncoder


def test_cnn_encoder():
    num_embeddings, embedding_dim = 64, 128
    sequence_length = 85
    num_filters = 32
    filter_length = 8
    cnn_encoder = CNNEncoder(
        num_embeddings=num_embeddings,
        embedding_dim=embedding_dim,
        sequence_length=sequence_length,
        num_kernels=num_filters,
        kernel_length=filter_length,
    ).eval()
    # assert cnn encoder shape
    assert cnn_encoder.embedding.weight.size() == (num_embeddings + 1, embedding_dim)
    assert cnn_encoder.conv1.__repr__() == "Conv1d(85, 32, kernel_size=(8,), stride=(1,))"
    assert cnn_encoder.conv2.__repr__() == "Conv1d(32, 64, kernel_size=(8,), stride=(1,))"
    assert cnn_encoder.conv3.__repr__() == "Conv1d(64, 96, kernel_size=(8,), stride=(1,))"
    assert cnn_encoder.global_max_pool.__repr__() == "AdaptiveMaxPool1d(output_size=1)"

    input_batch = torch.randint(high=num_embeddings, size=(8, sequence_length))
    output_encoding = cnn_encoder(input_batch)
    # assert output shape
    assert output_encoding.size() == (8, 96)


def test_gcn_encoder():
    gcn_encoder = GCNEncoder(in_channel=8, out_channel=32).eval()
    assert gcn_encoder.conv1.__repr__() == "GCNConv(8, 8)"
    assert gcn_encoder.conv2.__repr__() == "GCNConv(8, 16)"
    assert gcn_encoder.conv3.__repr__() == "GCNConv(16, 32)"
    N1, N2 = 4, 5
    x = torch.randn(N1 + N2, 8)
    batch = torch.tensor([0 for _ in range(N1)] + [1 for _ in range(N2)])
    edge_index = torch.tensor([[0, 0, 0, 1, 2, 3, 3, 5, 7, 8], [1, 2, 3, 0, 0, 0, 2, 6, 6, 0]], dtype=torch.long)
    row, col = edge_index
    from torch_sparse import SparseTensor

    adj = SparseTensor(row=row, col=col, value=None, sparse_sizes=(9, 9))

    out1 = gcn_encoder(x, edge_index, batch)
    assert out1.size() == (2, 32)
    # assert the consistency between types of LongTensor and SparseTensor.
    assert torch.allclose(gcn_encoder(x, adj.t(), batch), out1, atol=1e-6)
</file>

<file path="tests/embed/test_video_feature_extractor.py">
import pytest
import torch

from kale.embed.video_feature_extractor import get_video_feat_extractor

MODEL_NAME = ["I3D", "R3D_18", "R2PLUS1D_18", "MC3_18"]
IMAGE_MODALITY = ["rgb", "flow", "joint"]
# ATTENTION = ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerCT", "SELayerTC", "SELayerMAC", "None"]
ATTENTION = ["SELayerC", "SELayerT", "SELayerCT", "SELayerTC", "None"]
# NUM_CLASSES = [6, 7, 8]
NUM_CLASSES = [6]


@pytest.mark.parametrize("model_name", MODEL_NAME)
@pytest.mark.parametrize("image_modality", IMAGE_MODALITY)
@pytest.mark.parametrize("attention", ATTENTION)
@pytest.mark.parametrize("num_classes", NUM_CLASSES)
def test_get_video_feat_extractor(model_name, image_modality, attention, num_classes):
    feature_network, class_feature_dim, domain_feature_dim = get_video_feat_extractor(
        model_name, image_modality, attention, num_classes
    )

    assert isinstance(feature_network, dict)

    if image_modality == "joint":
        assert isinstance(feature_network["rgb"], torch.nn.Module)
        assert isinstance(feature_network["flow"], torch.nn.Module)
    elif image_modality == "rgb":
        assert isinstance(feature_network["rgb"], torch.nn.Module)
        assert feature_network["flow"] is None
    else:
        assert feature_network["rgb"] is None
        assert isinstance(feature_network["flow"], torch.nn.Module)

    if model_name == "I3D":
        assert domain_feature_dim == 1024
        assert class_feature_dim == 2048 if image_modality == "joint" else class_feature_dim == 1024

    else:
        assert domain_feature_dim == 512
        assert class_feature_dim == 1024 if image_modality == "joint" else class_feature_dim == 512
</file>

<file path="tests/embed/test_video_i3d_r3d.py">
"""Reference: https://github.com/pytorch/vision/blob/master/test/test_models.py#L257"""

import pytest
import torch

from kale.embed.video_i3d import InceptionI3d
from kale.embed.video_res3d import BasicBlock, BasicStem, Bottleneck, Conv3DSimple, VideoResNet
from kale.embed.video_se_i3d import SEInceptionI3DFlow, SEInceptionI3DRGB
from kale.embed.video_se_res3d import se_r3d_18_flow, se_r3d_18_rgb
from kale.utils.seed import set_seed

set_seed(36)
# The default input shape is batch_size * num_channel * frame_per_segment * height * weight.
# In our experiment, the height and weight are both 224. To avoid to allocate too many memory,
# height and weight are set as 112. It won't influence the model test.
# Differences between inputs of RGB and flow is channel number and frame_per_segment.
INPUT_BATCH_RGB = torch.randn(2, 3, 16, 112, 112)
INPUT_BATCH_FLOW = torch.randn(2, 2, 8, 112, 112)
SE_LAYERS = ["SELayerC", "SELayerT", "SELayerCoC", "SELayerMC", "SELayerMAC", "SELayerCT", "SELayerTC"]


def test_i3d_shapes():
    # test InceptionI3D
    i3d = InceptionI3d()
    i3d.eval()
    output_batch = i3d(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 1024, 1)

    # test InceptionI3D.extract_features
    output_batch = i3d.extract_features(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 1024, 1, 1, 1)


def test_videoresnet_basicblock_shapes():
    # test VideoResNet with BasicBlock
    resnet = VideoResNet(
        block=BasicBlock,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
    )
    resnet.eval()
    output_batch = resnet(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 512)


def test_videoresenet_bottleneck_shapes():
    # test VideoResNet with Bottleneck
    resnet_bottleneck = VideoResNet(
        block=Bottleneck,
        conv_makers=[Conv3DSimple] * 4,
        layers=[2, 2, 2, 2],
        stem=BasicStem,
    )
    resnet_bottleneck.eval()
    output_batch = resnet_bottleneck(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 2048)


@pytest.mark.parametrize("se_layers", SE_LAYERS)
def test_i3d_selayer_shapes(se_layers):
    # test I3D with SELayers
    se_rgb = SEInceptionI3DRGB(3, 8, se_layers)
    se_flow = SEInceptionI3DFlow(2, 8, se_layers)
    se_rgb.eval()
    se_flow.eval()

    output_batch = se_rgb(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 1024, 1)
    output_batch = se_flow(INPUT_BATCH_FLOW)
    assert output_batch.size() == (2, 1024, 1)


@pytest.mark.parametrize("se_layers", SE_LAYERS)
def test_r3d_selayer_shapes(se_layers):
    # test R3D with SELayers
    se_rgb = se_r3d_18_rgb(attention=se_layers, pretrained=False)
    se_flow = se_r3d_18_flow(attention=se_layers, pretrained=False)
    se_rgb.eval()
    se_flow.eval()

    output_batch = se_rgb(INPUT_BATCH_RGB)
    assert output_batch.size() == (2, 512)
    output_batch = se_flow(INPUT_BATCH_FLOW)
    assert output_batch.size() == (2, 512)
</file>

<file path="tests/evaluate/test_cross_validation.py">
import numpy as np
import pytest
from sklearn.dummy import DummyClassifier

from kale.evaluate import cross_validation
from kale.pipeline.multi_domain_adapter import CoIRLS


@pytest.fixture
def sample_data():
    # Sample data for testing
    x = np.array([np.random.rand(100)] * 8)
    y = np.array([0, 0, 0, 0, 1, 1, 1, 1])
    groups = np.array(["A", "A", "B", "B", "A", "A", "B", "B"])
    return x, y, groups


# Checks leave-one-group-out results for above sample data
def check_leave_one_group_out_results(sample_data, estimator, domain_adaptation=False):
    x, y, groups = sample_data
    results = cross_validation.leave_one_group_out(x, y, groups, estimator, domain_adaptation)

    # Check if all keys are present in the result dictionary
    assert "Target" in results
    assert "Num_samples" in results
    assert "Accuracy" in results

    # Check if the length of 'Target', 'Num_samples', and 'Accuracy' lists is the same
    assert len(results["Target"]) == len(results["Num_samples"]) == len(results["Accuracy"])

    # Check if the last element of 'Target' is "Average"
    assert results["Target"] == ["A", "B", "Average"]

    # Check if the number of samples is correct for each target group
    assert results["Num_samples"] == [4, 4, 8]

    # Check if the accuracy scores are within the expected range
    for accuracy in results["Accuracy"]:
        assert 0 <= accuracy <= 1


def test_leave_one_group_out_without_domain_adaptation(sample_data):
    estimator = DummyClassifier()
    check_leave_one_group_out_results(sample_data, estimator, domain_adaptation=False)


def test_leave_one_group_out_with_domain_adaptation(sample_data):
    estimator = CoIRLS(kernel="linear", lambda_=1.0, alpha=1.0)
    check_leave_one_group_out_results(sample_data, estimator, domain_adaptation=True)
</file>

<file path="tests/evaluate/test_metrics.py">
import random

import pytest
import torch

from kale.evaluate.metrics import (
    calculate_distance,
    DistanceMetric,
    multitask_topk_accuracy,
    protonet_loss,
    topk_accuracy,
)

# Dummy data: [batch_size, num_classes]
# Dummy ground truth: batch_size
FIRST_PREDS = torch.tensor(
    (
        [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
    )
)
FIRST_LABELS = torch.tensor((0, 2, 4, 5, 5))

SECOND_PREDS = torch.tensor(
    (
        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
        [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1],
    )
)
SECOND_LABELS = torch.tensor((0, 0, 4, 4, 5))

MULTI_PREDS = (FIRST_PREDS, SECOND_PREDS)
MULTI_LABELS = (FIRST_LABELS, SECOND_LABELS)


def test_topk_accuracy():
    # Test topk_accuracy with single-task input
    preds = FIRST_PREDS
    labels = FIRST_LABELS
    k = (1, 3, 5)

    top1, top3, top5 = topk_accuracy(preds, labels, k)
    top1_value = top1.double().mean()
    top3_value = top3.double().mean()
    top5_value = top5.double().mean()
    assert top1_value.cpu() == pytest.approx(1 / 5)
    assert top3_value.cpu() == pytest.approx(2 / 5)
    assert top5_value.cpu() == pytest.approx(3 / 5)


def test_multitask_topk_accuracy():
    # Test multitask_topk_accuracy with input for two tasks
    preds = MULTI_PREDS
    labels = MULTI_LABELS
    k = (1, 3, 5)

    top1, top3, top5 = multitask_topk_accuracy(preds, labels, k)
    top1_value = top1.double().mean()
    top3_value = top3.double().mean()
    top5_value = top5.double().mean()
    assert top1_value.cpu() == pytest.approx(1 / 5)
    assert top3_value.cpu() == pytest.approx(2 / 5)
    assert top5_value.cpu() == pytest.approx(3 / 5)


def test_protonet_loss():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    num_classes = random.randint(1, 10)
    num_support_samples = random.randint(1, 10)
    num_query_samples = random.randint(1, 10)
    n_dim = random.randint(1, 512)
    feature_support = torch.rand(num_classes, num_support_samples, n_dim)
    feature_query = torch.rand(num_classes * num_query_samples, n_dim)
    loss_fn = protonet_loss(num_classes=num_classes, num_query_samples=num_query_samples, device=device)
    loss, acc = loss_fn(feature_support, feature_query)
    assert isinstance(loss, torch.Tensor)
    assert isinstance(acc, torch.Tensor)
    prototypes = feature_support.mean(dim=1)
    dists = loss_fn.euclidean_dist_for_tensor_group(prototypes, feature_query)
    assert dists.shape == (num_classes, num_classes * num_query_samples)


@pytest.fixture
def x1():
    return torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float)


@pytest.fixture
def x2():
    return torch.tensor([[1, 1, 1], [2, 2, 2]], dtype=torch.float)


def test_cosine_distance(x1, x2):
    expected_output = torch.tensor([[0.9258, 0.9258], [0.9869, 0.9869]])
    output = calculate_distance(x1, x2, metric=DistanceMetric.COSINE)
    assert torch.allclose(output, expected_output, rtol=1e-3, atol=1e-3)


def test_cosine_distance_self(x1):
    expected_output = torch.tensor([[1.0000, 0.9746], [0.9746, 1.0000]])
    output = calculate_distance(x1, metric=DistanceMetric.COSINE)
    assert torch.allclose(output, expected_output, rtol=1e-3, atol=1e-3)


def test_cosine_distance_eps(x1, x2):
    expected_output = torch.tensor([[0.9258, 0.9258], [0.9869, 0.9869]])
    output = calculate_distance(x1, x2, eps=0, metric=DistanceMetric.COSINE)
    assert torch.allclose(output, expected_output, rtol=1e-3, atol=1e-3)


def test_unsupported_metric(x1, x2):
    with pytest.raises(Exception):
        calculate_distance(x1, x2, metric="unsupported_metric")
</file>

<file path="tests/evaluate/test_uncertainty_metrics.py">
import logging

import pytest

from kale.evaluate.uncertainty_metrics import evaluate_bounds, evaluate_jaccard
from kale.prepdata.tabular_transform import generate_struct_for_qbin

# from kale.utils.download import download_file_by_url
from kale.utils.seed import set_seed

# import os
LOGGER = logging.getLogger(__name__)


seed = 36
set_seed(seed)

ERRORS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
UNCERTAINTIES = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]


@pytest.fixture(scope="module")
def dummy_test_preds(landmark_uncertainty_tuples_path):
    bins_all_targets, bins_targets_sep, bounds_all_targets, bounds_targets_sep = generate_struct_for_qbin(
        ["U-NET"], [0, 1], landmark_uncertainty_tuples_path[2], "SA"
    )

    return bins_all_targets, bounds_all_targets


class TestEvaluateJaccard:
    # Using one uncertainty type, test numerous bins
    @pytest.mark.parametrize("num_bins", [2, 3, 4, 5])
    def test_one_uncertainty(self, dummy_test_preds, num_bins):
        jacc_dict = evaluate_jaccard(
            dummy_test_preds[0], [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"]], num_bins, [0, 1], num_folds=8
        )
        all_jaccard_data = jacc_dict["Jaccard All"]
        all_jaccard_bins_targets_sep = jacc_dict["Jaccard targets seperated"]

        assert list(all_jaccard_data.keys()) == ["U-NET S-MHA"]
        assert len(all_jaccard_data["U-NET S-MHA"]) == num_bins

        assert list(all_jaccard_bins_targets_sep.keys()) == ["U-NET S-MHA"]
        assert len(all_jaccard_bins_targets_sep["U-NET S-MHA"]) == num_bins
        assert (
            len(all_jaccard_bins_targets_sep["U-NET S-MHA"][0]) == 8 * 2
        )  # because each landmark has 8 folds - they are seperate

    def test_one_fold(self, dummy_test_preds):
        jacc_dict = evaluate_jaccard(
            dummy_test_preds[0], [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"]], 5, [0, 1], num_folds=1
        )

        all_jaccard_data = jacc_dict["Jaccard All"]
        all_jaccard_bins_targets_sep = jacc_dict["Jaccard targets seperated"]

        assert list(all_jaccard_data.keys()) == ["U-NET S-MHA"]
        assert len(all_jaccard_data["U-NET S-MHA"]) == 5

        assert list(all_jaccard_bins_targets_sep.keys()) == ["U-NET S-MHA"]
        assert len(all_jaccard_bins_targets_sep["U-NET S-MHA"]) == 5
        assert (
            len(all_jaccard_bins_targets_sep["U-NET S-MHA"][0]) == 2
        )  # because each landmark has 1 folds - they are sep

    def test_multiple_uncerts(self, dummy_test_preds):
        jacc_dict = evaluate_jaccard(
            dummy_test_preds[0],
            [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"]],
            5,
            [0, 1],
            num_folds=1,
        )

        all_jaccard_data = jacc_dict["Jaccard All"]
        all_jaccard_bins_targets_sep = jacc_dict["Jaccard targets seperated"]

        assert list(all_jaccard_data.keys()) == ["U-NET S-MHA", "U-NET E-MHA"]
        assert len(all_jaccard_data["U-NET S-MHA"]) == len(all_jaccard_data["U-NET E-MHA"]) == 5

        assert list(all_jaccard_bins_targets_sep.keys()) == ["U-NET S-MHA", "U-NET E-MHA"]
        assert len(all_jaccard_bins_targets_sep["U-NET S-MHA"]) == len(all_jaccard_bins_targets_sep["U-NET E-MHA"]) == 5
        assert (
            len(all_jaccard_bins_targets_sep["U-NET S-MHA"][0])
            == len(all_jaccard_bins_targets_sep["U-NET E-MHA"][0])
            == 2
        )  # because each landmark has 8 folds - they are sep


class TestEvaluateBounds:
    @pytest.mark.parametrize("num_bins", [2, 3, 4, 5])
    def test_one_uncertainty(self, dummy_test_preds, num_bins):
        bound_dict = evaluate_bounds(
            dummy_test_preds[1],
            dummy_test_preds[0],
            [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"]],
            num_bins,
            [0, 1],
            num_folds=8,
        )

        all_bound_percents = bound_dict["Error Bounds All"]
        all_bound_percents_notargetsep = bound_dict["all_bound_percents_notargetsep"]

        assert list(all_bound_percents.keys()) == ["U-NET S-MHA"]
        assert len(all_bound_percents["U-NET S-MHA"]) == num_bins

        assert list(all_bound_percents_notargetsep.keys()) == ["U-NET S-MHA"]
        assert len(all_bound_percents_notargetsep["U-NET S-MHA"]) == num_bins
        assert (
            len(all_bound_percents_notargetsep["U-NET S-MHA"][0]) == 8 * 2
        )  # because each landmark has 8 folds - they are seperate

    def test_one_fold(self, dummy_test_preds):
        bound_dict = evaluate_bounds(
            dummy_test_preds[1],
            dummy_test_preds[0],
            [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"]],
            5,
            [0, 1],
            num_folds=1,
        )
        all_bound_percents = bound_dict["Error Bounds All"]
        all_bound_percents_notargetsep = bound_dict["all_bound_percents_notargetsep"]

        assert list(all_bound_percents.keys()) == ["U-NET S-MHA"]
        assert len(all_bound_percents["U-NET S-MHA"]) == 5

        assert list(all_bound_percents_notargetsep.keys()) == ["U-NET S-MHA"]
        assert len(all_bound_percents_notargetsep["U-NET S-MHA"]) == 5
        assert (
            len(all_bound_percents_notargetsep["U-NET S-MHA"][0]) == 2
        )  # because each landmark has 1 folds - they are sep

    def test_multiple_uncerts(self, dummy_test_preds):
        bound_dict = evaluate_bounds(
            dummy_test_preds[1],
            dummy_test_preds[0],
            [["S-MHA", "S-MHA Error", "S-MHA Uncertainty"], ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"]],
            5,
            [0, 1],
            num_folds=8,
        )

        all_bound_percents = bound_dict["Error Bounds All"]
        all_bound_percents_notargetsep = bound_dict["all_bound_percents_notargetsep"]

        assert list(all_bound_percents.keys()) == ["U-NET S-MHA", "U-NET E-MHA"]
        assert len(all_bound_percents["U-NET S-MHA"]) == len(all_bound_percents["U-NET E-MHA"]) == 5

        assert list(all_bound_percents_notargetsep.keys()) == ["U-NET S-MHA", "U-NET E-MHA"]
        assert (
            len(all_bound_percents_notargetsep["U-NET S-MHA"])
            == len(all_bound_percents_notargetsep["U-NET E-MHA"])
            == 5
        )
        assert (
            len(all_bound_percents_notargetsep["U-NET S-MHA"][0])
            == len(all_bound_percents_notargetsep["U-NET E-MHA"][0])
            == 8 * 2
        )  # because each landmark has 8 folds - they are sep
</file>

<file path="tests/helpers/boring_model.py">
""" Models for efficient test following PyTorch-Lightning.

References: https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/helpers/boring_model.py
"""

import torch.nn as nn


class VideoBoringModel(nn.Module):
    def __init__(self, in_channel):
        super().__init__()
        self.avg_pool3d = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Linear(in_channel, 1024)

    def forward(self, x):
        x = self.avg_pool3d(x).squeeze()
        x = self.fc(x)
        return x

    def output_size(self):
        return self.fc.in_features
</file>

<file path="tests/helpers/pipe_test_helper.py">
import pytorch_lightning as pl

from kale.pipeline import domain_adapter


class ModelTestHelper:
    @staticmethod
    def test_model(model, train_params, **kwargs):
        assert isinstance(model, domain_adapter.BaseAdaptTrainer)
        # training process
        trainer = pl.Trainer(
            default_root_dir="tests/outputs",
            min_epochs=train_params["nb_init_epochs"],
            max_epochs=train_params["nb_adapt_epochs"],
            devices=1,
            **kwargs,
        )
        trainer.fit(model)
        trainer.test()
        metric_values = trainer.callback_metrics
        assert isinstance(metric_values, dict)
</file>

<file path="tests/interpret/test_uncertainty_quantiles.py">
import logging

import numpy as np
import pytest

from kale.interpret.uncertainty_quantiles import quantile_binning_and_est_errors
from kale.loaddata.tabular_access import load_csv_columns

# from kale.utils.download import download_file_by_url
from kale.utils.seed import set_seed

# import os
LOGGER = logging.getLogger(__name__)


seed = 36
set_seed(seed)

ERRORS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
UNCERTAINTIES = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]


@pytest.fixture(scope="module")
def dummy_test_data(landmark_uncertainty_dl):
    dummy_tabular_data_dict = load_csv_columns(
        landmark_uncertainty_dl[1], "Testing Fold", np.arange(0), cols_to_return="All"
    )

    dummy_errors = dummy_tabular_data_dict["S-MHA Error"].values
    dummy_uncertainties = dummy_tabular_data_dict["S-MHA Error"].values

    return dummy_errors, dummy_uncertainties


class TestQuantileBinningAndEstErrors:
    def test_empty(self):
        with pytest.raises(ValueError, match=r"Length of errors .*"):
            quantile_binning_and_est_errors(ERRORS, [0, 1, 2], num_bins=5)
        with pytest.raises(ValueError, match=r"Length of errors .*"):
            quantile_binning_and_est_errors([], [0, 1, 2], num_bins=5)

    # Using 11 datapoints from 0-N, we test if we can create 10 bins between these:
    # <0.1, 0.1,0.2,0.3,0.4,0.5,0.6, 0.7,0.8,0.9, >0.9
    # same logic with expected errors
    def test_dummy_1(self):
        est_bounds, est_errors = quantile_binning_and_est_errors(ERRORS, UNCERTAINTIES, num_bins=10)

        assert pytest.approx(np.squeeze(est_bounds)) == UNCERTAINTIES[1:-1]
        assert pytest.approx(np.squeeze(est_errors)) == ERRORS[1:-1]
</file>

<file path="tests/loaddata/test_avmnist_datasets.py">
import os
import tempfile

import numpy as np
import pytest

from kale.loaddata.avmnist_datasets import AVMNISTDataset


@pytest.fixture(scope="module")
def dummy_data():
    # Create temporary directory for data
    temp_dir = tempfile.TemporaryDirectory()
    data_dir = temp_dir.name

    # Create dummy AVMNIST data
    create_dummy_data(data_dir)

    yield data_dir

    # Clean up temporary directory
    temp_dir.cleanup()


def create_dummy_data(data_dir):
    os.makedirs(os.path.join(data_dir, "image"))
    os.makedirs(os.path.join(data_dir, "audio"))

    train_image_data = np.random.rand(100, 28, 28)
    train_audio_data = np.random.rand(100, 112, 112)
    train_labels = np.random.randint(0, 10, size=(100,))

    test_image_data = np.random.rand(20, 28, 28)
    test_audio_data = np.random.rand(20, 112, 112)
    test_labels = np.random.randint(0, 10, size=(20,))

    np.save(os.path.join(data_dir, "image/train_data.npy"), train_image_data)
    np.save(os.path.join(data_dir, "audio/train_data.npy"), train_audio_data)
    np.save(os.path.join(data_dir, "train_labels.npy"), train_labels)

    np.save(os.path.join(data_dir, "image/test_data.npy"), test_image_data)
    np.save(os.path.join(data_dir, "audio/test_data.npy"), test_audio_data)
    np.save(os.path.join(data_dir, "test_labels.npy"), test_labels)


def test_avmnist_dataset(dummy_data):
    # Initialize AVMNISTDataset with temporary data directory
    dataset = AVMNISTDataset(dummy_data)

    # Test train, validation, and test data loaders
    train_loader = dataset.get_train_loader()
    valid_loader = dataset.get_valid_loader()
    test_loader = dataset.get_test_loader()

    # Check if the loaders are working and have the correct dimensions
    for loader in [train_loader, valid_loader, test_loader]:
        for images, audios, labels in loader:
            assert images.shape[1:] == (1, 28, 28)
            assert audios.shape[1:] == (1, 112, 112)
            assert labels.min() >= 0
            assert labels.max() < 10
            break
</file>

<file path="tests/loaddata/test_few_shot.py">
import random
from pathlib import Path

import numpy as np
import pytest
import torch
from yacs.config import CfgNode

from kale.loaddata.few_shot import NWayKShotDataset
from kale.prepdata.image_transform import get_transform

modes = ["train", "val", "test"]


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    cfg = CfgNode()
    cfg.DATASET = CfgNode()
    cfg.DATASET.ROOT = str(Path(download_path) / "omniglot_demo")
    yield cfg


@pytest.mark.parametrize("mode", modes)
def test_few_shot(mode, testing_cfg):
    cfg = testing_cfg

    transform = get_transform(kind="few-shot")
    num_support_samples = random.randint(1, 10)
    num_query_samples = random.randint(1, 10)
    num_classes = random.randint(1, 10)
    dataset = NWayKShotDataset(
        path=cfg.DATASET.ROOT,
        mode=mode,
        num_support_samples=num_support_samples,
        num_query_samples=num_query_samples,
        transform=transform,
    )
    assert len(dataset) == len(dataset.classes) > 0
    assert isinstance(dataset._get_idx(0), np.ndarray)
    assert isinstance(dataset._sample_data(dataset._get_idx(0)), list)
    assert isinstance(dataset._sample_data(dataset._get_idx(0))[0], torch.Tensor)
    assert isinstance(dataset.__getitem__(0), tuple)
    assert isinstance(dataset.__getitem__(0)[0], torch.Tensor)
    assert isinstance(dataset.__getitem__(0)[1], int)

    dataloader = torch.utils.data.DataLoader(dataset, batch_size=num_classes, shuffle=True, drop_last=True)
    for batch in dataloader:
        assert len(dataloader) > 0
        assert isinstance(batch, list)
        assert isinstance(batch[0], torch.Tensor)
        assert isinstance(batch[1], torch.Tensor)
        assert batch[0].shape == (num_classes, num_support_samples + num_query_samples, 3, 224, 224)
        assert batch[1].shape == (num_classes,)
        break
</file>

<file path="tests/loaddata/test_image_access.py">
import pytest
import torch
from numpy import testing
from yacs.config import CfgNode

from kale.loaddata.dataset_access import get_class_subset
from kale.loaddata.image_access import DigitDataset, DigitDatasetAccess, get_cifar, ImageAccess
from kale.loaddata.multi_domain import DomainsDatasetBase, MultiDomainAdapDataset, MultiDomainDatasets


@pytest.mark.parametrize("test_on_all", [True, False])
def test_office31(office_path, test_on_all):
    office_access = ImageAccess.get_multi_domain_images(
        "OFFICE31", office_path, download=False, return_domain_label=True
    )
    testing.assert_equal(len(office_access.class_to_idx), 31)
    testing.assert_equal(len(office_access.domain_to_idx), 3)
    dataset = MultiDomainAdapDataset(office_access, test_on_all=test_on_all)
    dataset.prepare_data_loaders()
    domain_labels = list(dataset.domain_to_idx.values())
    for split in ["train", "valid", "test"]:
        dataloader = dataset.get_domain_loaders(split=split)
        x, y, z = next(iter(dataloader))
        for domain_label_ in domain_labels:
            testing.assert_equal(y[z == domain_label_].shape[0], 10)


def test_office_caltech(office_path):
    office_access = ImageAccess.get_multi_domain_images(
        "OFFICE_CALTECH", office_path, download=False, return_domain_label=True
    )
    testing.assert_equal(len(office_access.class_to_idx), 10)
    testing.assert_equal(len(office_access.domain_to_idx), 4)


@pytest.mark.parametrize("split_ratio", [0.9, 1])
def test_custom_office(office_path, split_ratio):
    kwargs = {"download": False, "split_train_test": True, "split_ratio": split_ratio}
    source = ImageAccess.get_multi_domain_images("office", office_path, sub_domain_set=["dslr"], **kwargs)
    target = ImageAccess.get_multi_domain_images("office", office_path, sub_domain_set=["webcam"], **kwargs)
    dataset = MultiDomainDatasets(source_access=source, target_access=target)
    dataset.prepare_data_loaders()
    dataloader = dataset.get_domain_loaders()
    testing.assert_equal(len(next(iter(dataloader))), 2)


SOURCES = ["MNIST", "USPS"]
TARGETS = ["MNISTM", "SVHN"]
ALL = SOURCES + TARGETS  # ["SVHN", "USPS", "MNISTM"]  # SOURCES + TARGETS

WEIGHT_TYPE = ["natural", "balanced", "preset0"]
DATASIZE_TYPE = ["max", "source"]
VALID_RATIO = [0.1]

CLASS_SUBSETS = [[1, 3, 8]]


@pytest.mark.parametrize("source_name", SOURCES)
@pytest.mark.parametrize("target_name", TARGETS)
def test_get_source_target(source_name, target_name, download_path):
    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(source_name), DigitDataset(target_name), download_path
    )
    assert num_channels == 3
    assert isinstance(source, DigitDatasetAccess)
    assert isinstance(target, DigitDatasetAccess)


@pytest.mark.parametrize("weight_type", WEIGHT_TYPE)
@pytest.mark.parametrize("datasize_type", DATASIZE_TYPE)
def test_multi_domain_datasets(weight_type, datasize_type, download_path):
    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(SOURCES[0]), DigitDataset(TARGETS[0]), download_path
    )
    assert num_channels == 3
    assert isinstance(source, DigitDatasetAccess)
    assert isinstance(target, DigitDatasetAccess)

    dataset = MultiDomainDatasets(source, target, config_weight_type=weight_type, config_size_type=datasize_type)
    assert isinstance(dataset, DomainsDatasetBase)


@pytest.mark.parametrize("dataset_name", ALL)
def test_get_train_test(dataset_name, download_path):
    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(dataset_name), DigitDataset(dataset_name), download_path
    )
    source_train = source.get_train()
    source_test = source.get_test()
    assert source.n_classes() == 10
    assert isinstance(source_train, torch.utils.data.Dataset)
    assert isinstance(source_test, torch.utils.data.Dataset)


@pytest.mark.parametrize("class_subset", CLASS_SUBSETS)
@pytest.mark.parametrize("valid_ratio", VALID_RATIO)
def test_class_subsets(class_subset, valid_ratio, download_path):
    dataset_name = ALL[1]
    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(dataset_name), DigitDataset(dataset_name), download_path
    )

    dataset_subset = MultiDomainDatasets(
        source,
        target,
        config_weight_type=WEIGHT_TYPE[0],
        config_size_type=DATASIZE_TYPE[1],
        class_ids=class_subset,
    )

    train, valid = source.get_train_valid(valid_ratio)
    test = source.get_test()
    dataset_subset._source_by_split["train"] = get_class_subset(train, class_subset)
    dataset_subset._target_by_split["train"] = dataset_subset._source_by_split["train"]
    dataset_subset._source_by_split["valid"] = get_class_subset(valid, class_subset)
    dataset_subset._source_by_split["test"] = get_class_subset(test, class_subset)

    # Ground truth lengths
    train_dataset_subset_length = len([1 for data in train if data[1] in class_subset])
    valid_dataset_subset_length = len([1 for data in valid if data[1] in class_subset])
    test_dataset_subset_length = len([1 for data in test if data[1] in class_subset])

    assert len(dataset_subset._source_by_split["train"]) == train_dataset_subset_length
    assert len(dataset_subset._source_by_split["valid"]) == valid_dataset_subset_length
    assert len(dataset_subset._source_by_split["test"]) == test_dataset_subset_length
    assert len(dataset_subset) == train_dataset_subset_length


def test_multi_domain_digits(download_path):
    data_access = ImageAccess.get_multi_domain_images(
        "DIGITS", download_path, sub_domain_set=["SVHN", "USPS_RGB"], return_domain_label=True
    )
    dataset = MultiDomainAdapDataset(data_access)
    dataset.prepare_data_loaders()
    dataloader = dataset.get_domain_loaders(split="test", batch_size=10)
    assert len(next(iter(dataloader))) == 3


DATASET_NAMES = ["CIFAR10", "CIFAR100"]


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    cfg = CfgNode()
    cfg.DATASET = CfgNode()
    cfg.SOLVER = CfgNode()
    cfg.DATASET.ROOT = download_path
    cfg.DATASET.DOWNLOAD = False
    cfg.SOLVER.TRAIN_BATCH_SIZE = 16
    cfg.SOLVER.TEST_BATCH_SIZE = 20
    cfg.DATASET.NUM_WORKERS = 1
    yield cfg

    # Teardown: remove data files, or not?
    # files = glob.glob(cfg.DATASET.ROOT)
    # for f in files:
    #     os.remove(f)


@pytest.mark.parametrize("dataset", DATASET_NAMES)
def test_get_cifar(dataset, testing_cfg):
    cfg = testing_cfg
    cfg.DATASET.NAME = dataset
    train_loader, valid_loader = get_cifar(cfg)
    assert isinstance(train_loader, torch.utils.data.DataLoader)
    assert isinstance(valid_loader, torch.utils.data.DataLoader)
</file>

<file path="tests/loaddata/test_multiomics_datasets.py">
import os

import pytest
import torch
from torch_geometric.data import Data, DataLoader

from kale.loaddata.multiomics_datasets import MultiomicsDataset, SparseMultiomicsDataset
from kale.prepdata.tabular_transform import ToOneHotEncoding, ToTensor
from kale.utils.seed import set_seed


@pytest.mark.parametrize("random_split", [True, False])
def test_multiomics_datasets(random_split):
    num_modalities = 3
    num_classes = 2
    url = "https://github.com/pykale/data/raw/main/multiomics/ROSMAP.zip"

    if random_split:
        root = "tests/test_data/multiomics/random/"
        set_seed(2023)
    else:
        root = "tests/test_data/multiomics/pre_split/"

    file_names = []
    for modality in range(1, num_modalities + 1):
        file_names.append(f"{modality}_tr.csv")
        file_names.append(f"{modality}_lbl_tr.csv")
        if not random_split:
            file_names.append(f"{modality}_te.csv")
            file_names.append(f"{modality}_lbl_te.csv")

    dataset = MultiomicsDataset(
        root=root,
        num_modalities=num_modalities,
        num_classes=num_classes,
        url=url,
        raw_file_names=file_names,
        random_split=random_split,
        train_size=0.7,
        pre_transform=ToTensor(dtype=torch.float),
        target_pre_transform=ToOneHotEncoding(dtype=torch.float),
    )

    # Test download method
    assert os.path.isfile(os.path.join(root, "raw/ROSMAP.zip"))
    assert os.path.isfile(os.path.join(root, "processed/data.pt"))

    for modality in range(num_modalities * 4):
        assert os.path.exists(dataset.raw_paths[0])

    # Test load preprocessed data
    assert len(dataset) == 1
    dataloader = DataLoader(dataset, batch_size=1)
    next_batch = next(iter(dataloader))
    assert len(next_batch) == num_modalities

    # Test process method
    assert dataset.len() == num_modalities
    for modality in range(num_modalities):
        data = dataset.get(modality)
        assert isinstance(data, Data)
        assert len(data.x) == data.num_train + data.num_test
        assert len(data.y) == data.num_train + data.num_test
        assert len(data.train_idx) > 0
        assert len(data.train_idx) == data.num_train
        assert len(data.test_idx) > 0
        assert len(data.test_idx) == data.num_test

    for modality in range(num_modalities):
        assert dataset.get(modality).x.dtype == torch.float
        assert dataset.get(modality).y.dtype == torch.float


def test_sparse_multiomics_datasets():
    num_modalities = 3
    num_classes = 2
    url = "https://github.com/pykale/data/raw/main/multiomics/ROSMAP.zip"
    root = "tests/test_data/sparse/"
    file_names = []
    for modality in range(1, num_modalities + 1):
        file_names.append(f"{modality}_tr.csv")
        file_names.append(f"{modality}_lbl_tr.csv")
        file_names.append(f"{modality}_te.csv")
        file_names.append(f"{modality}_lbl_te.csv")

    dataset = SparseMultiomicsDataset(
        root=root,
        raw_file_names=file_names,
        num_modalities=num_modalities,
        num_classes=num_classes,
        edge_per_node=10,
        url=url,
        random_split=False,
        train_size=0.7,
        equal_weight=False,
        pre_transform=ToTensor(dtype=torch.float),
        target_pre_transform=ToOneHotEncoding(dtype=torch.float),
    )

    # Test download method
    assert os.path.isfile(os.path.join(root, "raw/ROSMAP.zip"))
    assert os.path.isfile(os.path.join(root, "processed/data.pt"))

    for modality in range(num_modalities * 4):
        assert os.path.exists(dataset.raw_paths[0])

    assert dataset.num_modalities == num_modalities
    assert dataset.num_classes == num_classes

    # Test process method
    assert dataset.len() == num_modalities

    for modality in range(num_modalities):
        data = dataset.get(modality)
        assert isinstance(data, Data)
        assert len(data.x) == data.num_train + data.num_test
        assert len(data.y) == data.num_train + data.num_test
        assert len(data.train_idx) > 0
        assert len(data.train_idx) == data.num_train
        assert len(data.test_idx) > 0
        assert len(data.test_idx) == data.num_test

    for modality in range(num_modalities):
        assert dataset.get(modality).x.dtype == torch.float
        assert dataset.get(modality).y.dtype == torch.float

    # Test extend_data method
    for modality in range(num_modalities):
        data = dataset.get(modality)
        assert len(data.train_sample_weight) == len(data.y[data.train_idx])
        assert hasattr(data, "train_sample_weight")
        assert hasattr(data, "edge_index_train")
        assert hasattr(data, "edge_weight_train")
        assert hasattr(data, "adj_t_train")

    assert str(dataset) is not None
    assert len(str(dataset)) > 0
</file>

<file path="tests/loaddata/test_sampler.py">
"""
Test for sampler.py API adapted from https://github.com/criteo-research/pytorch-ada/blob/master/tests/datasets/test_sampler.py
"""

import itertools
from collections import Counter

import numpy as np

from kale.loaddata.sampler import BalancedBatchSampler, ReweightedBatchSampler


class MyDataset:
    def __init__(self):
        self.targets = np.tile(["1", "2", "3", "3", "3", "4"], 10)


def idx_to_class(idx):
    idx = idx % 6
    if idx < 2:
        return str(idx + 1)
    elif idx < 5:
        return "3"
    else:
        return "4"


def test_balanced_small_batches():
    sampler = BalancedBatchSampler(MyDataset(), batch_size=4 * 4)
    assert len(sampler) == 3
    batches = list(itertools.islice(sampler, 8))
    assert len(batches) == 3
    for batch in batches:
        assert len(batch) == 16
        counts = Counter(map(idx_to_class, batch))
        assert counts == {"1": 4, "2": 4, "3": 4, "4": 4}

    # no duplicates in the 2 first batches
    first_batches = batches[0] + batches[1]
    assert len(first_batches) == len(set(first_batches))

    # duplicates for classes 1, 2, 4 in the last batch
    for i in range(2, 3):
        first_batches += batches[i]
    counts = Counter(first_batches)
    assert {idx_to_class(idx) for idx, nb in counts.items() if nb > 1} == {"1", "2", "4"}


def test_balanced_bigger_batches():
    sampler = BalancedBatchSampler(MyDataset(), batch_size=12 * 4)
    assert len(sampler) == 1
    batches = list(itertools.islice(sampler, 3))
    assert len(batches) == 1
    batch = batches[0]
    assert len(batch) == 48
    counts = Counter(map(idx_to_class, batch))
    assert counts == {"1": 12, "2": 12, "3": 12, "4": 12}

    # 2 duplicates for class 1, 2, 4
    counts = Counter(batch)
    assert sorted(idx_to_class(idx) for idx, nb in counts.items() if nb > 1) == ["1", "1", "2", "2", "4", "4"]


def test_reweighter():
    sampler = ReweightedBatchSampler(MyDataset(), batch_size=11, class_weights=np.array([1, 2, 1, 3]))
    assert len(sampler) == 5
    one_part = 1 / (1 + 2 + 1 + 3)
    batches = list(itertools.islice(sampler, 10))
    assert len(batches) == 5
    counts = Counter()
    for _ in range(1000):
        for batch in sampler:
            assert len(batch) == 11
            counts.update(map(idx_to_class, batch))
    assert len(counts) == 4
    total = sum(counts.values())
    assert total == 1000 * 5 * 11
    assert round(abs(counts["1"] / total - one_part), 2) == 0
    assert round(abs(counts["2"] / total - 2 * one_part), 2) == 0
    assert round(abs(counts["3"] / total - one_part), 2) == 0
    assert round(abs(counts["4"] / total - 3 * one_part), 2) == 0
</file>

<file path="tests/loaddata/test_tabular_access.py">
import logging

import numpy as np
import pytest

from kale.loaddata.tabular_access import load_csv_columns

# from kale.utils.download import download_file_by_url
from kale.utils.seed import set_seed

# import os


seed = 36
set_seed(seed)

LOGGER = logging.getLogger(__name__)

EXPECTED_COLS = [
    "uid",
    "E-CPV Error",
    "E-CPV Uncertainty",
    "E-MHA Error",
    "E-MHA Uncertainty",
    "S-MHA Error",
    "S-MHA Uncertainty",
    "Validation Fold",
    "Testing Fold",
]


@pytest.mark.parametrize(
    "return_columns",
    [
        ("All", EXPECTED_COLS),
        ([], []),
        ("S-MHA Error", ["S-MHA Error"]),
        (["S-MHA Error", "E-MHA Error"], ["S-MHA Error", "E-MHA Error"]),
    ],
)
def test_load_csv_columns_cols_return(landmark_uncertainty_tuples_path, return_columns):
    returned_cols = load_csv_columns(
        landmark_uncertainty_tuples_path[0],
        "Testing Fold",
        np.arange(8),
        cols_to_return=return_columns[0],
    )
    assert list(returned_cols.columns) == return_columns[1]


# Ensure getting a single fold works
@pytest.mark.parametrize("folds", [0])
def test_load_csv_columns_single_fold(landmark_uncertainty_tuples_path, folds):
    returned_single_fold = load_csv_columns(
        landmark_uncertainty_tuples_path[0],
        "Validation Fold",
        folds,
        cols_to_return=["S-MHA Error", "E-MHA Error", "Validation Fold"],
    )
    assert list(returned_single_fold["Validation Fold"]).count(folds) == len(
        list(returned_single_fold["Validation Fold"])
    )


# Ensure getting a list of folds only return those folds and
# Ensure all samples are being returned
@pytest.mark.parametrize("folds", [[3, 1, 2]])
def test_load_csv_columns_multiple_folds(landmark_uncertainty_tuples_path, folds):
    returned_list_of_folds = load_csv_columns(
        landmark_uncertainty_tuples_path[0],
        "Validation Fold",
        folds,
        cols_to_return=["S-MHA Error", "E-MHA Error", "Validation Fold"],
    )

    assert all(elem in folds for elem in list(returned_list_of_folds["Validation Fold"]))

    assert len(returned_list_of_folds.index) == 114
</file>

<file path="tests/loaddata/test_tdc_datasets.py">
import pytest
import torch

from kale.loaddata.tdc_datasets import BindingDBDataset

SOURCES = ["BindingDB_Kd", "BindingDB_Ki"]


@pytest.mark.parametrize("source_name", SOURCES)
def test_tdc_datasets(download_path, source_name):
    test_dataset = BindingDBDataset(name=source_name, split="test", path=download_path)
    assert isinstance(test_dataset, torch.utils.data.Dataset)
    attributes = ["Drug", "Target", "Y"]
    for attribute in attributes:
        assert attribute in test_dataset.data.columns
    drug, protein, label = test_dataset[0]
    assert drug.dtype == torch.int64
    assert protein.dtype == torch.int64
    assert label.dtype == torch.float32
</file>

<file path="tests/loaddata/test_video_access.py">
import os

import pytest
import torch
from yacs.config import CfgNode

from kale.loaddata.dataset_access import get_class_subset
from kale.loaddata.multi_domain import DomainsDatasetBase
from kale.loaddata.video_access import get_image_modality, VideoDataset, VideoDatasetAccess
from kale.loaddata.video_multi_domain import VideoMultiDomainDatasets
from kale.utils.seed import set_seed

SOURCES = [
    "EPIC;8;epic_D1_train.pkl;epic_D1_test.pkl",
    "ADL;7;adl_P_11_train.pkl;adl_P_11_test.pkl",
    "GTEA;6;gtea_train.pkl;gtea_test.pkl",
    "KITCHEN;6;kitchen_train.pkl;kitchen_test.pkl",
]
TARGETS = [
    "EPIC;8;epic_D1_train.pkl;epic_D1_test.pkl",
    # "ADL;7;adl_P_04_train.pkl;adl_P_04_test.pkl",
    # "GTEA;6;gtea_train.pkl;gtea_test.pkl",
    # "KITCHEN;6;kitchen_train.pkl;kitchen_test.pkl",
]
ALL = SOURCES + TARGETS
IMAGE_MODALITY = ["rgb", "flow", "joint"]
WEIGHT_TYPE = ["natural", "balanced", "preset0"]
# DATASIZE_TYPE = ["max", "source"]
DATASIZE_TYPE = ["max"]
VALID_RATIO = [0.1]
seed = 36
set_seed(seed)
CLASS_SUBSETS = [[1, 3, 8]]


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    cfg = CfgNode()
    cfg.DATASET = CfgNode()
    cfg.DATASET.ROOT = os.path.join(download_path, "video_test_data")
    cfg.DATASET.IMAGE_MODALITY = "joint"
    cfg.DATASET.FRAMES_PER_SEGMENT = 16
    yield cfg


@pytest.mark.parametrize("image_modality", IMAGE_MODALITY)
def test_get_image_modality(image_modality):
    rgb, flow = get_image_modality(image_modality)

    assert isinstance(rgb, bool)
    assert isinstance(flow, bool)


@pytest.mark.parametrize("source_cfg", SOURCES)
@pytest.mark.parametrize("target_cfg", TARGETS)
@pytest.mark.parametrize("valid_ratio", VALID_RATIO)
@pytest.mark.parametrize("weight_type", WEIGHT_TYPE)
@pytest.mark.parametrize("datasize_type", DATASIZE_TYPE)
@pytest.mark.parametrize("class_subset", CLASS_SUBSETS)
def test_get_source_target(source_cfg, target_cfg, valid_ratio, weight_type, datasize_type, testing_cfg, class_subset):
    source_name, source_n_class, source_trainlist, source_testlist = source_cfg.split(";")
    target_name, target_n_class, target_trainlist, target_testlist = target_cfg.split(";")
    n_class = eval(min(source_n_class, target_n_class))

    # get cfg parameters
    cfg = testing_cfg
    cfg.DATASET.SOURCE = source_name
    cfg.DATASET.SRC_TRAINLIST = source_trainlist
    cfg.DATASET.SRC_TESTLIST = source_testlist
    cfg.DATASET.TARGET = target_name
    cfg.DATASET.TGT_TRAINLIST = target_trainlist
    cfg.DATASET.TGT_TESTLIST = target_testlist
    cfg.DATASET.WEIGHT_TYPE = weight_type
    cfg.DATASET.SIZE_TYPE = datasize_type

    # test get_source_target
    source, target, num_classes = VideoDataset.get_source_target(
        VideoDataset(source_name), VideoDataset(target_name), seed, cfg
    )

    assert num_classes == n_class
    assert isinstance(source, dict)
    assert isinstance(target, dict)
    assert isinstance(source["rgb"], VideoDatasetAccess)
    assert isinstance(target["rgb"], VideoDatasetAccess)
    assert isinstance(source["flow"], VideoDatasetAccess)
    assert isinstance(target["flow"], VideoDatasetAccess)

    # test get_train & get_test
    assert isinstance(source["rgb"].get_train(), torch.utils.data.Dataset)
    assert isinstance(source["rgb"].get_test(), torch.utils.data.Dataset)
    assert isinstance(source["flow"].get_train(), torch.utils.data.Dataset)
    assert isinstance(source["flow"].get_test(), torch.utils.data.Dataset)

    # test get_train_valid
    train_valid = source["rgb"].get_train_valid(valid_ratio)
    assert isinstance(train_valid, list)
    assert isinstance(train_valid[0], torch.utils.data.Dataset)
    assert isinstance(train_valid[1], torch.utils.data.Dataset)

    # test action_multi_domain_datasets
    dataset = VideoMultiDomainDatasets(
        source,
        target,
        image_modality=cfg.DATASET.IMAGE_MODALITY,
        seed=seed,
        config_weight_type=cfg.DATASET.WEIGHT_TYPE,
        config_size_type=cfg.DATASET.SIZE_TYPE,
    )
    assert isinstance(dataset, DomainsDatasetBase)

    # test class subsets
    if source_cfg == SOURCES[1] and target_cfg == TARGETS[0]:
        dataset_subset = VideoMultiDomainDatasets(
            source,
            target,
            image_modality="rgb",
            seed=seed,
            config_weight_type=cfg.DATASET.WEIGHT_TYPE,
            config_size_type=cfg.DATASET.SIZE_TYPE,
            class_ids=class_subset,
        )

        train, valid = source["rgb"].get_train_valid(valid_ratio)
        test = source["rgb"].get_test()
        dataset_subset._rgb_source_by_split = {}
        dataset_subset._rgb_target_by_split = {}
        dataset_subset._rgb_source_by_split["train"] = get_class_subset(train, class_subset)
        dataset_subset._rgb_target_by_split["train"] = dataset_subset._rgb_source_by_split["train"]
        dataset_subset._rgb_source_by_split["valid"] = get_class_subset(valid, class_subset)
        dataset_subset._rgb_source_by_split["test"] = get_class_subset(test, class_subset)

        # Ground truth length of the subset dataset
        train_dataset_subset_length = len([1 for data in train if data[1] in class_subset])
        valid_dataset_subset_length = len([1 for data in valid if data[1] in class_subset])
        test_dataset_subset_length = len([1 for data in test if data[1] in class_subset])
        assert len(dataset_subset._rgb_source_by_split["train"]) == train_dataset_subset_length
        assert len(dataset_subset._rgb_source_by_split["valid"]) == valid_dataset_subset_length
        assert len(dataset_subset._rgb_source_by_split["test"]) == test_dataset_subset_length
        assert len(dataset_subset) == train_dataset_subset_length
</file>

<file path="tests/pipeline/test_base_nn_trainer.py">
import pytest
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from kale.embed.image_cnn import SimpleCNNBuilder
from kale.pipeline.base_nn_trainer import BaseNNTrainer, CNNTransformerTrainer, MultimodalNNTrainer
from kale.predict.class_domain_nets import ClassNet


@pytest.fixture
def data():
    # Create dummy data for testing. The dimension is following the CIFAR10 dataset.
    x = torch.randn(8, 3, 32, 32)
    y = torch.randint(0, 2, (8,))
    return TensorDataset(x, y)


@pytest.fixture
def dataloader(data):
    # Create a DataLoader for the dummy data.
    return DataLoader(data, batch_size=4)


@pytest.fixture
def batch(dataloader):
    # Create a batch from the DataLoader.
    return next(iter(dataloader))


class TestBaseTrainer:
    @pytest.fixture
    def trainer(self):
        # Create a default BaseNNTrainer for testing.
        return BaseNNTrainer(None, 5)

    def test_forward(self, trainer, batch):
        # Test forward pass. Return NotImplementError.
        with pytest.raises(NotImplementedError) as excinfo:
            trainer.forward(batch[0])
            assert "Forward pass needs to be defined." in str(excinfo.value)

    def test_compute_loss(self, trainer, batch):
        # Test loss computation. Return NotImplementError.
        with pytest.raises(NotImplementedError) as excinfo:
            trainer.compute_loss(batch)
            assert "Loss function needs to be defined." in str(excinfo.value)

    def test_configure_optimizers_with_default(self, trainer):
        # Test default optimizer configuration. Return a Adam optimizer.
        trainer.optimizers = ClassNet()
        # trainer._parameters = ClassNet()._parameters
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 1
        assert isinstance(optimizers, list)
        assert isinstance(optimizers[0], torch.optim.Adam)
        assert optimizers[0].defaults["lr"] == 0.001

    def test_configure_optimizers_with_adam(self, trainer):
        # Test Adam optimizer configuration. Return a configured Adam optimizer.
        trainer.optimizers = ClassNet()
        trainer._optimizer_params = {"type": "Adam", "optim_params": {"eps": 0.2, "weight_decay": 0.3}}
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 1
        assert isinstance(optimizers, list)
        assert isinstance(optimizers[0], torch.optim.Adam)
        assert optimizers[0].defaults["eps"] == 0.2
        assert optimizers[0].defaults["weight_decay"] == 0.3

    def test_configure_optimizers_with_sgd(self, trainer):
        # Test SGD optimizer configuration. Return a configured SGD optimizer.
        trainer.optimizers = ClassNet()
        trainer._optimizer_params = {"type": "SGD", "optim_params": {"momentum": 0.2, "weight_decay": 0.3}}
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 1
        assert isinstance(optimizers, list)
        assert isinstance(optimizers[0], torch.optim.SGD)
        assert optimizers[0].defaults["momentum"] == 0.2
        assert optimizers[0].defaults["weight_decay"] == 0.3

    def test_configure_optimizers_with_sgd_cosine_annealing(self, trainer):
        # Test SGD optimizer configuration with CosineAnnealingLR scheduler. Return a configured SGD optimizer.
        trainer.optimizers = ClassNet()
        trainer._adapt_lr = True
        trainer._optimizer_params = {"type": "SGD", "optim_params": {"momentum": 0.2, "weight_decay": 0.3}}
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 2
        assert isinstance(optimizers, tuple)
        assert isinstance(optimizers[0], list)
        assert isinstance(optimizers[1], list)
        assert isinstance(optimizers[0][0], torch.optim.SGD)
        assert isinstance(optimizers[1][0], torch.optim.lr_scheduler.CosineAnnealingLR)

    def test_configure_optimizers_with_unknown_type(self, trainer):
        # Test unknown optimizer configuration. Return NotImplementedError.
        trainer._optimizer_params = {"type": "Unknown", "optim_params": {}}
        with pytest.raises(NotImplementedError) as excinfo:
            trainer.configure_optimizers()
            assert "Unknown optimizer type Unknown." in str(excinfo.value)


class TestCNNTransformerTrainer:
    @pytest.fixture
    def trainer(self):
        # Create a CNNTransformerTrainer for testing.
        cnn = SimpleCNNBuilder([[4, 3], [16, 1]])
        classifier = ClassNet()
        return CNNTransformerTrainer(
            feature_extractor=cnn,
            task_classifier=classifier,
            lr_milestones=[1, 2],
            lr_gamma=0.1,
            optimizer={"type": "SGD", "optim_params": {"momentum": 0.2, "weight_decay": 0.3}},
            max_epochs=5,
            adapt_lr=True,
        )

    def test_forward(self, trainer, batch):
        # Test forward pass. Return torch.Tensor.
        output = trainer.forward(batch[0])
        assert isinstance(output, torch.Tensor)

    def test_compute_loss(self, trainer, batch):
        # Test loss computation. Return torch.Tensor and dict.
        loss, log_metrics = trainer.compute_loss(batch)
        assert isinstance(loss, torch.Tensor)
        assert isinstance(log_metrics, dict)

    def test_configure_optimizers_with_default(self, trainer):
        # Test default SGD optimizer configuration. Return a configured SGD optimizer.
        trainer._adapt_lr = False
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 1
        assert isinstance(optimizers, list)
        assert isinstance(optimizers[0], torch.optim.SGD)
        assert optimizers[0].defaults["lr"] == 0.001
        assert optimizers[0].defaults["momentum"] == 0.2
        assert optimizers[0].defaults["weight_decay"] == 0.3

    def test_configure_optimizers_with_adapt_lr(self, trainer):
        # Test SGD optimizer configuration with MultiStepLR scheduler. Return a configured SGD optimizer.
        trainer._adapt_lr = True
        optimizers = trainer.configure_optimizers()
        assert len(optimizers) == 2
        assert isinstance(optimizers, tuple)
        assert isinstance(optimizers[0], list)
        assert isinstance(optimizers[1], list)
        assert isinstance(optimizers[0][0], torch.optim.SGD)
        assert isinstance(optimizers[1][0], torch.optim.lr_scheduler.MultiStepLR)

    def test_training_step(self, trainer, batch):
        # Test training step. Return torch.Tensor.
        loss = trainer.training_step(batch, batch_idx=0)
        assert isinstance(loss, torch.Tensor)

    def test_validation_step(self, trainer, batch):
        # Test validation step.
        trainer.validation_step(batch, batch_idx=0)

    def test_test_step(self, trainer, batch):
        # Test testing step.
        trainer.test_step(batch, batch_idx=0)


class _TestEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(_TestEncoder, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.fc(x)


class _TestFusion(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(_TestFusion, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        x_concat = torch.cat(x, dim=1)
        return self.fc(x_concat)


class _TestClassifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(_TestClassifier, self).__init__()
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        return self.fc(x)


@pytest.fixture
def multimodal_model():
    encoders = [_TestEncoder(5, 10) for _ in range(2)]
    fusion = _TestFusion(20, 10)
    head = _TestClassifier(10, 2)
    model = MultimodalNNTrainer(encoders, fusion, head)
    return model


def test_compute_loss(multimodal_model):
    x = [torch.rand(1, 5) for _ in range(2)]
    y = torch.tensor([1])
    batch = [*x, y]
    loss, metrics = multimodal_model.compute_loss(batch)
    assert isinstance(loss, torch.Tensor)
    assert set(metrics.keys()) == {"loss", "accuracy"}
    assert isinstance(metrics["loss"], float)
    assert isinstance(metrics["accuracy"], float)


def test_training_step(multimodal_model):
    x = [torch.rand(1, 5) for _ in range(2)]
    y = torch.tensor([1])
    batch = [*x, y]
    loss = multimodal_model.training_step(batch, 0)
    assert isinstance(loss, torch.Tensor)


def test_validation_step(multimodal_model):
    x = [torch.rand(1, 5) for _ in range(2)]
    y = torch.tensor([1])
    batch = [*x, y]
    multimodal_model.validation_step(batch, 0)


def test_test_step(multimodal_model):
    x = [torch.rand(1, 5) for _ in range(2)]
    y = torch.tensor([1])
    batch = [*x, y]
    multimodal_model.test_step(batch, 0)


def test_configure_optimizers(multimodal_model):
    optimizer = multimodal_model.configure_optimizers()
    assert isinstance(optimizer, torch.optim.Optimizer)
</file>

<file path="tests/pipeline/test_deepdta.py">
import pytest
import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader

from kale.embed.seq_nn import CNNEncoder
from kale.loaddata.tdc_datasets import BindingDBDataset
from kale.pipeline.deepdta import BaseDTATrainer, DeepDTATrainer
from kale.predict.decode import MLPDecoder

DATASET = "BindingDB_Kd"


def test_deep_data(download_path):
    test_dataset = BindingDBDataset(name=DATASET, split="test", path=download_path)
    subset_indices = list(range(0, 32, 2))
    test_subset = torch.utils.data.Subset(test_dataset, subset_indices)
    test_dataloader = DataLoader(dataset=test_subset, shuffle=False, batch_size=8)
    valid_dataloader = DataLoader(dataset=test_subset, shuffle=True, batch_size=8)
    train_dataloader = DataLoader(dataset=test_subset, shuffle=True, batch_size=4)
    test_batch = next(iter(test_dataloader))

    drug_encoder = CNNEncoder(num_embeddings=64, embedding_dim=128, sequence_length=85, num_kernels=32, kernel_length=8)
    target_encoder = CNNEncoder(
        num_embeddings=25, embedding_dim=128, sequence_length=1200, num_kernels=32, kernel_length=8
    )
    decoder = MLPDecoder(in_dim=192, hidden_dim=16, out_dim=16, include_decoder_layers=True)
    # test deep_dta trainer
    save_parameters = {"seed": 2020, "batch_size": 256}
    model = DeepDTATrainer(drug_encoder, target_encoder, decoder, lr=0.001, ci_metric=True, **save_parameters).eval()
    trainer = pl.Trainer(default_root_dir="./tests/outputs", accelerator="cpu", max_epochs=1, devices=1)
    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)
    trainer.test(dataloaders=test_dataloader)
    assert isinstance(model.drug_encoder, CNNEncoder)
    assert isinstance(model.target_encoder, CNNEncoder)
    assert isinstance(model.decoder, MLPDecoder)
    # model.configure_optimizers()

    # test base_dta trainer
    model = BaseDTATrainer(drug_encoder, target_encoder, decoder, lr=0.001, ci_metric=True, **save_parameters).eval()
    with pytest.raises(NotImplementedError) as excinfo:
        model.training_step(test_batch, 0)
        assert "Forward pass needs to be defined" in str(excinfo.value)
</file>

<file path="tests/pipeline/test_domain_adapter.py">
import pytest

import kale.pipeline.domain_adapter as domain_adapter
from kale.embed.image_cnn import SmallCNNFeature
from kale.loaddata.image_access import DigitDataset
from kale.loaddata.multi_domain import MultiDomainDatasets
from kale.predict.class_domain_nets import ClassNetSmallImage, DomainNetSmallImage
from tests.helpers.pipe_test_helper import ModelTestHelper

# from kale.utils.seed import set_seed

SOURCE = "USPS"
TARGET = "USPS"

DA_METHODS = ["DANN", "CDAN", "CDAN-E", "WDGRL", "WDGRLMod", "DAN", "JAN", "FSDANN", "MME", "Source"]

WEIGHT_TYPE = "natural"
DATASIZE_TYPE = "source"
NUM_CLASSES = 10
FEW_SHOT = [None, 2]
# Not checking values so seed is not needed. If seed, move all seeds to conftest later
# seed = 36
# set_seed(seed)


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    config_params = {
        "train_params": {
            "adapt_lambda": True,
            "adapt_lr": True,
            "lambda_init": 1.0,
            "nb_adapt_epochs": 2,
            "nb_init_epochs": 1,
            "init_lr": 0.001,
            "batch_size": 10,
            "optimizer": {"type": "SGD", "optim_params": {"momentum": 0.9, "weight_decay": 0.0005, "nesterov": True}},
        }
    }
    yield config_params


@pytest.mark.parametrize("da_method", DA_METHODS)
@pytest.mark.parametrize("n_fewshot", FEW_SHOT)
def test_domain_adaptor(da_method, n_fewshot, download_path, testing_cfg):
    if n_fewshot is None:
        if da_method in ["FSDANN", "MME", "Source"]:
            return
    else:
        if da_method in ["DANN", "CDAN", "CDAN-E", "WDGRL", "WDGRLMod", "DAN", "JAN"]:
            return

    source, target, num_channels = DigitDataset.get_source_target(
        DigitDataset(SOURCE), DigitDataset(TARGET), download_path
    )
    dataset = MultiDomainDatasets(
        source, target, config_weight_type=WEIGHT_TYPE, config_size_type=DATASIZE_TYPE, n_fewshot=n_fewshot
    )

    # setup feature extractor
    feature_network = SmallCNNFeature(num_channels)
    # setup classifier
    feature_dim = feature_network.output_size()
    classifier_network = ClassNetSmallImage(feature_dim, NUM_CLASSES)
    train_params = testing_cfg["train_params"]
    method_params = {}
    da_method = domain_adapter.Method(da_method)

    if da_method.is_mmd_method():
        model = domain_adapter.create_mmd_based(
            method=da_method,
            dataset=dataset,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            **method_params,
            **train_params,
        )
    else:  # All other non-mmd DA methods are dann like with critic
        critic_input_size = feature_dim
        # setup critic network
        if da_method.is_cdan_method():
            critic_input_size = 1024
            method_params["use_random"] = True

        critic_network = DomainNetSmallImage(critic_input_size)

        # The following calls kale.loaddata.dataset_access for the first time
        model = domain_adapter.create_dann_like(
            method=da_method,
            dataset=dataset,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            critic=critic_network,
            **method_params,
            **train_params,
        )

    ModelTestHelper.test_model(model, train_params)
</file>

<file path="tests/pipeline/test_fewshot_trainer.py">
import pytest
import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader, TensorDataset
from yacs.config import CfgNode as CfgNode

from kale.embed.image_cnn import ResNet18Feature
from kale.pipeline.fewshot_trainer import ProtoNetTrainer

modes = ["train", "val", "test"]


@pytest.fixture(scope="module")
def testing_cfg():
    _C = CfgNode()
    _C.SEED = 1397
    _C.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    _C.MODEL = CfgNode()
    _C.MODEL.BACKBONE = "ResNet18Feature"
    _C.MODEL.PRETRAIN_WEIGHTS = None

    _C.TRAIN = CfgNode()
    _C.TRAIN.EPOCHS = 1
    _C.TRAIN.OPTIMIZER = "SGD"
    _C.TRAIN.LEARNING_RATE = 1e-3
    _C.TRAIN.NUM_CLASSES = 5
    _C.TRAIN.NUM_SUPPORT_SAMPLES = 5
    _C.TRAIN.NUM_QUERY_SAMPLES = 15

    _C.VAL = CfgNode()
    _C.VAL.NUM_CLASSES = 5
    _C.VAL.NUM_SUPPORT_SAMPLES = 5
    _C.VAL.NUM_QUERY_SAMPLES = 15
    yield _C.clone()


@pytest.fixture
def data():
    # Create dummy data for testing. The dimension is following the CIFAR10 dataset.
    x = torch.randn(5, 20, 3, 84, 84)
    y = torch.randint(0, 10, (5,))
    return TensorDataset(x, y)


@pytest.fixture
def dataloader(data):
    # Create a DataLoader for the dummy data.
    return DataLoader(data, batch_size=5)


@pytest.mark.parametrize("mode", modes)
def test_fewshot_trainer(mode, testing_cfg, dataloader):
    cfg = testing_cfg

    assert len(dataloader) > 0
    net = ResNet18Feature(weights=cfg.MODEL.PRETRAIN_WEIGHTS).to(cfg.DEVICE)
    model = ProtoNetTrainer(
        net=net,
        train_num_classes=cfg.TRAIN.NUM_CLASSES,
        train_num_support_samples=cfg.TRAIN.NUM_SUPPORT_SAMPLES,
        train_num_query_samples=cfg.TRAIN.NUM_QUERY_SAMPLES,
        val_num_classes=cfg.VAL.NUM_CLASSES,
        val_num_support_samples=cfg.VAL.NUM_SUPPORT_SAMPLES,
        val_num_query_samples=cfg.VAL.NUM_QUERY_SAMPLES,
        devices=cfg.DEVICE,
        optimizer=cfg.TRAIN.OPTIMIZER,
        lr=cfg.TRAIN.LEARNING_RATE,
    )
    trainer = pl.Trainer(max_epochs=cfg.TRAIN.EPOCHS, accelerator=cfg.DEVICE)

    if mode == "train":
        for images, _ in dataloader:
            feature_support, feature_query = model.forward(images, cfg.TRAIN.NUM_SUPPORT_SAMPLES, cfg.TRAIN.NUM_CLASSES)
            loss, metrics = model.compute_loss(feature_support, feature_query, mode="train")
            assert isinstance(feature_query, torch.Tensor)
            assert isinstance(feature_support, torch.Tensor)
            assert isinstance(loss, torch.Tensor)
            assert isinstance(metrics, dict)
            break

        assert isinstance(model.configure_optimizers(), torch.optim.Optimizer)
        trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)
    else:
        trainer.test(model, dataloaders=dataloader)
</file>

<file path="tests/pipeline/test_mpca_trainer.py">
import matplotlib.figure
import numpy as np
import pytest
from numpy import testing
from sklearn.metrics import accuracy_score, roc_auc_score

from kale.interpret import model_weights, visualize
from kale.pipeline.mpca_trainer import MPCATrainer

CLASSIFIERS = ["svc", "linear_svc", "lr"]
PARAMS = [
    {"classifier_params": "auto", "mpca_params": None, "n_features": None, "search_params": None},
    {
        "classifier_params": {"C": 1},
        "mpca_params": {"var_ratio": 0.9, "vectorize": True},
        "n_features": 100,
        "search_params": {"cv": 3},
    },
]


@pytest.mark.parametrize("classifier", CLASSIFIERS)
@pytest.mark.parametrize("params", PARAMS)
def test_mpca_trainer(classifier, params, gait):
    x = gait["fea3D"].transpose((3, 0, 1, 2))
    x = x[:20, :]
    y = gait["gnd"][:20].reshape(-1)
    trainer = MPCATrainer(classifier=classifier, **params)
    trainer.fit(x, y)
    y_pred = trainer.predict(x)
    testing.assert_equal(np.unique(y), np.unique(y_pred))
    assert accuracy_score(y, y_pred) >= 0.8

    if classifier == "linear_svc":
        with pytest.raises(Exception):
            y_proba = trainer.predict_proba(x)
    else:
        y_proba = trainer.predict_proba(x)
        assert np.max(y_proba) <= 1.0
        assert np.min(y_proba) >= 0.0
        y_ = np.zeros(y.shape)
        y_[np.where(y == 1)] = 1
        assert roc_auc_score(y_, y_proba[:, 0]) >= 0.8

    y_dec_score = trainer.decision_function(x)
    assert roc_auc_score(y, y_dec_score) >= 0.8

    if classifier == "svc" and trainer.clf.kernel == "rbf":
        with pytest.raises(Exception):
            trainer.mpca.inverse_transform(trainer.clf.coef_)
    else:
        weights = trainer.mpca.inverse_transform(trainer.clf.coef_) - trainer.mpca.mean_
        top_weights = model_weights.select_top_weight(weights, select_ratio=0.1)
        fig = visualize.plot_weights(top_weights[0][0], background_img=x[0][0])
        assert isinstance(fig, matplotlib.figure.Figure)


def test_invalid_init():
    with pytest.raises(Exception):
        MPCATrainer(classifier="Ridge")
    with pytest.raises(Exception):
        MPCATrainer(classifier_params=False)
</file>

<file path="tests/pipeline/test_multi_domain_adapter.py">
import matplotlib.figure
import pytest
import torch
from sklearn.metrics import accuracy_score
from torch.nn.functional import one_hot

from kale.embed.image_cnn import ResNet18Feature
from kale.interpret.visualize import distplot_1d
from kale.loaddata.image_access import ImageAccess
from kale.loaddata.multi_domain import MultiDomainAdapDataset
from kale.pipeline.multi_domain_adapter import CoIRLS, create_ms_adapt_trainer
from kale.predict.class_domain_nets import ClassNetSmallImage
from tests.helpers.pipe_test_helper import ModelTestHelper


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    config_params = {
        "train_params": {
            "adapt_lambda": True,
            "adapt_lr": True,
            "lambda_init": 1,
            "nb_adapt_epochs": 2,
            "nb_init_epochs": 1,
            "init_lr": 0.001,
            "batch_size": 60,
            "optimizer": {"type": "SGD", "optim_params": {"momentum": 0.9, "weight_decay": 0.0005, "nesterov": True}},
        }
    }
    yield config_params


@pytest.fixture(scope="module")
def office_caltech_access(office_path):
    return ImageAccess.get_multi_domain_images("OFFICE_CALTECH", office_path, download=True, return_domain_label=True)


MSDA_METHODS = ["MFSAN", "M3SDA", "DIN"]


@pytest.mark.parametrize("method", MSDA_METHODS)
@pytest.mark.parametrize("input_dimension", [1, 2])
def test_multi_source(method, input_dimension, office_caltech_access, testing_cfg):
    if method != "MFSAN" and input_dimension == 2:
        pytest.skip()
    dataset = MultiDomainAdapDataset(office_caltech_access)
    feature_network = ResNet18Feature()
    # setup classifier
    classifier_network = ClassNetSmallImage
    train_params = testing_cfg["train_params"].copy()

    if method == "MFSAN":
        train_params["input_dimension"] = input_dimension
        if input_dimension == 2:
            feature_network = torch.nn.Sequential(*(list(feature_network.model.children())[:-1]))

    model = create_ms_adapt_trainer(
        method=method,
        dataset=dataset,
        feature_extractor=feature_network,
        task_classifier=classifier_network,
        n_classes=10,
        target_domain="amazon",
        **train_params,
    )

    kwargs = {"limit_train_batches": 0.1, "limit_val_batches": 0.3, "limit_test_batches": 0.2}
    ModelTestHelper.test_model(model, train_params, **kwargs)


@pytest.mark.parametrize("kernel", ["linear", "rbf"])
def test_coirls(kernel, office_caltech_access):
    dataset = MultiDomainAdapDataset(office_caltech_access)
    dataset.prepare_data_loaders()
    dataloader = dataset.get_domain_loaders(split="train", batch_size=100)
    feature_network = ResNet18Feature()

    x, y, z = next(iter(dataloader))
    tgt_idx = torch.where(z == 0)
    src_idx = torch.where(z != 0)

    x_feat = feature_network(x)
    z_one_hot = one_hot(z)
    clf = CoIRLS(kernel=kernel, alpha=1.0)

    x_train = torch.cat((x_feat[src_idx], x_feat[tgt_idx]))
    y_train = y[src_idx]
    z_train = torch.cat((z_one_hot[src_idx], z_one_hot[tgt_idx]))
    clf.fit(x_train, y_train, z_train)
    y_pred = clf.predict(x_feat[tgt_idx])
    acc = accuracy_score(y[tgt_idx], y_pred)

    assert 0 <= acc <= 1

    if kernel == "linear":
        scores = [clf.decision_function(x_feat[src_idx])[:, 0], clf.decision_function(x_feat[tgt_idx])[:, 0]]
        domain_labels = ["source", "target"]
        title = "Decision score distribution"
        title_kwargs = {"fontsize": 14, "fontweight": "bold"}
        hist_kwargs = {"alpha": 0.7}
        fig = distplot_1d(
            scores,
            labels=domain_labels,
            xlabel="Decision Scores",
            ylabel="Count",
            title=title,
            title_kwargs=title_kwargs,
            hist_kwargs=hist_kwargs,
        )
        assert isinstance(fig, matplotlib.figure.Figure)
</file>

<file path="tests/pipeline/test_multiomics_trainer.py">
import pytest
import pytorch_lightning as pl
import torch
from torch.nn import CrossEntropyLoss

from kale.embed.mogonet import MogonetGCN
from kale.loaddata.multiomics_datasets import SparseMultiomicsDataset
from kale.pipeline.multiomics_trainer import MultiomicsTrainer
from kale.predict.decode import LinearClassifier, VCDN
from kale.prepdata.tabular_transform import ToOneHotEncoding, ToTensor
from kale.utils.seed import set_seed

binary_class_data_url = "https://github.com/pykale/data/raw/main/multiomics/ROSMAP.zip"
multi_class_data_url = "https://github.com/pykale/data/raw/main/multiomics/TCGA_BRCA.zip"


@pytest.fixture
def test_model(num_classes, url):
    num_modalities = 3
    gcn_hidden_dim = [200, 200, 100]
    vcdn_hidden_dim = pow(num_classes, num_modalities)
    gcn_dropout_rate = 0.5
    gcn_lr = 5e-4
    vcdn_lr = 1e-3
    loss_function = CrossEntropyLoss(reduction="none")

    unimodal_encoder = []
    unimodal_decoder = []
    multimodal_decoder = None

    if num_classes > 2:
        root = "tests/test_data/multiomics/trainer/multi_class"
    else:
        root = "tests/test_data/multiomics/trainer/binary_class"

    file_names = []
    for modality in range(1, num_modalities + 1):
        file_names.append(f"{modality}_tr.csv")
        file_names.append(f"{modality}_lbl_tr.csv")
        file_names.append(f"{modality}_te.csv")
        file_names.append(f"{modality}_lbl_te.csv")

    dataset = SparseMultiomicsDataset(
        root=root,
        raw_file_names=file_names,
        num_modalities=num_modalities,
        num_classes=num_classes,
        edge_per_node=10,
        url=url,
        random_split=False,
        train_size=0.7,
        equal_weight=False,
        pre_transform=ToTensor(dtype=torch.float),
        target_pre_transform=ToOneHotEncoding(dtype=torch.float),
    )

    for modality in range(num_modalities):
        unimodal_encoder.append(
            MogonetGCN(
                in_channels=dataset.get(modality).num_features,
                hidden_channels=gcn_hidden_dim,
                dropout=gcn_dropout_rate,
            )
        )

        unimodal_decoder.append(LinearClassifier(in_dim=gcn_hidden_dim[-1], out_dim=num_classes))

    if num_modalities >= 2:
        multimodal_decoder = VCDN(num_modalities=num_modalities, num_classes=num_classes, hidden_dim=vcdn_hidden_dim)

    trainer = MultiomicsTrainer(
        dataset=dataset,
        num_modalities=num_modalities,
        num_classes=num_classes,
        unimodal_encoder=unimodal_encoder,
        unimodal_decoder=unimodal_decoder,
        loss_fn=loss_function,
        multimodal_decoder=multimodal_decoder,
        train_multimodal_decoder=True,
        gcn_lr=gcn_lr,
        vcdn_lr=vcdn_lr,
    )

    return trainer


@pytest.mark.parametrize("num_classes, url", [(2, binary_class_data_url), (5, multi_class_data_url)])
def test_init(test_model, num_classes, url):
    assert isinstance(test_model.dataset, SparseMultiomicsDataset)
    assert test_model.num_modalities == 3
    assert test_model.num_classes == num_classes
    assert all(isinstance(encoder, MogonetGCN) for encoder in test_model.unimodal_encoder)
    assert all(isinstance(decoder, LinearClassifier) for decoder in test_model.unimodal_decoder)
    assert isinstance(test_model.loss_fn, CrossEntropyLoss)
    assert isinstance(test_model.multimodal_decoder, VCDN)
    assert test_model.train_multimodal_decoder
    assert test_model.gcn_lr == 5e-4
    assert test_model.vcdn_lr == 1e-3


@pytest.mark.parametrize("num_classes, url", [(2, binary_class_data_url), (5, multi_class_data_url)])
def test_configure_optimizers(test_model, num_classes, url):
    optimizers = test_model.configure_optimizers()
    assert len(optimizers) == 4
    assert all(isinstance(optimizer, torch.optim.Adam) for optimizer in optimizers)
    for modality in range(test_model.num_modalities):
        assert optimizers[modality].param_groups[0]["lr"] == 5e-4
    assert optimizers[test_model.num_modalities].param_groups[0]["lr"] == 1e-3


@pytest.mark.parametrize(
    "num_classes, url, multimodal",
    [
        (2, binary_class_data_url, False),
        (2, binary_class_data_url, True),
        (5, multi_class_data_url, False),
        (5, multi_class_data_url, True),
    ],
)
def test_forward2(test_model, num_classes, url, multimodal):
    x = []
    adj_t = []
    for modality in range(test_model.num_modalities):
        data = test_model.dataset.get(modality)
        x.append(data.x[data.train_idx])
        adj_t.append(data.adj_t_train)

    outputs = test_model.forward(x, adj_t, multimodal)

    assert isinstance(outputs, list) != multimodal
    if not multimodal:
        assert len(outputs) == test_model.num_modalities
        for modality in range(test_model.num_modalities):
            assert outputs[modality].shape == (test_model.dataset.get(modality).num_train, test_model.num_classes)
    else:
        assert isinstance(outputs, torch.Tensor)
        assert outputs.shape == (test_model.dataset.get(0).num_train, test_model.num_classes)
        test_model.multimodal_decoder = None
        with pytest.raises(TypeError):
            _ = test_model.forward(x, adj_t, multimodal)


@pytest.mark.parametrize("num_classes, url", [(2, binary_class_data_url), (5, multi_class_data_url)])
def test_pipeline(test_model, num_classes, url):
    set_seed(2023)
    trainer_pretrain = pl.Trainer(
        default_root_dir="./tests/outputs",
        max_epochs=2,
        accelerator="cpu",
        enable_model_summary=False,
    )
    trainer_pretrain.fit(test_model)
    result = trainer_pretrain.test(test_model)
    if num_classes > 2:
        assert 0 <= result[0]["Accuracy"] <= 1
        assert 0 <= result[0]["F1 weighted"] <= 1
        assert 0 <= result[0]["F1 macro"] <= 1
    else:
        assert 0 <= result[0]["Accuracy"] <= 1
        assert 0 <= result[0]["F1"] <= 1
        assert 0 <= result[0]["AUC"] <= 1

    assert str(test_model) is not None
    assert len(str(test_model)) > 0
</file>

<file path="tests/pipeline/test_uncertainty_qbin_pipeline.py">
import os
from typing import Any

import numpy as np
import pandas as pd
import pytest
import seaborn as sns

import kale.utils.logger as logging
from kale.embed.uncertainty_fitting import fit_and_predict
from kale.interpret.uncertainty_quantiles import generate_fig_comparing_bins, generate_fig_individual_bin_comparison


@pytest.fixture(scope="module")
def testing_cfg():
    config_params = {
        "DATASET": {
            "SOURCE": "https://github.com/pykale/data/raw/main/tabular/cardiac_landmark_uncertainty/Uncertainty_tuples.zip",
            "ROOT": "tests/test_data",
            "BASE_DIR": "Uncertainty_tuples",
            "FILE_FORMAT": "zip",
            "CONFIDENCE_INVERT": [["S-MHA", True], ["E-MHA", True], ["E-CPV", False]],
            "DATA": "4CH",
            "LANDMARKS": [0, 1, 2],
            "NUM_FOLDS": 8,
            "GROUND_TRUTH_TEST_ERRORS_AVAILABLE": True,
            "UE_PAIRS_VAL": "uncertainty_pairs_valid",
            "UE_PAIRS_TEST": "uncertainty_pairs_test",
        },
        "PIPELINE": {
            "NUM_QUANTILE_BINS": [3, 5],
            "COMPARE_INDIVIDUAL_Q": True,
            "INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS": [
                ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
                ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
            ],
            "INDIVIDUAL_Q_MODELS": ["U-NET", "PHD-NET"],
            "COMPARE_Q_VALUES": True,
            "COMPARE_Q_MODELS": ["PHD-NET", "U-NET"],
            "COMPARE_Q_UNCERTAINTY_ERROR_PAIRS": [
                ["E-MHA", "E-MHA Error", "E-MHA Uncertainty"],
                ["S-MHA", "S-MHA Error", "S-MHA Uncertainty"],
            ],
            "COMBINE_MIDDLE_BINS": False,
            "PIXEL_TO_MM_SCALE": 1.0,
            "IND_LANDMARKS_TO_SHOW": [0],
            "SHOW_IND_LANDMARKS": True,
        },
        "IM_KWARGS": {"cmap": "gray"},
        "MARKER_KWARGS": {
            "marker": "o",
            "markerfacecolor": (1, 1, 1, 0.1),
            "markeredgewidth": 1.5,
            "markeredgecolor": "r",
        },
        "WEIGHT_KWARGS": {"markersize": 6, "alpha": 0.7},
        "BOXPLOT": {"SAMPLES_AS_DOTS": False, "ERROR_LIM": 256, "SHOW_SAMPLE_INFO_MODE": "Average"},
        "OUTPUT": {"SAVE_FOLDER": "tests/test_data", "SAVE_PREPEND": "testing", "SAVE_FIGURES": True},
    }

    yield config_params


# DEFINE constants for testing
EXPECTED_FILES_IND_3 = [
    "3Bins/fitted_quantile_binningcumulative_error.pdf",
    "3Bins/fitted_quantile_binning/target_errors.xlsx",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undotted_error_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undotted_error_target_0.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_errorbound_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_errorbound_target_0.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_jaccard_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_recall_jaccard_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_precision_jaccard_all_targets.pdf",
    "3Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalsejaccard_target_0.pdf",
]
# /home/schobs/Documents/results/testing/4CH/3Bins/
EXPECTED_FILES_IND_5 = [
    "5Bins/fitted_quantile_binningcumulative_error.pdf",
    "5Bins/fitted_quantile_binning/target_errors.xlsx",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undotted_error_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undotted_error_target_0.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_errorbound_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_errorbound_target_0.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_jaccard_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_recall_jaccard_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalse_precision_jaccard_all_targets.pdf",
    "5Bins/fitted_quantile_binning/testing_ind_4CH_U-NET_PHD-NET_S-MHA_E-MHA_combinedFalsejaccard_target_0.pdf",
]
EXPECED_FILES_FIT = [
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t0.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t0.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t0.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t1.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t1.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t1.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t2.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t2.csv",
    "3Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t2.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t0.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t0.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t0.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t1.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t1.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t1.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t2.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t2.csv",
    "3Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t2.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t0.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t0.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t0.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t1.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t1.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t1.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/res_predicted_bins_t2.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/estimated_error_bounds_t2.csv",
    "5Bins/fitted_quantile_binning/PHD-NET/4CH/uncertainty_bounds_t2.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t0.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t0.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t0.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t1.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t1.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t1.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/res_predicted_bins_t2.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/estimated_error_bounds_t2.csv",
    "5Bins/fitted_quantile_binning/U-NET/4CH/uncertainty_bounds_t2.csv",
]
EXPECTED_FILES_COMP = [
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_undotted_error_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_undotted_error_target_0.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_errorbound_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_errorbound_target_0.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_recall_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalse_precision_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_E-MHA_4CH_combinedFalsejaccard_target_0.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_undotted_error_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_undotted_error_target_0.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_errorbound_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_errorbound_target_0.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_recall_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalse_precision_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_PHD-NET_S-MHA_4CH_combinedFalsejaccard_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_undotted_error_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_undotted_error_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_errorbound_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_errorbound_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_recall_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalse_precision_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_E-MHA_4CH_combinedFalsejaccard_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_undotted_error_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_undotted_error_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_undottedmean_error_folds_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_errorbound_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_errorbound_target_0.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_recall_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalse_precision_jaccard_all_targets.pdf",
    "ComparisonBins/testing_compQ_U-NET_S-MHA_4CH_combinedFalsejaccard_target_0.pdf",
]


def test_qbin_pipeline(testing_cfg):
    """Test the uncertainty quantile binning pipeline."""

    # ---- setup output ----
    os.makedirs(testing_cfg["OUTPUT"]["SAVE_FOLDER"], exist_ok=True)
    logger = logging.construct_logger("q_bin", testing_cfg["OUTPUT"]["SAVE_FOLDER"], log_to_terminal=True)

    # ---- setup dataset ----
    base_dir = testing_cfg["DATASET"]["BASE_DIR"]

    uncertainty_pairs_val = testing_cfg["DATASET"]["UE_PAIRS_VAL"]
    uncertainty_pairs_test = testing_cfg["DATASET"]["UE_PAIRS_TEST"]
    gt_test_error_available = testing_cfg["DATASET"]["GROUND_TRUTH_TEST_ERRORS_AVAILABLE"]

    ind_q_uncertainty_error_pairs = testing_cfg["PIPELINE"]["INDIVIDUAL_Q_UNCERTAINTY_ERROR_PAIRS"]
    ind_q_models_to_compare = testing_cfg["PIPELINE"]["INDIVIDUAL_Q_MODELS"]

    compare_q_uncertainty_error_pairs = testing_cfg["PIPELINE"]["COMPARE_Q_UNCERTAINTY_ERROR_PAIRS"]
    compare_q_models_to_compare = testing_cfg["PIPELINE"]["COMPARE_Q_MODELS"]

    dataset = testing_cfg["DATASET"]["DATA"]
    landmarks = testing_cfg["DATASET"]["LANDMARKS"]
    num_folds = testing_cfg["DATASET"]["NUM_FOLDS"]

    ind_landmarks_to_show = testing_cfg["PIPELINE"]["IND_LANDMARKS_TO_SHOW"]

    pixel_to_mm_scale = testing_cfg["PIPELINE"]["PIXEL_TO_MM_SCALE"]

    # Define parameters for visualization
    cmaps = sns.color_palette("deep", 10).as_hex()

    show_individual_landmark_plots = testing_cfg["PIPELINE"]["SHOW_IND_LANDMARKS"]

    for num_bins in testing_cfg["PIPELINE"]["NUM_QUANTILE_BINS"]:
        # create the folder to save to
        save_folder = os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset, str(num_bins) + "Bins")

        # ---- This is the Fitting Phase ----
        # Fit all the options for the individual q selection and comparison q selection

        all_models_to_compare = np.unique(ind_q_models_to_compare + compare_q_models_to_compare)
        all_uncert_error_pairs_to_compare = np.unique(
            ind_q_uncertainty_error_pairs + compare_q_uncertainty_error_pairs, axis=0
        )

        for model in all_models_to_compare:
            for landmark in landmarks:
                # Define Paths for this loop
                landmark_results_path_val = os.path.join(
                    testing_cfg["DATASET"]["ROOT"],
                    base_dir,
                    model,
                    dataset,
                    uncertainty_pairs_val + "_t" + str(landmark),
                )
                landmark_results_path_test = os.path.join(
                    testing_cfg["DATASET"]["ROOT"],
                    base_dir,
                    model,
                    dataset,
                    uncertainty_pairs_test + "_t" + str(landmark),
                )

                fitted_save_at = os.path.join(save_folder, "fitted_quantile_binning", model, dataset)
                os.makedirs(save_folder, exist_ok=True)

                helper_test_qbin_fit(
                    landmark,
                    all_uncert_error_pairs_to_compare,
                    landmark_results_path_val,
                    landmark_results_path_test,
                    num_bins,
                    testing_cfg,
                    gt_test_error_available,
                    fitted_save_at,
                    model,
                )

        # Evaluation Phase ##########################

        comparisons_models = "_".join(ind_q_models_to_compare)

        comparisons_um = [str(x[0]) for x in ind_q_uncertainty_error_pairs]
        comparisons_um = "_".join(comparisons_um)

        save_file_preamble = "_".join(
            [
                testing_cfg["OUTPUT"]["SAVE_PREPEND"],
                "ind",
                dataset,
                comparisons_models,
                comparisons_um,
                "combined" + str(testing_cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"]),
            ]
        )

        generate_fig_individual_bin_comparison(
            data=[
                ind_q_uncertainty_error_pairs,
                ind_q_models_to_compare,
                dataset,
                landmarks,
                num_bins,
                cmaps,
                os.path.join(save_folder, "fitted_quantile_binning"),
                save_file_preamble,
                testing_cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"],
                testing_cfg["OUTPUT"]["SAVE_FIGURES"],
                testing_cfg["DATASET"]["CONFIDENCE_INVERT"],
                testing_cfg["BOXPLOT"]["SAMPLES_AS_DOTS"],
                testing_cfg["BOXPLOT"]["SHOW_SAMPLE_INFO_MODE"],
                testing_cfg["BOXPLOT"]["ERROR_LIM"],
                show_individual_landmark_plots,
                True,
                num_folds,
                ind_landmarks_to_show,
                pixel_to_mm_scale,
            ],
            display_settings={
                "cumulative_error": True,
                "errors": True,
                "jaccard": True,
                "error_bounds": True,
                "correlation": False,
            },
        )

        if num_bins == 3:
            exp = EXPECTED_FILES_IND_3
        else:
            exp = EXPECTED_FILES_IND_5

        search_dir = os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset, "/")

        helper_test_expected_files_exist(exp, search_dir)

        # Now delete for memory
        for expected_file in exp:
            file_path = os.path.join(os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset), expected_file)
            os.remove(file_path)

    # If we are comparing bins against each other, we need to wait until all the bins have been fitted.
    for c_model in compare_q_models_to_compare:
        for c_er_pair in compare_q_uncertainty_error_pairs:
            save_file_preamble = "_".join(
                [
                    testing_cfg["OUTPUT"]["SAVE_PREPEND"],
                    "compQ",
                    c_model,
                    c_er_pair[0],
                    dataset,
                    "combined" + str(testing_cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"]),
                ]
            )

            all_fitted_save_paths = [
                os.path.join(
                    testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset, str(x_bins) + "Bins", "fitted_quantile_binning"
                )
                for x_bins in testing_cfg["PIPELINE"]["NUM_QUANTILE_BINS"]
            ]

            hatch_type = "o" if "PHD-NET" == c_model else ""
            color = cmaps[0] if c_er_pair[0] == "S-MHA" else cmaps[1] if c_er_pair[0] == "E-MHA" else cmaps[2]
            save_folder_comparison = os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset, "ComparisonBins")
            os.makedirs(save_folder_comparison, exist_ok=True)

            logger.info("Comparison Q figures for: %s and %s ", c_model, c_er_pair)
            generate_fig_comparing_bins(
                data=[
                    c_er_pair,
                    c_model,
                    dataset,
                    landmarks,
                    testing_cfg["PIPELINE"]["NUM_QUANTILE_BINS"],
                    cmaps,
                    all_fitted_save_paths,
                    save_folder_comparison,
                    save_file_preamble,
                    testing_cfg["PIPELINE"]["COMBINE_MIDDLE_BINS"],
                    testing_cfg["OUTPUT"]["SAVE_FIGURES"],
                    testing_cfg["BOXPLOT"]["SAMPLES_AS_DOTS"],
                    testing_cfg["BOXPLOT"]["SHOW_SAMPLE_INFO_MODE"],
                    testing_cfg["BOXPLOT"]["ERROR_LIM"],
                    show_individual_landmark_plots,
                    True,
                    num_folds,
                    ind_landmarks_to_show,
                    pixel_to_mm_scale,
                ],
                display_settings={
                    "cumulative_error": True,
                    "errors": True,
                    "jaccard": True,
                    "error_bounds": True,
                    "hatch": hatch_type,
                    "color": color,
                },
            )

    helper_test_expected_files_exist(EXPECTED_FILES_COMP, os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset))

    # Now delete for memory
    for expected_file in EXPECTED_FILES_COMP:
        file_path = os.path.join(os.path.join(testing_cfg["OUTPUT"]["SAVE_FOLDER"], dataset), expected_file)
        os.remove(file_path)


def helper_test_expected_files_exist(expected_files, save_folder):
    """
    Test whether all expected files are present in the specified directory or any of its subdirectories.

    Args:
        expected_files (list): A list of file names or paths that are expected to be found in the `save_folder` or its subdirectories.
        save_folder (str): The root directory in which to search for the expected files.

    Returns:
        None

    Raises:
        AssertionError: If any expected file is not found in the `save_folder` or its subdirectories.
    """
    num_found = 0
    not_found_files = []
    found_files = []
    for expected_file in expected_files:
        found = False
        for dirpath, _, _ in os.walk(save_folder):
            file_path = os.path.join(dirpath, expected_file)
            if os.path.isfile(file_path):
                num_found += 1
                found_files.append(expected_file)
                found = True
                break
        if not found:
            not_found_files.append(expected_file)

    assert num_found == len(expected_files), (
        "Not all expected files were found in the save folder: "
        + str(not_found_files)
        + "\n FOUND: "
        + str(found_files)
    )


def helper_test_qbin_fit(
    landmark: int,
    all_uncert_error_pairs_to_compare: np.ndarray,
    landmark_results_path_val: str,
    landmark_results_path_test: str,
    num_bins: int,
    cfg: Any,
    gt_test_error_available: bool,
    save_folder: str,
    model: str,
) -> None:
    """
    Test the `fit_and_predict()` function with a specific landmark.

    Args:
        landmark (int): The landmark index to use for testing.
        all_uncert_error_pairs_to_compare (np.ndarray): An array of shape `(N, 2)` containing uncertainty-error
            pairs to use for testing.
        landmark_results_path_val (str): The path to the directory containing validation results for the
            specified landmark.
        landmark_results_path_test (str): The path to the directory containing test results for the specified
            landmark.
        num_bins (int): The number of bins to use for quantile binning.
        cfg (Any): A configuration object containing various settings for the algorithm.
        gt_test_error_available (bool): Whether or not ground truth test errors are available.
        save_folder (str): The path to the directory where intermediate results should be saved.

    Returns:
        None

    Raises:
        AssertionError: If any of the tests fail.

    This function tests the `fit_and_predict()` function with a specific landmark and set of inputs. It first creates
    a directory at the `save_folder` path if one does not already exist. It then calls `fit_and_predict()` with the
    specified inputs to generate predictions and associated uncertainty estimates. The function then tests the
    resulting `estimated_errors`, `uncert_boundaries`, and `predicted_bins` arrays using the `csv_equality_helper()`
    function.

    Note that this function assumes that there are ground truth test errors available for testing. If ground truth
    errors are not available, you should set `gt_test_error_available` to `False`.
    """
    # Create the `save_folder` directory if it does not exist
    os.makedirs(save_folder, exist_ok=True)

    # Call `fit_and_predict()` with the specified inputs
    uncert_boundaries, estimated_errors, predicted_bins = fit_and_predict(
        landmark,
        all_uncert_error_pairs_to_compare,
        landmark_results_path_val,
        landmark_results_path_test,
        num_bins,
        cfg,
        groundtruth_test_errors=gt_test_error_available,
        save_folder=save_folder,
    )

    # Test `estimated_errors` using `csv_equality_helper()`
    csv_equality_helper(
        estimated_errors,
        os.path.join(cfg["DATASET"]["ROOT"], cfg["DATASET"]["BASE_DIR"], model)
        + "/4CH/"
        + str(num_bins)
        + "Bins_fit/estimated_error_bounds",
        landmark,
    )

    # Test `uncert_boundaries` using `csv_equality_helper()`
    csv_equality_helper(
        uncert_boundaries,
        os.path.join(cfg["DATASET"]["ROOT"], cfg["DATASET"]["BASE_DIR"], model)
        + "/4CH/"
        + str(num_bins)
        + "Bins_fit/uncertainty_bounds",
        landmark,
    )

    # Test `predicted_bins` using `csv_equality_helper()`
    csv_equality_helper(
        predicted_bins,
        os.path.join(cfg["DATASET"]["ROOT"], cfg["DATASET"]["BASE_DIR"], model)
        + "/4CH/"
        + str(num_bins)
        + "Bins_fit/res_predicted_bins",
        landmark,
    )


def csv_equality_helper(array, csv_preamble, landmark):
    """
    Test if a given numpy array is equal to a csv file with a specific landmark and path.

    Parameters:
        array (numpy.ndarray): The numpy array to test for equality with the CSV file.
        csv_preamble (str): The path preamble for the CSV file to be compared with the numpy array.
        landmark (int): The landmark index for the CSV file.

    Raises:
        AssertionError: If the numpy array and the CSV file do not match.

    Returns:
        None.
    """

    array = array.to_numpy()
    # Convert the DataFrame to a numpy array
    read_array = read_csv_landmark(csv_preamble, landmark)

    for idx, val in enumerate(read_array):
        for inner_idx, inner_val in enumerate(val):
            if inner_idx == 0:
                continue
            assert str(inner_val) == str(array[idx][inner_idx])


def read_csv_landmark(csv_preamble, landmark):
    """
    Read a CSV file with landmark-specific data.

    Args:
        csv_preamble (str): The path preamble to the directory containing the CSV file.
        landmark (int): The landmark index associated with the CSV file.

    Returns:
        np.ndarray: A numpy array containing the contents of the CSV file.

    Raises:
        FileNotFoundError: If the CSV file at the specified path cannot be found.
        ValueError: If the CSV file contains invalid data.

    This function reads a CSV file containing landmark-specific data. The path to the CSV file is constructed
    from the `csv_preamble` argument and the `landmark` argument. The CSV file should contain only numeric data,
    with one row per data point and one column per feature. The function reads the CSV file using the pandas
    `read_csv()` function and converts the resulting DataFrame to a numpy array using the `to_numpy()` method.
    If the CSV file cannot be found or contains invalid data, the function raises an appropriate error.
    """

    csv_path = os.path.join(csv_preamble + "_t" + str(landmark) + ".csv")
    # Read the CSV file
    df = pd.read_csv(csv_path)

    # Convert the DataFrame to a numpy array
    read_array = df.to_numpy()
    return read_array
</file>

<file path="tests/pipeline/test_video_domain_adapter.py">
from pathlib import Path

import pytest
from yacs.config import CfgNode

from kale.loaddata.video_access import VideoDataset
from kale.loaddata.video_multi_domain import VideoMultiDomainDatasets
from kale.pipeline import domain_adapter, video_domain_adapter
from kale.predict.class_domain_nets import ClassNetVideo, DomainNetVideo
from kale.utils.seed import set_seed
from tests.helpers.boring_model import VideoBoringModel
from tests.helpers.pipe_test_helper import ModelTestHelper

SOURCES = [
    "ADL;7;adl_P_11_train.pkl;adl_P_11_test.pkl",
]
TARGETS = [
    "ADL;7;adl_P_11_train.pkl;adl_P_11_test.pkl",
]
ALL = SOURCES + TARGETS
IMAGE_MODALITY = ["rgb", "flow", "joint"]
DA_METHODS = ["DANN", "CDAN", "CDAN-E", "WDGRL", "DAN", "JAN", "Source"]
WEIGHT_TYPE = "natural"
DATASIZE_TYPE = "max"
VALID_RATIO = 0.1
seed = 36
set_seed(seed)

root_dir = str(Path.cwd().parent.parent)


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    cfg = CfgNode()
    cfg.DATASET = CfgNode()
    cfg.DATASET.ROOT = str(Path(download_path) / "video_test_data")
    cfg.DAN = CfgNode()
    cfg.DATASET.FRAMES_PER_SEGMENT = 16
    yield cfg


@pytest.fixture(scope="module")
def testing_training_cfg():
    config_params = {
        "train_params": {
            "adapt_lambda": True,
            "adapt_lr": True,
            "lambda_init": 1,
            "nb_adapt_epochs": 2,
            "nb_init_epochs": 1,
            "init_lr": 0.001,
            "batch_size": 2,
            "optimizer": {"type": "SGD", "optim_params": {"momentum": 0.9, "weight_decay": 0.0005, "nesterov": True}},
        }
    }
    yield config_params


@pytest.mark.parametrize("source_cfg", SOURCES)
@pytest.mark.parametrize("target_cfg", TARGETS)
@pytest.mark.parametrize("image_modality", IMAGE_MODALITY)
@pytest.mark.parametrize("da_method", DA_METHODS)
def test_video_domain_adapter(source_cfg, target_cfg, image_modality, da_method, testing_cfg, testing_training_cfg):
    source_name, source_n_class, source_trainlist, source_testlist = source_cfg.split(";")
    target_name, target_n_class, target_trainlist, target_testlist = target_cfg.split(";")

    # get cfg parameters
    cfg = testing_cfg
    cfg.DATASET.SOURCE = source_name
    cfg.DATASET.SRC_TRAINLIST = source_trainlist
    cfg.DATASET.SRC_TESTLIST = source_testlist
    cfg.DATASET.TARGET = target_name
    cfg.DATASET.TGT_TRAINLIST = target_trainlist
    cfg.DATASET.TGT_TESTLIST = target_testlist
    cfg.DATASET.IMAGE_MODALITY = image_modality
    cfg.DATASET.WEIGHT_TYPE = WEIGHT_TYPE
    cfg.DATASET.SIZE_TYPE = DATASIZE_TYPE
    cfg.DAN.USERANDOM = False

    # build dataset
    source, target, num_classes = VideoDataset.get_source_target(
        VideoDataset(source_name), VideoDataset(target_name), seed, cfg
    )

    dataset = VideoMultiDomainDatasets(
        source,
        target,
        image_modality=cfg.DATASET.IMAGE_MODALITY,
        seed=seed,
        config_weight_type=cfg.DATASET.WEIGHT_TYPE,
        config_size_type=cfg.DATASET.SIZE_TYPE,
    )

    # setup feature extractor
    if cfg.DATASET.IMAGE_MODALITY in ["rgb", "flow"]:
        class_feature_dim = 1024
        domain_feature_dim = class_feature_dim
        if cfg.DATASET.IMAGE_MODALITY == "rgb":
            feature_network = {"rgb": VideoBoringModel(3), "flow": None}
        else:
            feature_network = {"rgb": None, "flow": VideoBoringModel(2)}
    else:
        class_feature_dim = 2048
        domain_feature_dim = int(class_feature_dim / 2)
        feature_network = {"rgb": VideoBoringModel(3), "flow": VideoBoringModel(2)}

    # setup classifier
    classifier_network = ClassNetVideo(input_size=class_feature_dim, n_class=num_classes)
    train_params = testing_training_cfg["train_params"]
    method_params = {}
    method = domain_adapter.Method(da_method)

    # setup DA method
    if method.is_mmd_method():
        model = video_domain_adapter.create_mmd_based_video(
            method=method,
            dataset=dataset,
            image_modality=cfg.DATASET.IMAGE_MODALITY,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            **method_params,
            **train_params,
        )
    else:
        critic_input_size = domain_feature_dim
        # setup critic network
        if method.is_cdan_method():
            if cfg.DAN.USERANDOM:
                critic_input_size = 1024
            else:
                critic_input_size = domain_feature_dim * num_classes
        critic_network = DomainNetVideo(input_size=critic_input_size)

        if da_method == "CDAN":
            method_params["use_random"] = cfg.DAN.USERANDOM

        model = video_domain_adapter.create_dann_like_video(
            method=method,
            dataset=dataset,
            image_modality=cfg.DATASET.IMAGE_MODALITY,
            feature_extractor=feature_network,
            task_classifier=classifier_network,
            critic=critic_network,
            **method_params,
            **train_params,
        )

    ModelTestHelper.test_model(model, train_params)
</file>

<file path="tests/predict/test_class_domain_nets.py">
import pytest
import torch

from kale.predict.class_domain_nets import (
    ClassNet,
    ClassNetSmallImage,
    ClassNetVideo,
    ClassNetVideoConv,
    DomainNetSmallImage,
    DomainNetVideo,
    SoftmaxNet,
)
from kale.utils.seed import set_seed

set_seed(36)
BATCH_SIZE = 2
# The default input shape for basic ClassNet and DomainNet is batch_size * dimension. However, for ClassNetVideoConv,
# the input is the output of the I3D last average pooling layer and the shape is
# batch_size * num_channel * frame_per_segment * height * weight.
INPUT_BATCH = torch.randn(BATCH_SIZE, 128)
INPUT_BATCH_AVERAGE = torch.randn(BATCH_SIZE, 1024, 1, 1, 1)
# The default input shape for ClassNet module is batch_size * channels * height * width
INPUT_BATCH_CLASSNET = torch.randn(BATCH_SIZE, 64, 8, 8)
CLASSNET_MODEL = [ClassNetSmallImage, ClassNetVideo]
DOMAINNET_MODEL = [DomainNetSmallImage, DomainNetVideo]


def test_softmaxnet_shapes():
    model = SoftmaxNet(input_dim=128, n_classes=8)
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, 8)


def test_classnet_shapes():
    model = ClassNet()
    model.eval()
    output_batch = model(INPUT_BATCH_CLASSNET)
    assert output_batch.size() == (BATCH_SIZE, 10)  # (batch size, num_classes)


@pytest.mark.parametrize("model", CLASSNET_MODEL)
def test_classnetmodel_shapes(model):
    model = model(input_size=128, n_class=8)
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, 8)


def test_classnetvideoconv_shapes():
    model = ClassNetVideoConv(n_class=8)
    model.eval()
    output_batch = model(INPUT_BATCH_AVERAGE)
    assert output_batch.size() == (BATCH_SIZE, 8, 1, 1, 1)


@pytest.mark.parametrize("model", DOMAINNET_MODEL)
def test_domainnet_shapes(model):
    model = model(input_size=128)
    model.eval()
    output_batch = model(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, 2)
</file>

<file path="tests/predict/test_decode.py">
import pytest
import torch

from kale.predict.decode import LinearClassifier, MLPDecoder, VCDN
from kale.utils.seed import set_seed


def test_mlp_decoder():
    # Test with additional layers
    in_dim, hidden_dim, out_dim = 8, 16, 32
    include_decoder_layers = True
    dropout_rate = 0.1
    mlp_decoder = MLPDecoder(
        in_dim=in_dim,
        hidden_dim=hidden_dim,
        out_dim=out_dim,
        dropout_rate=dropout_rate,
        include_decoder_layers=include_decoder_layers,
    )
    assert mlp_decoder.fc1.weight.size() == (hidden_dim, in_dim)
    assert mlp_decoder.fc2.weight.size() == (hidden_dim, hidden_dim)
    assert mlp_decoder.fc3.weight.size() == (out_dim, hidden_dim)
    assert mlp_decoder.fc4.weight.size() == (1, out_dim)
    input_batch = torch.randn((16, in_dim))
    output = mlp_decoder(input_batch)
    assert output.size() == (16, 1)

    # Test without additional layers
    in_dim, hidden_dim, out_dim = 8, 16, 2
    include_decoder_layers = False
    mlp_decoder = MLPDecoder(
        in_dim=in_dim,
        hidden_dim=hidden_dim,
        out_dim=out_dim,
        dropout_rate=dropout_rate,
        include_decoder_layers=include_decoder_layers,
    )
    assert mlp_decoder.fc1.weight.size() == (hidden_dim, in_dim)
    assert mlp_decoder.fc2.weight.size() == (out_dim, hidden_dim)
    assert not hasattr(mlp_decoder, "fc3")  # There should be no fc3 layer
    assert not hasattr(mlp_decoder, "fc4")  # There should be no fc4 layer
    input_batch = torch.randn((16, in_dim))
    output = mlp_decoder(input_batch)
    assert output.size() == (16, out_dim)


def test_linear_classifier_shape():
    in_dim = 10
    out_dim = 5
    batch_size = 16
    x = torch.randn(batch_size, in_dim)
    model = LinearClassifier(in_dim, out_dim)
    y = model(x)
    # model shape test
    assert model.fc.weight.size() == (out_dim, in_dim)
    # output shape test
    assert y.shape == (batch_size, out_dim)


def test_linear_classifier_no_bias():
    in_dim = 10
    out_dim = 5
    model = LinearClassifier(in_dim, out_dim, bias=False)
    assert model.fc.bias is None


def test_linear_classifier_parameter_initialization():
    in_dim = 10
    out_dim = 5
    set_seed(2021)
    model = LinearClassifier(in_dim, out_dim)
    for name, param in model.named_parameters():
        if "bias" in name:
            assert torch.allclose(param.data, torch.zeros_like(param.data))
        else:
            assert param.std().detach().numpy() == pytest.approx(1 / in_dim**0.5, rel=1e-1)


@pytest.fixture
def vcdn():
    num_modalities = 3
    num_classes = 4
    hidden_dim = pow(num_classes, num_modalities)
    return VCDN(num_modalities=num_modalities, num_classes=num_classes, hidden_dim=hidden_dim)


def test_vcdn_forward(vcdn):
    x1 = torch.randn(2, 4)
    x2 = torch.randn(2, 4)
    x3 = torch.randn(2, 4)
    output = vcdn([x1, x2, x3])
    assert output.shape == (2, 4)


def test_vcdn_reset_parameters(vcdn):
    # Set parameters to a fixed value
    with torch.no_grad():
        for param in vcdn.parameters():
            param.fill_(1.0)
    # Reset the parameters
    vcdn.reset_parameters()
    # Check that the parameters are now different
    for param in vcdn.parameters():
        assert torch.any(param != 1.0)
</file>

<file path="tests/predict/test_isonet.py">
import pytest
import torch

import kale.predict.isonet as isonet

BATCH_SIZE = 2
INPUT_BATCH = torch.randn(BATCH_SIZE, 3, 224, 224)
TRANS_FUNS = ["basic_transform"]  # "bottleneck_transform"]


@pytest.fixture(scope="module")
def testing_cfg(download_path):
    config_params = {
        "net_params": {
            "use_dirac": True,
            "use_dropout": True,
            "dropout_rate": 0.0,
            "nc": 10,
            "depths": 34,
            "has_bn": True,
            "use_srelu": True,
            "transfun": "basic_transform",
            "has_st": True,
        }
    }
    yield config_params


@pytest.mark.parametrize("transfun", TRANS_FUNS)
def test_isonet(transfun, testing_cfg):
    net_params = testing_cfg["net_params"]
    net_params["transfun"] = transfun
    net = isonet.ISONet(net_params)
    net.eval()
    output_batch = net(INPUT_BATCH)
    assert output_batch.size() == (BATCH_SIZE, net_params["nc"])
</file>

<file path="tests/predict/test_uncertainty_binning.py">
import logging

import pytest

from kale.predict.uncertainty_binning import quantile_binning_predictions
from kale.utils.seed import set_seed

seed = 36
set_seed(seed)

LOGGER = logging.getLogger(__name__)

DUMMY_TABULAR_DATA = {
    "uid": ["PHD_2154", "PHD_2158", "PHD_217", "PHD_2194"],
    "E-CPV Error": [1.4142135, 3.1622777, 5.0990195, 61.846584],
    "E-CPV Uncertainty": [4.25442667, 4.449976897, 1.912124681, 35.76085777],
    "E-MHA Error": [3.1622777, 3.1622777, 4, 77.00649],
    "E-MHA Uncertainty": [0.331125357, 0.351173535, 1.4142135, 0.142362904],
    "S-MHA Error": [3.1622777, 1.4142135, 5.0990195, 56.32051],
    "S-MHA Uncertainty": [0.500086973, 0.235296882, 1.466040241, 0.123874651],
    "Validation Fold": [1, 1, 1, 1],
    "Testing Fold": [0, 0, 0, 0],
}


class TestQuantileBinningPredictions:
    @pytest.mark.parametrize(
        "uncertainty_thresh_list, expected",
        [
            ([[1], [3], [5], [9]], {"PHD_2154": 2, "PHD_2158": 2, "PHD_217": 1, "PHD_2194": 4}),
            ([[1], [1.5], [1.6], [2]], {"PHD_2154": 4, "PHD_2158": 4, "PHD_217": 3, "PHD_2194": 4}),
            ([[40], [42], [43], [44]], {"PHD_2154": 0, "PHD_2158": 0, "PHD_217": 0, "PHD_2194": 0}),
        ],
    )
    def test_quantile_binning_predictions_thresh(self, uncertainty_thresh_list, expected):
        test_dict = dict(zip(DUMMY_TABULAR_DATA["uid"], DUMMY_TABULAR_DATA["E-CPV Uncertainty"]))
        assert quantile_binning_predictions(test_dict, uncertainty_thresh_list) == expected

    # test wrong dict format
    def test_dict_format(self):
        with pytest.raises(ValueError, match="uncertainties_test must be of type dict"):
            quantile_binning_predictions([1, 2, 3], [[1], [1.5], [1.6], [2]])
        with pytest.raises(ValueError, match=r"Dict uncertainties_test should be of structure .*"):
            quantile_binning_predictions(
                {"PHD_2154": "2", "PHD_2158": 2, "PHD_217": 1, "PHD_2194": 4}, [[1], [1.5], [1.6], [2]]
            )
        with pytest.raises(ValueError, match=r"uncert_thresh list should be 2D .*"):
            quantile_binning_predictions({"PHD_2154": 2, "PHD_2158": 2, "PHD_217": 1, "PHD_2194": 4}, [1, 1.5, 1.6, 2])
</file>

<file path="tests/prepdata/test_chem_transform.py">
from kale.prepdata.chem_transform import integer_label_protein, integer_label_smiles


def test_chem_wrong_smiles():
    wrong_smiles = "NS(=O)(=O)*c1cc2C"
    assert len(integer_label_smiles(wrong_smiles))


def test_chem_illegal_character():
    smiles = "O=C(c1ccc(F)cc1)C1CCN(CCC2Cc3cc(F)ccc3C2=O)CC1*"
    target = "MEILCEDNISLSSIPNSLM*QLGDGPRL"
    drug_encoding = integer_label_smiles(smiles)
    target_encoding = integer_label_protein(target)
    assert drug_encoding.size == 85
    assert target_encoding.size == 1200
</file>

<file path="tests/prepdata/test_image_transform.py">
import os

import matplotlib.figure
import numpy as np
import pytest
from numpy import testing

from kale.interpret.visualize import plot_multi_images
from kale.loaddata.image_access import check_dicom_series_uid, dicom2arraylist, read_dicom_dir
from kale.prepdata.image_transform import mask_img_stack, normalize_img_stack, reg_img_stack, rescale_img_stack

SCALES = [4, 8]


@pytest.fixture(scope="module")
def images(download_path):
    img_path = os.path.join(download_path, "SA_64x64_v2.0", "DICOM")
    cmr_dcm_list = read_dicom_dir(img_path, sort_instance=True, sort_patient=True, check_series_uid=True)
    dcms = []
    for i in range(5):
        for j in range(len(cmr_dcm_list[i])):
            dcms.append(cmr_dcm_list[i][j])
    dcm5_list = check_dicom_series_uid(dcms)
    cmr_images = dicom2arraylist(dicom_patient_list=dcm5_list, return_patient_id=False)

    return cmr_images


@pytest.fixture(scope="module")
def coords():
    landmarks = np.asarray(
        [
            [32.0, 39.75, 29.25, 23.75, 19.0, 41.0],
            [24.5, 40.0, 28.5, 23.75, 11.0, 37.25],
            [26.25, 40.5, 27.75, 24.25, 12.25, 40.75],
            [34.25, 38.0, 34.25, 21.25, 23.0, 41.0],
            [33.0, 40.25, 31.5, 24.25, 19.5, 40.5],
        ]
    )
    return landmarks


def test_reg(images, coords):
    marker_kwargs = {"marker": "+", "color": (1, 1, 1, 0.1), "s": 50}
    im_kwargs = {"cmap": "gray"}
    title_kwargs = {"fontsize": 20}
    marker_names = ["inf insertion point", "sup insertion point", "RV inf"]

    n_samples = len(images)
    fig = plot_multi_images(
        [images[i][0, ...] for i in range(n_samples)],
        n_cols=5,
        marker_locs=coords,
        marker_titles=marker_names,
        marker_cmap="Set1",
        im_kwargs=im_kwargs,
        marker_kwargs=marker_kwargs,
        title_kwargs=title_kwargs,
    )
    assert isinstance(fig, matplotlib.figure.Figure)
    with pytest.raises(Exception):
        reg_img_stack(images, coords[1:, :], coords[0])
    images_reg, max_dist = reg_img_stack(images, coords, target_coords=coords[0])
    # images after registration should be close to original images, because values of noise are small
    for i in range(n_samples):
        testing.assert_allclose(images_reg[i], images[i])
    # add one for avoiding inf relative difference
    testing.assert_allclose(max_dist + 1, np.ones(n_samples), rtol=2, atol=2)
    fig = plot_multi_images([images_reg[i][0, ...] for i in range(n_samples)], n_cols=5)
    assert isinstance(fig, matplotlib.figure.Figure)


@pytest.mark.parametrize("scale", SCALES)
def test_rescale(scale, images):
    img_rescaled = rescale_img_stack(images, 1 / scale)
    n_samples = len(img_rescaled)
    testing.assert_equal(n_samples, len(images))
    for i in range(n_samples):
        # dim1 and dim2 have been rescaled
        testing.assert_equal(img_rescaled[i].shape[-1], round(images[i].shape[-1] / scale))
        testing.assert_equal(img_rescaled[i].shape[-2], round(images[i].shape[-2] / scale))
        # n_phases are unchanged
        testing.assert_equal(img_rescaled[i].shape[0], images[i].shape[0])


def test_masking(images):
    # generate synthetic mask randomly
    mask = np.random.randint(0, 2, size=(images[0].shape[-2], images[0].shape[-1]))
    idx_zeros = np.where(mask == 0)
    idx_ones = np.where(mask == 1)
    img_masked = mask_img_stack(images, mask)
    n_samples = len(images)
    for i in range(n_samples):
        n_phases = images[i].shape[0]
        for j in range(n_phases):
            img = img_masked[i][j, ...]
            testing.assert_equal(np.sum(img[idx_zeros]), 0)
            img_orig = images[i][j, ...]
            testing.assert_equal(img[idx_ones], img_orig[idx_ones])


def test_normalize(images):
    norm_image = normalize_img_stack(images)
    for i in range(len(norm_image)):
        assert np.min(norm_image[i]) >= 0
        assert np.max(norm_image[i]) <= 1
</file>

<file path="tests/prepdata/test_tabular_transform.py">
import logging

import numpy as np
import pandas as pd
import pytest
import torch

from kale.prepdata.tabular_transform import (
    apply_confidence_inversion,
    generate_struct_for_qbin,
    ToOneHotEncoding,
    ToTensor,
)
from kale.utils.seed import set_seed


class TestToTensor:
    def test_to_tensor_output(self):
        data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])
        to_tensor = ToTensor()
        output = to_tensor(data)
        assert isinstance(output, torch.Tensor)

    def test_to_tensor_dtype(self):
        data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])
        to_tensor = ToTensor(dtype=torch.float32)
        output = to_tensor(data)
        assert output.dtype == torch.float32

    def test_to_tensor_device(self):
        data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])
        to_tensor = ToTensor(device=torch.device("cpu"))
        output = to_tensor(data)
        assert output.device == torch.device("cpu")


class TestToOneHotEncoding:
    @pytest.mark.parametrize("num_classes", [3, -1])
    def test_onehot_encoding_output(self, num_classes: int):
        labels = [1, 0, 2]
        to_onehot = ToOneHotEncoding(num_classes=num_classes)
        output = to_onehot(labels)
        assert output.tolist() == [[0, 1, 0], [1, 0, 0], [0, 0, 1]]

    @pytest.mark.parametrize("num_classes, shape", [(3, (3, 3)), (4, (3, 4)), (5, (3, 5))])
    def test_onehot_encoding_shape(self, num_classes: int, shape: tuple):
        labels = [1, 0, 2]
        to_onehot = ToOneHotEncoding(num_classes=num_classes)
        output = to_onehot(labels)
        assert output.shape == shape

    def test_onehot_encoding_dtype(self):
        data = [1, 0, 2]
        to_onehot = ToOneHotEncoding(dtype=torch.float32)
        output = to_onehot(data)
        assert output.dtype == torch.float32

    def test_onehot_encoding_device(self):
        data = [1, 0, 2]
        to_onehot = ToOneHotEncoding(device=torch.device("cpu"))
        output = to_onehot(data)
        assert output.device == torch.device("cpu")


seed = 36
set_seed(seed)

LOGGER = logging.getLogger(__name__)

EXPECTED_COLS = [
    "uid",
    "E-CPV Error",
    "E-CPV Uncertainty",
    "E-CPV Uncertainty bins",
    "E-MHA Error",
    "E-MHA Uncertainty",
    "E-MHA Uncertainty bins",
    "S-MHA Error",
    "S-MHA Uncertainty",
    "S-MHA Uncertainty bins",
    "Validation Fold",
    "Testing Fold",
    "target_idx",
]

DUMMY_DICT = pd.DataFrame({"data": [0.1, 0.2, 0.9, 1.5]})


@pytest.mark.parametrize("input, expected", [(DUMMY_DICT, [1 / 0.1, 1 / 0.2, 1 / 0.9, 1 / 1.5])])
def test_apply_confidence_inversion(input, expected):
    # test that it inverts correctly
    assert list(apply_confidence_inversion(input, "data")["data"]) == pytest.approx(expected)

    # test that a KeyError is raised successfully if key not in dict.
    with pytest.raises(KeyError, match=r".* key .*"):
        apply_confidence_inversion({}, "data") == pytest.approx(expected)


# Test that we can read csvs in the correct structure and return a dict of pandas dataframes in correct structure.
def test_get_data_struct(landmark_uncertainty_tuples_path):
    bins_all_targets, bins_targets_sep, bounds_all_targets, bounds_targets_sep = generate_struct_for_qbin(
        ["U-NET"], [0, 1], landmark_uncertainty_tuples_path[2], "SA"
    )

    assert isinstance(bins_all_targets, dict)
    assert isinstance(bins_all_targets["U-NET"], pd.DataFrame)
    assert sorted(list(bins_all_targets["U-NET"].keys())) == sorted(EXPECTED_COLS)
    assert isinstance(bins_targets_sep, dict)
    assert list(bins_targets_sep.keys()) == ["U-NET L0", "U-NET L1"]
    assert sorted(list(bins_targets_sep["U-NET L0"].keys())) == sorted(EXPECTED_COLS)

    assert isinstance(bounds_all_targets, dict)
    assert isinstance(bounds_targets_sep, dict)
</file>

<file path="tests/utils/test_download.py">
import os
from pathlib import Path

import pytest

from kale.utils.download import download_file_by_url, download_file_gdrive

output_directory = Path().absolute().joinpath("tests/test_data/download")
PARAM = [
    "https://github.com/pykale/data/raw/main/videos/video_test_data/ADL/annotations/labels_train_test/adl_P_11_train.pkl;a.pkl;pkl",
    "https://github.com/pykale/data/raw/main/videos/video_test_data.zip;video_test_data.zip;zip",
]

GDRIVE_PARAM = [
    "1U4D23R8u8MJX9KVKb92bZZX-tbpKWtga;demo_datasets.zip;zip",
    "1SV7fmAnWj-6AU9X5BGOrvGMoh2Gu9Nih;dummy_data.csv;csv",
]


@pytest.mark.parametrize("param", PARAM)
def test_download_file_by_url(param):
    url, output_file_name, file_format = param.split(";")

    # run twice to test the code when the file exist
    download_file_by_url(url, output_directory, output_file_name, file_format)
    download_file_by_url(url, output_directory, output_file_name, file_format)

    assert os.path.exists(output_directory.joinpath(output_file_name)) is True


@pytest.mark.parametrize("param", GDRIVE_PARAM)
def test_download_file_gdrive(param):
    id, output_file_name, file_format = param.split(";")

    # run twice to test the code when the file exist
    download_file_gdrive(id, output_directory, output_file_name, file_format)
    download_file_gdrive(id, output_directory, output_file_name, file_format)

    assert os.path.exists(output_directory.joinpath(output_file_name)) is True
</file>

<file path="tests/utils/test_initialize_nn.py">
import pytest
import torch
import torch.nn as nn

from kale.utils.initialize_nn import bias_init, xavier_init
from kale.utils.seed import set_seed


def test_xavier_init():
    set_seed(2022)
    module = nn.Linear(10, 20)
    xavier_init(module)
    assert torch.var(module.weight).item() == pytest.approx(2.0 / (module.in_features + module.out_features), rel=1e-1)


def test_bias_init():
    module = nn.Linear(10, 20)
    bias_init(module)
    assert torch.mean(module.bias).item() == pytest.approx(0.0, rel=1e-2)
    assert torch.var(module.bias).item() == pytest.approx(0.0, rel=1e-2)
</file>

<file path="tests/utils/test_logger.py">
import logging
import os

import pytest

from kale.utils import logger


@pytest.fixture(scope="module")
def testing_logger():
    save_dir = os.getcwd()
    testing_logger = logger.construct_logger("test", save_dir)
    yield testing_logger

    # Gather info
    filehandler = testing_logger.handlers[0]
    log_file_name = filehandler.baseFilename

    # Teardown log file
    filehandler.close()
    os.remove(log_file_name)

    # Teardown gitdiff.patch file
    [folder, file] = os.path.split(log_file_name)
    file_core = os.path.splitext(file)[0]
    gitdiff_file_name = os.path.join(folder, file_core + ".gitdiff.patch")
    os.remove(gitdiff_file_name)


def test_construct_logger_terminal(caplog):
    """Test that logger outputs to terminal when log_to_terminal is True."""
    logger_name = "test_logger"
    save_dir = "./tests/outputs"
    os.makedirs(save_dir, exist_ok=True)

    t_logger = logger.construct_logger(logger_name, save_dir, log_to_terminal=True)
    t_logger.debug("This is a debug message")
    t_logger.info("This is an info message")

    # Assert that messages were logged to the terminal
    assert "This is a debug message" in caplog.text
    assert "This is an info message" in caplog.text


@pytest.fixture
def log_file_name(testing_logger):
    filehandler = testing_logger.handlers[0]
    yield filehandler.baseFilename


@pytest.fixture
def gitdiff_file_name(log_file_name):
    [folder, file] = os.path.split(log_file_name)
    file_core = os.path.splitext(file)[0]
    yield os.path.join(folder, file_core + ".gitdiff.patch")


def test_out_file_core():
    out_file_eg = logger.out_file_core()
    assert isinstance(out_file_eg, str)


def test_logger_type(testing_logger):
    assert isinstance(testing_logger, logging.Logger)


def test_log_file_exists(log_file_name):
    assert os.path.isfile(log_file_name)


def test_gitdiff_file_exists(gitdiff_file_name):
    assert os.path.isfile(gitdiff_file_name)
</file>

<file path="tests/utils/test_print.py">
"""
Ref1: https://stackoverflow.com/questions/33767627/python-write-unittest-for-console-print
Ref2: https://docs.pytest.org/en/latest/capture.html#accessing-captured-output-from-a-test-function
"""

from kale.utils.print import pprint, pprint_without_newline, tprint


def test_tprint(capsys):
    tprint("hello")
    captured = capsys.readouterr()
    assert captured.out == "\rhello"


def test_pprint(capsys):
    pprint("hello")  # noqa: T203
    captured = capsys.readouterr()
    assert captured.out == "\rhello\n"


def test_pprint_without_newline(capsys):
    pprint_without_newline("hello")
    captured = capsys.readouterr()
    assert captured.out == "\rhello "
</file>

<file path="tests/utils/test_seed.py">
import random

import numpy as np
import pytest
import torch
from numpy import testing

from kale.utils.seed import set_seed


@pytest.fixture
def base_rand():
    return 0.7773566427005639


def test_set_seed_base(base_rand):
    set_seed()
    result = random.random()
    testing.assert_equal(result, base_rand)


@pytest.fixture
def np_rand():
    return 0.6535895854646095


def test_set_seed_numpy(np_rand):
    set_seed()
    result = np.random.rand()
    testing.assert_equal(result, np_rand)


@pytest.fixture
def torch_rand():
    return 0.3189346194267273


def test_set_seed_torch(torch_rand):
    set_seed()
    result = torch.rand(1).item()
    testing.assert_equal(result, torch_rand)
</file>

<file path="tests/conftest.py">
# Global settings for tests. Run before any test
import os

import pytest
from scipy.io import loadmat


@pytest.fixture(scope="session")
def download_path():
    path = os.path.join("tests", "test_data")
    os.makedirs(path, exist_ok=True)
    return path


@pytest.fixture(scope="session")
def gait(download_path):
    return loadmat(os.path.join(download_path, "gait.mat"))


@pytest.fixture(scope="session")
def office_path(download_path):
    path_ = os.path.join(download_path, "office")
    os.makedirs(path_, exist_ok=True)
    return path_


# Downloads and unzips remote file
@pytest.fixture(scope="session")
def landmark_uncertainty_tuples_path(download_path):
    path_ = download_path
    os.makedirs(path_, exist_ok=True)

    valid_path = os.path.join(path_, "Uncertainty_tuples/U-NET/SA/uncertainty_pairs_valid_t0")
    test_path = os.path.join(path_, "Uncertainty_tuples/U-NET/SA/uncertainty_pairs_test_t0")

    dl_path = os.path.join(path_, "Uncertainty_tuples")
    return valid_path, test_path, dl_path
</file>

<file path="tests/download_test_data.py">
from pathlib import Path

from tdc.multi_pred import DTI
from torchvision.datasets import CIFAR10, CIFAR100, MNIST, SVHN

from kale.loaddata.mnistm import MNISTM
from kale.loaddata.usps import USPS
from kale.prepdata.image_transform import get_transform
from kale.utils.download import download_file_by_url


def download_path():
    path = Path("tests").joinpath("test_data")
    path.mkdir(parents=True, exist_ok=True)
    return str(path)


def download_gait_gallery_data(save_path):
    # Downloading gait gallery data for tests/conftest.py test
    url = "https://github.com/pykale/data/raw/main/videos/gait/gait_gallery_data.mat"
    download_file_by_url(url, save_path, "gait.mat", "mat")


def download_landmark_global_data(save_path):
    # Downloading landmark global fixtures data for tests/conftest.py test
    url = "https://github.com/pykale/data/raw/main/tabular/cardiac_landmark_uncertainty/Uncertainty_tuples.zip"
    download_file_by_url(url, save_path, "Uncertainty_tuples.zip", "zip")


def download_mpca_data(save_path):
    # Downloading MPCA data for tests/embed/test_factorization.py test
    url = "https://github.com/pykale/data/raw/main/videos/gait/mpca_baseline.mat"
    download_file_by_url(url, save_path, "baseline.mat", "mat")


def download_video_data(save_path):
    # Downloading video data for tests/loaddata/test_video_access.py and tests/pipeline/test_video_domain_adapter.py tests
    url = "https://github.com/pykale/data/raw/main/videos/video_test_data.zip"
    download_file_by_url(url, save_path, "video_test_data.zip", "zip")


def download_multiomics_data(save_path):
    # Downloading Binary Class and Multi Class Multiomics datasets for tests/pipeline/test_multiomics_trainer.py test
    url = "https://github.com/pykale/data/raw/main/multiomics/ROSMAP.zip"
    binary_save_path = Path(save_path).joinpath("multiomics/trainer/binary_class/")
    download_file_by_url(url, binary_save_path, "binary_class.zip", "zip")

    url = "https://github.com/pykale/data/raw/main/multiomics/TCGA_BRCA.zip"
    multi_save_path = Path(save_path).joinpath("multiomics/trainer/multi_class/")
    download_file_by_url(url, multi_save_path, "multi_class.zip", "zip")


def download_cmr_data(save_path):
    # Downloading CMR dataset for tests/prepdata/test_image_transform.py test
    url = "https://github.com/pykale/data/raw/main/images/ShefPAH-179/SA_64x64_v2.0.zip"
    download_file_by_url(url, save_path, "SA_64x64.zip", "zip")


def download_mnist_data(save_path):
    # Downloading MNIST datasets for tests/loaddata/test_image_access.py test
    MNIST(save_path, train=True, transform=get_transform("mnistm"), download=True)
    MNISTM(save_path, train=True, transform=get_transform("mnistm"), download=True)


def download_usps_data(save_path):
    # Downloading USPS dataset for tests/loaddata/test_image_access.py test
    USPS(save_path, train=True, transform=get_transform("mnistm"), download=True)


def download_svhn_data(save_path):
    # Downloading SVHN dataset for tests/loaddata/test_image_access.py test
    SVHN(save_path, split="train", transform=get_transform("mnistm"), download=True)
    SVHN(save_path, split="test", transform=get_transform("mnistm"), download=True)


def download_cifar_data(save_path):
    # Downloading CIFAR10 and CIFAR100 datasets for tests/loaddata/test_image_access.py test
    CIFAR10(save_path, train=True, download=True, transform=get_transform("cifar", augment=True))
    CIFAR100(save_path, train=True, download=True, transform=get_transform("cifar", augment=True))


def download_office_data(save_path):
    # Downloading Office-31 dataset for tests/loaddata/test_image_access.py test
    office_save_path = Path(save_path).joinpath("office")
    url = "https://github.com/pykale/data/raw/main/images/office"
    for domain in ["amazon", "caltech", "dslr", "webcam"]:
        filename = "%s.zip" % domain
        data_url = "%s/%s" % (url, filename)
        download_file_by_url(data_url, office_save_path, filename, "zip")


def download_dti_data(save_path):
    # Downloading Drug-Target Interaction (DTI) datasets for tests/loaddata/test_tdc_datasets.py test
    for source_name in ["BindingDB_Kd", "BindingDB_Ki"]:
        _ = DTI(name=source_name, path=save_path)


def download_omniglot_data(save_path):
    # Downloading Omniglot Dataset for tests/loaddata/test_few_shot.py test
    url = "https://github.com/pykale/data/raw/main/images/omniglot/omniglot_demo.zip"
    download_file_by_url(url, save_path, "omniglot_demo.zip", "zip")


if __name__ == "__main__":
    path_to_test_data = download_path()

    download_gait_gallery_data(path_to_test_data)
    download_landmark_global_data(path_to_test_data)
    download_mpca_data(path_to_test_data)
    download_video_data(path_to_test_data)
    download_multiomics_data(path_to_test_data)
    download_cmr_data(path_to_test_data)
    download_mnist_data(path_to_test_data)
    download_usps_data(path_to_test_data)
    download_svhn_data(path_to_test_data)
    download_cifar_data(path_to_test_data)
    download_office_data(path_to_test_data)
    download_dti_data(path_to_test_data)
    download_omniglot_data(path_to_test_data)
</file>

<file path="tests/README.md">
# Test Guidelines

All code should be covered by [unit tests](https://carpentries-incubator.github.io/python-testing/04-units/index.html) with at least 70% coverage, and [regression tests](https://carpentries-incubator.github.io/python-testing/07-integration/index.html) where appropriate. When a bug is located and fixed, new test(s) should be added that would catch this bug. Please use [pykale discussions on testing](https://github.com/pykale/pykale/discussions/categories/testing) to talk about tests and ask for help. The overall and specific coverage of your commits can be checked conveniently by browsing the respective branch at the [codecov report](https://app.codecov.io/gh/pykale/pykale/commits), e.g., [check commits under the rewrite-gripnet branch](https://app.codecov.io/gh/pykale/pykale/commits?branch=rewrite-gripnet) by clicking the specific commits.

These guidelines will help you to write tests to address sufficiently compact pieces of code such that it is easy to identify causes of failure and for tests to also cover larger workflows such that confidence or trust can be built in reproducibility of outputs. We use [pytest](https://docs.pytest.org/en/stable/) (see tutorials [python testing software carpentry (alpha)](https://carpentries-incubator.github.io/python-testing/) and [tutorialspoint pytest tutorial](https://www.tutorialspoint.com/pytest/pytest_tutorial.pdf)). There is some subjectivity involved in deciding how much of the potential behaviour of your code to check.

## Quick start

- [Compact pytest tutorial](https://www.tutorialspoint.com/pytest/pytest_tutorial.pdf), [pytest fixtures](https://docs.pytest.org/en/stable/fixture.html), [pytest exceptions](https://docs.pytest.org/en/stable/assert.html#assertions-about-expected-exceptions)
- Example [unit tests for deep learning code of variational autoencoder in PyTorch](https://github.com/tilman151/unittest_dl) and the related post [How to Trust Your Deep Learning Code](https://krokotsch.eu/cleancode/2020/08/11/Unit-Tests-for-Deep-Learning.html). *(It uses `unittest` but we use `pytest`. To convert a `unittest` to a `pytest`, [unittest2pytest](https://github.com/pytest-dev/unittest2pytest) is a good starting point.)*
- Learn from [existing pykale tests](https://github.com/pykale/pykale/tree/main/tests), [pytorch tests](https://github.com/pytorch/pytorch/tree/master/test), [torchvision tests](https://github.com/pytorch/vision/tree/master/test), and pytest+pytorch examples [fastai1 tests](https://github.com/fastai/fastai1/tree/master/tests) and [Kornia tests](https://github.com/kornia/kornia/tree/master/test)
- Use GitHub code links to find out definitions and references
- Use [Python Test Explorer for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=LittleFoxTeam.vscode-python-test-adapter) or [pytest in pycharm](https://www.jetbrains.com/help/pycharm/pytest.html) to run tests conveniently.
- [fastai testing](https://fastai1.fast.ai/dev/test.html) is a good high-level reference. We adapt its recommendations on [writing tests](https://fastai1.fast.ai/dev/test.html#writing-tests) below:
  - Think about how to create a test of the real functionality that runs quickly, e.g. based on our [`examples`](https://github.com/pykale/pykale/tree/main/examples).
  - Use module scope fixtures to run initial code that can be shared amongst tests. When using fixtures, make sure the test doesn‚Äôt modify the global object it received. If there's a risk of modifying a broadly scoped fixture, you could clone it with a more tightly scoped fixture or create a fresh fixture/object instead.
  - Avoid pretrained models, since they have to be downloaded from the internet to run the test.
  - Create some minimal data for your test, or use data already in repo‚Äôs data/ directory.

See more details below, particularly [test data](#test-data), [common parameters](#common-parameters), and [running tests locally](#running-tests-locally).

## Test workflow

The test workflow is defined in [`pykale/.github/workflows/test.yml`](https://github.com/pykale/pykale/blob/main/.github/workflows/test.yml). It is triggered on every pull request and every push to the main branch, and we also run it at midnight (UK time) each day.

To minimize additional downloading time, test data and pip packages, once downloaded, are stored in the [GitHub Actions Caches](https://github.com/pykale/pykale/actions/caches) for future reuse. Test data caching uses strict matching of the cache key, meaning any change in `tests/download_test_data.py` will update the cache. For efficiency, pip package caching uses soft matching, resulting in cache updates driven by both the date tag and any modifications in `**/setup.py`.

## Test data

Data needed for testing should be uploaded to [pykale/data](https://github.com/pykale/data) (preferred) or other external sources. All data downloading processes should be included in [`tests/download_test_data.py`](https://github.com/pykale/pykale/blob/refine_test_readme/tests/download_test_data.py). It is recommended to use `download_file_by_url` from `kale.utils.download` for downloading data during tests to `tests/test_data`, as defined `download_path` of [`tests/conftest.py`](https://github.com/pykale/pykale/blob/main/tests/conftest.py). More complex test data requirements for your **pull request** can be discussed in the motivating **issue** or [pykale discussions on testing](https://github.com/pykale/pykale/discussions/categories/testing).

## Common parameters

Consider adding parameters (or objects etc.) that may be useful to multiple tests as fixtures in a [`conftest.py`](
https://docs.pytest.org/en/stable/fixture.html#conftest-py-sharing-fixtures-across-multiple-files) file, either in `tests/` or the appropriate sub-module.

## Running tests locally

To run tests locally you will need to have installed `pykale` with the development requirements:

```sh
git clone https://github.com/pykale/pykale
cd pykale
pip install -e .[dev]
```

then run:

```sh
pytest
```

## Unit tests

A **unit test** checks that a small "unit" of software (e.g. a function) performs correctly. It might, for example, check that the function `add` returns the number `2` when a list `[1, 1]` is the input.

Within the `tests/` folder is a folder structure that mimics that of the `kale` python module. Unit tests for code in a given file in `kale/` should be placed in their equivalent file in `tests/` e.g. unit tests for a function in `pykale/kale/loaddata/image_access.py` should be located in `pykale/tests/loaddata/test_image_access.py`.

Philosophically, the author of a "unit" of code knows exactly what it should do and can write the test criteria accordingly.

## Regression tests

A **regression test** checks that software produces the same results after a change is made. In `pykale`, we expect regression tests to achieve this by testing several different parts of the software at once (in effect, an [integration test](https://carpentries-incubator.github.io/python-testing/07-integration/index.html)). A single regression test might test *loading some input files*, *setting up a model* and *generating a plot* based on the model. This could be achieved by running the software with previously stored baseline inputs and checking the output is the same as previously stored baseline outputs.

Regression tests should be placed in `tests/regression`. Further subfolders can be added, as required. We plan to add regression tests covering existing functionality based on examples in the `examples/` folder.

Philosophically, regression tests treat the "past as truth" - the correct output / behaviour is the way it worked before a change.

## Testing DataFrames and arrays

Comparisons / assertions involving `pandas` `DataFrames` (or other `pandas` objects) should be made using `pandas` utility functions: [`pandas.testing.assert_frame_equal`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.testing.assert_frame_equal.html), [`pandas.testing.assert_series_equal`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.testing.assert_series_equal.html), [`pandas.testing.assert_index_equal`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.testing.assert_index_equal.html), [`pandas.testing.assert_extension_array_equal`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.testing.assert_extension_array_equal.html).

Comparisons / assertions involving `numpy` `arrays` (or other `numpy` objects) should be made using [`numpy` testing routines](https://numpy.org/doc/stable/reference/routines.testing.html). `numpy` floating point "problem" response will be [as default](https://numpy.org/doc/stable/reference/generated/numpy.seterr.html#numpy.seterr).

## Random Numbers

Random numbers in pykale are generated using base python, numpy and pytorch. Prior to making an assertion where objects that make use of random numbers are compared, the `set_seed()` function from `kale.utils.seed` should be called e.g.

In `__init__.py` or `test_<modulename>.py`:

```python
from kale.utils.seed import set_seed
```

In test, before assertion:

```python
set_seed()
```

## Logging and handling of warnings

`pytest` [captures log messages of level WARNING or above](https://docs.pytest.org/en/stable/logging.html) and outputs them to the terminal.

## Floating point errors

`numpy` can be configured to [respond differently to floating point errors](
https://numpy.org/doc/stable/reference/generated/numpy.seterr.html#numpy.seterr). `pykale` normally uses the default configuration.

## Side effects

Be aware that the code for which you are adding a test may have [side effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science)) (e.g. a function changing something in a file or database, as well as returning a variable). e.g.

```python
import math

def add(numbers):
  math.pi += numbers[1] # Add first number in list to math.pi
  return sum(numbers) # Add list of numbers together

print("Sum:", add([1, 1])) # Show numbers are added
print("pi:", math.pi) # Show side effect
```

will output:

```python
Sum: 2
pi: 4.141592653589793
```

...having redefined the value of `math.pi`! `math.pi` will be redefined each time the function is run and nothing returned by the function gives any indication this has happened.

Minimising side effects makes code easier to test. Try and minimise side effects and ensure, where present, they are covered by tests.
</file>

<file path=".gitignore">
*/**/__pycache__
*/__pycache__
*.dcm
*.pt
*.pth
*.zip
*.ckpt
*.json

# Distribution / packaging
build/
dist/
*.egg-info/


# Pycharm
.idea

# vscode
.vscode

# Data and outputs
examples/data/
examples/*/data/
examples/*/outputs/
examples/*/lightning_logs/
examples/*/tb_logs/
examples/*/logs/
examples/*/demo*/
examples/*/configs_experiment/

# Logs
log-*.txt
*gitdiff.patch

# Sphinx documentation
docs/build/
docs/source/backup

# Testing
.coverage
cov*.xml
tests/test_data
tests/*/tests


# Lightning
lightning_logs
tb_logs/*
outputs/*

# Jupyter Notebook Checkpoints
*-checkpoint.ipynb

# Examples
# cmri_mpca - the saved images
examples/cmri_mpca/outputs
</file>

<file path=".pre-commit-config.yaml">
repos:
-   repo: https://github.com/datarootsio/databooks
    rev: 1.3.7
    hooks:
    -   id: databooks-meta
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v3.4.0
    hooks:
    -   id: check-added-large-files
        args: ['--maxkb=300']
    -   id: check-byte-order-marker
    -   id: check-case-conflict
    -   id: check-merge-conflict
    -   id: end-of-file-fixer
    -   id: forbid-new-submodules
    -   id: mixed-line-ending
    -   id: trailing-whitespace
    -   id: debug-statements
    -   id: check-yaml
        exclude: .conda/meta.yaml
    -   id: requirements-txt-fixer
-   repo: https://github.com/pycqa/flake8.git
    rev: 6.1.0
    hooks:
    -   id: flake8
        additional_dependencies: [flake8-print, "importlib-metadata<5.0.0"]
        args: [--config=setup.cfg]
        exclude: ^examples/
-   repo: https://github.com/psf/black
    rev: 23.11.0
    hooks:
    -   id: black
        language_version: python3
        additional_dependencies: ['click==8.0.4']
-   repo: https://github.com/pycqa/isort
    rev: 5.11.2
    hooks:
      - id: isort
        name: isort
        entry: python -m isort
        args: [--settings-path, ./pyproject.toml]
        language: system
        types: [python]
-   repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.5.0
    hooks:
    -   id: mypy
</file>

<file path=".readthedocs.yaml">
# Read the Docs configuration file for Sphinx projects
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required. Represents Configuration file v2.
version: 2

# Set the OS, Python version and other tools.
build:
  os: ubuntu-22.04
  tools:
    python: "3.10"

# Build documentation in the "docs/" directory with Sphinx. Not used in this project. Keep this for use in the future.
#sphinx:
#  configuration: docs/conf.py

# Build documentation via the Python requirements file. Used in this project.
# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html
python:
  install:
  - requirements: docs/requirements.txt
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2020 pykale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pyproject.toml">
[build-system]
requires = [
    "setuptools==59.5.0",
    "wheel",
]

[tool.black]
# https://github.com/psf/black
line-length = 120
target-version = ["py38"]

[tool.isort]
known_first_party = [
    "docs",
    "examples",
    "kale",
    "tests",
]
known_third_party = [
    "matplotlib",
    "numpy",
    "setuptools==59.5.0",
    "sphinx_rtd_theme",
    "torch",
]
profile = "black"
line_length = 120
force_sort_within_sections = "False"
order_by_type = "False"

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"
</file>

<file path="README.md">
<p align="center">
  <img src="https://github.com/pykale/pykale/raw/main/docs/images/pykale_logo_long.png" width="40%" alt='project-pykale'>
</p>

> *Very cool library with lots of great ideas on moving toward 'green', efficient multimodal machine learning and AI*.

[Kevin Carlberg](https://kevintcarlberg.net/), AI Research Science Manager at Facebook Reality Labs (quoted from [tweet](https://twitter.com/kcarlberg/status/1387511298259177474)).

-----------------------------------------

<!-- Keep badges to just ONE line, i.e. only the most important badges! -->
[![tests](https://github.com/pykale/pykale/workflows/test/badge.svg)](https://github.com/pykale/pykale/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/pykale/pykale/branch/main/graph/badge.svg?token=jmIYPbA2le)](https://codecov.io/gh/pykale/pykale)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
[![Python](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue)](https://www.python.org)
[![PyPI version](https://img.shields.io/pypi/v/pykale?color=blue)](https://pypi.org/project/pykale/)
[![PyPI downloads](https://pepy.tech/badge/pykale)](https://pepy.tech/project/pykale)
<!-- [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5557244.svg)](https://doi.org/10.5281/zenodo.5557244) -->

[Getting Started](https://github.com/pykale/pykale#how-to-use) |
[Documentation](https://pykale.readthedocs.io/) |
[Contributing](https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md) |
[Discussions](https://github.com/pykale/pykale/discussions) |
[Changelog](https://github.com/pykale/pykale/tree/main/.github/CHANGELOG.md)

PyKale is a library in the [PyTorch ecosystem](https://pytorch.org/ecosystem/) aiming to make machine learning more accessible to interdisciplinary research by bridging gaps between data, software, and end users. Both machine learning experts and end users can do better research with our accessible, scalable, and sustainable design, guided by green machine learning principles. PyKale has a unified *pipeline-based* API and focuses on [multimodal learning](https://en.wikipedia.org/wiki/Multimodal_learning) and [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) for graphs, images, and videos at the moment, with supporting models on [deep learning](https://en.wikipedia.org/wiki/Deep_learning) and [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction).

PyKale enforces *standardization* and *minimalism*, via green machine learning concepts of *reducing* repetitions and redundancy, *reusing* existing resources, and *recycling* learning models across areas. PyKale will enable and accelerate *interdisciplinary*, *knowledge-aware* machine learning research for graphs, images, and videos in applications including bioinformatics, graph analysis, image/video recognition, and medical imaging, with an overarching theme of leveraging knowledge from multiple sources for accurate and *interpretable* prediction.

See our [arXiv preprint](https://arxiv.org/abs/2106.09756) and four short introductory videos on YouTube: [Why build PyKale?](https://youtu.be/nybYgw-T2bM) [How was PyKale built?](https://youtu.be/jaIbkjkQvYs) [What's in PyKale?](https://youtu.be/I3vifU2rcc0) and [a 5-min summary](https://youtu.be/Snou2gg7pek).

#### Pipeline-based API

- `loaddata` loads data from disk or online resources as input
- `prepdata` preprocesses data to fit machine learning modules below (transforms)
- `embed` embeds data in a new space to learn a new representation (feature extraction/selection)
- `predict` predicts a desired output
- `evaluate` evaluates the performance using some metrics
- `interpret` interprets the features and outputs via post-prediction analysis mainly via visualization
- `pipeline` specifies a machine learning workflow by combining several other modules

#### Example usage

- `examples` demonstrate real applications on specific datasets with a standardized structure.

## How to Use

### Step 0: Installation

PyKale supports Python 3.8, 3.9, or 3.10. Before installing `pykale`, we suggest you to first [install PyTorch](https://pytorch.org/get-started/locally/) matching your hardware, and if graphs will be used, install [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) following its [official instructions](https://github.com/rusty1s/pytorch_geometric#installation).

Simple installation of `pykale` from [PyPI](https://pypi.org/project/pykale/):

```bash
pip install pykale
```

For more details and other options, please refer to [the installation guide](https://pykale.readthedocs.io/en/latest/installation.html).

### Step 1: Tutorials and Examples

Start with a brief [tutorial](https://pykale.readthedocs.io/en/latest/tutorial.html#usage-of-pipeline-based-api-in-examples) walking through API usage in examples or *interactive* [Jupyter notebook tutorials](https://pykale.readthedocs.io/en/latest/notebooks.html), e.g. [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pykale/pykale/blob/main/examples/digits_dann/tutorial.ipynb) or  [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pykale/pykale/HEAD?filepath=examples%2Fdigits_dann%2Ftutorial.ipynb) for a basic digit classification problem.

Browse through the [**examples**](https://github.com/pykale/pykale/tree/main/examples) to see the usage of PyKale in performing various prediction tasks in a wide range of applications, using a variety of settings, e.g. with or without [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning).

Ask questions on [PyKale's GitHub Discussions tab](https://github.com/pykale/pykale/discussions) if you need help or create an issue if you find som

### Step 2: Building and Contributing

Build new modules and/or projects with PyKale referring to the [tutorial](https://pykale.readthedocs.io/en/latest/tutorial.html#building-new-modules-or-projects), e.g., on how to modify an existing pipeline or build a new one.

This is an open-source project welcoming your contributions. You can contribute in three ways:

- [Star](https://docs.github.com/en/github/getting-started-with-github/saving-repositories-with-stars) and [fork](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) PyKale to follow its latest developments, share it with your networks, and [ask questions](https://github.com/pykale/pykale/discussions)  about it.
- Use PyKale in your project and let us know any bugs (& fixes) and feature requests/suggestions via creating an [issue](https://github.com/pykale/pykale/issues).
- Contribute via [branch, fork, and pull](https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md#branch-fork-and-pull) for minor fixes and new features, functions, or examples to become one of the [contributors](https://github.com/pykale/pykale/graphs/contributors).

See [contributing guidelines](https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.md) for more details. You can also reach us via <a href="mailto:pykale-group&#64;sheffield.ac.uk">email</a> if needed. The participation in this open source project is subject to [Code of Conduct](https://github.com/pykale/pykale/blob/main/.github/CODE_OF_CONDUCT.md).

## Who We Are

### The Team

PyKale is maintained by [Haiping Lu](http://staffwww.dcs.shef.ac.uk/people/H.Lu/), [Shuo Zhou](https://sz144.github.io/), [Xianyuan Liu](https://github.com/XianyuanLiu), and [Peizhen Bai](https://github.com/pz-white), with contributions from many other [contributors](https://github.com/pykale/pykale/graphs/contributors).

### Citation

```lang-latex
    @inproceedings{pykale-cikm2022,
      title     = {{PyKale}: Knowledge-Aware Machine Learning from Multiple Sources in {Python}},
      author    = {Haiping Lu and Xianyuan Liu and Shuo Zhou and Robert Turner and Peizhen Bai and Raivo Koot and Mustafa Chasmai and Lawrence Schobs and Hao Xu},
      booktitle = {Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM)},
      doi       = {10.1145/3511808.3557676},
      year      = {2022}
    }
```

Please consider citing our [CIKM2022 paper](https://doi.org/10.1145/3511808.3557676) above if you find _PyKale_ useful to your research.

### Acknowledgements

The development of PyKale is partially supported by the following project(s).

- Wellcome Trust Innovator Awards: Digital Technologies Ref 215799/Z/19/Z "Developing a Machine Learning Tool to Improve Prognostic and Treatment Response Assessment on Cardiac MRI Data".
</file>

<file path="setup.cfg">
[bdist_wheel]
universal=1

# [pep8]
# max-line-length = 120

[flake8]
max-line-length = 120
exclude = build,examples
ignore = E203, E501, W503
# E203 - whitespace before ':'. Opposite convention enforced by black
# E501 - line too long. Long-line code is reformated by black; remaining long lines in docstrings are OK
# W503 - line break before binary operator. W503 is incompatible with PEP 8, don't use it

[mypy]
# Suppress all missing import errors for all libraries
ignore_missing_imports = True

[build_sphinx]
all-files = 1
source-dir = docs/source
build-dir = docs/build
warning-is-error = 1

[metadata]
# This includes the license file(s) in the wheel.
# https://wheel.readthedocs.io/en/stable/user_guide.html#including-license-files-in-the-generated-wheel-file
license_files = LICENSE
</file>

<file path="setup.py">
#!/usr/bin/env python3
import re
from io import open
from os import path

from setuptools import find_packages, setup

# Dependencies with options for different user needs. If updating this, you may need to update docs/requirements.txt too.
# If option names are changed, you need to update the installation guide at docs/source/installation.md respectively.
# Not all have a min-version specified, which is not uncommon. Specify when known or necessary (e.g. errors).
# The recommended practice is to install PyTorch from the official website to match the hardware first.
# To work on graphs, install torch-geometric following the official instructions at https://github.com/pyg-team/pytorch_geometric#installation

# Key reference followed: https://github.com/pyg-team/pytorch_geometric/blob/master/setup.py

# Core dependencies frequently used in PyKale Core API
install_requires = [
    "numpy<2.0.0",  # sure
    "pandas",  # sure
    "pytorch-lightning>=2.0.0",  # in pipeline API only
    "scikit-learn>=0.23.2",  # sure
    "scipy>=1.5.4",  # in factorization API only
    "tensorly>=0.5.1",  # in factorization and model_weights API only
    "torch==2.3.0",  # also change the version in the test.yaml when changing this next time, and update the pytorch version in the bindingdb_deepdta tutorial notebook
    "torchvision>=0.12.0",  # in download, sampler (NON-ideal), and vision API only
]

# Application-specific dependencies sorted alphabetically below

# Dependencies for graph analysis, e.g., for drug discovery using Therapeutics Data Commons (TDC)
graph_requires = [
    "networkx",
    "PyTDC<=0.3.6",
]

# Dependencies for image analysis
image_requires = [
    "glob2",
    "pydicom",
    "pylibjpeg",
    "python-gdcm",
    "scikit-image>=0.16.2",
]

# End application-specific dependencies

# Dependencies for all examples and tutorials
example_requires = [
    "ipykernel",
    "ipython<=8.12.0",  # IPython 8.13+ support Python 3.9+ only and IPython 8.0-8.12 supports Python 3.8+.
    "matplotlib<=3.5.2",
    "nilearn>=0.7.0",
    "Pillow",
    "PyTDC",
    "seaborn",
    "torchsummary>=1.5.0",
    "yacs>=0.1.7",
    "pwlf",
    "xlsxwriter",
]

# Full dependencies except for development
full_requires = graph_requires + image_requires + example_requires

# Additional dependencies for development
dev_requires = full_requires + [
    "black==19.10b0",
    "coverage",
    "flake8",
    "flake8-print",
    "ipywidgets",
    "isort",
    "m2r",
    "mypy",
    "nbmake>=0.8",
    "nbsphinx",
    "nbsphinx-link",
    "nbval",
    "pre-commit",
    "pytest",
    "pytest-cov",
    "recommonmark",
    "sphinx",
    "sphinx-rtd-theme",
]


# Get version
def read(*names, **kwargs):
    with open(path.join(path.dirname(__file__), *names), encoding=kwargs.get("encoding", "utf8")) as fp:
        return fp.read()


def find_version(*file_paths):
    version_file = read(*file_paths)
    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", version_file, re.M)
    if version_match:
        return version_match.group(1)
    raise RuntimeError("Unable to find version string.")


readme = open("README.md").read()
version = find_version("kale", "__init__.py")

# Run the setup
setup(
    name="pykale",
    version=version,
    description="Knowledge-aware machine learning from multiple sources in Python",
    long_description=readme,
    long_description_content_type="text/markdown",
    author="The PyKale team",
    url="https://github.com/pykale/pykale",
    author_email="pykale-group@sheffield.ac.uk",
    project_urls={
        "Bug Tracker": "https://github.com/pykale/pykale/issues",
        "Documentation": "https://pykale.readthedocs.io",
        "Source": "https://github.com/pykale/pykale",
    },
    packages=find_packages(exclude=("docs", "examples", "tests")),
    python_requires=">=3.8,<3.11",
    install_requires=install_requires,
    extras_require={
        "graph": graph_requires,
        "image": image_requires,
        "example": example_requires,
        "full": full_requires,
        "dev": dev_requires,
    },
    setup_requires=["setuptools==59.5.0"],
    license="MIT",
    keywords="machine learning, pytorch, deep learning, multimodal learning, transfer learning",
    classifiers=[
        "Intended Audience :: Developers",
        "Intended Audience :: Education",
        "Intended Audience :: Healthcare Industry",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Scientific/Engineering :: Medical Science Apps.",
        "Topic :: Software Development :: Libraries",
        "Natural Language :: English",
    ],
)
</file>

</files>
